{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######imports\n",
    "\n",
    "import os\n",
    "\n",
    "# Add an indented block of code here\n",
    "# For example:\n",
    "print(\"Hello, world!\")\n",
    "songs = util.import_data(file_loc)\n",
    "\n",
    "def remove_sections(text):\n",
    "    # Define a regular expression pattern to match the song sections and their variations\n",
    "    pattern = r\"\\[(Verse|Intro|Chorus|Interlude|Outro).*?\\]\"\n",
    "    \n",
    "    # Replace the matched patterns with an empty string\n",
    "    cleaned_text = re.sub(pattern, \"\", text)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "\n",
    "songs['Lyrics'] = songs['Lyrics'].apply(remove_sections)\n",
    "print(f\"Lyrics after remove_sections: {songs['Lyrics'][0]}\")\n",
    "\n",
    "songs['Lyrics'] = songs['Lyrics'].apply(util.handle_special)\n",
    "print(songs['Lyrics'][0])   \n",
    "      \n",
    "songs['Lyrics'] = songs['Lyrics'].apply(util.remove_non_ascii_and_print)\n",
    "print(songs['Lyrics'][0])\n",
    "\n",
    "\n",
    "contractions_dict = {\n",
    "    \"im\": \"i am\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"its\": \"it is\",\n",
    "    \"were\": \"we are\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    # Add more contractions and their expansions as needed\n",
    "}\n",
    "import re\n",
    "\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_word, rest = re.match(r\"(\\w+)(.*)\", match, re.IGNORECASE).groups()\n",
    "        expanded_contraction = contractions_dict.get(first_word.lower(), first_word.lower()) + rest\n",
    "        return expanded_contraction\n",
    "\n",
    "    contractions_pattern = re.compile(r'\\b({})\\b'.format('|'.join(contractions_dict.keys())), flags=re.IGNORECASE)\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "songs['Lyrics'] = songs['Lyrics'].apply(expand_contractions, args=(contractions_dict,))\n",
    "print(f\"Lyrics after expanding contractions: {songs['Lyrics'][0]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "special_tokens = {'pad_token': '<PAD>'}\n",
    "special_tokens_dict = {'additional_special_tokens': ['<startsong>', '<endsong>']}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<startsong> '+ txt + ' <endsong>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "\n",
    "# Assuming `songs` is a list containing all your song lyrics\n",
    "dataset = SongDataset(songs['Lyrics'], tokenizer, max_length=512)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True) \n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move the model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "epochs = 4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * epochs)\n",
    "\n",
    "\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_perplexities = []\n",
    "# Training loop with gradient accumulation\n",
    "epochs = 10\n",
    "gradient_accumulation_steps = 4  # Increase this if facing memory issues\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    model.zero_grad()  # Move the zero_grad() outside the batch loop for gradient accumulation\n",
    "    for batch_idx, (input_ids, masks) in enumerate(dataloader):\n",
    "        input_ids, masks = input_ids.to(device), masks.to(device)\n",
    "        outputs = model(input_ids, labels=input_ids, attention_mask=masks)\n",
    "        loss = outputs.loss / gradient_accumulation_steps  # Scale the loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        perplexity = np.exp(loss.item())\n",
    "        print(f\"Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item()} - Perplexity: {perplexity}\")\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item()}\")\n",
    "\n",
    "            # Decode and print the input, target, and prediction\n",
    "            input_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            target_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)  # same as input for LM\n",
    "            prediction_ids = torch.argmax(outputs.logits, dim=-1)[0]\n",
    "            prediction_tokens = tokenizer.decode(prediction_ids, skip_special_tokens=True)\n",
    "\n",
    "            print(f\"  Input Sequence: {input_tokens}\")\n",
    "            print(f\"  Target Sequence: {target_tokens}\")\n",
    "            print(f\"  Prediction: {prediction_tokens}\\n\")\n",
    "\n",
    "    # Calculate average loss and perplexity for the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_perplexity = np.exp(avg_loss)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    epoch_perplexities.append(avg_perplexity)\n",
    "    print(f\"Average Loss: {avg_loss} - Average Perplexity: {avg_perplexity}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(epoch_losses, epoch_perplexities):\n",
    "    epochs_range = range(1, len(epoch_losses) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, epoch_losses, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, epoch_perplexities, label='Training Perplexity')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Training Perplexity Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(epoch_losses, epoch_perplexities)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "prompt = \"<startsong>\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "def generate_text_samples(model, tokenizer, prompt, device, num_samples=3):\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    generated = generated.to(device)\n",
    "\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "    # Define different generation configurations\n",
    "    generation_configs = [\n",
    "        {\"config\": {\"temperature\": 0.8, \"top_k\": 50, \"top_p\": 0.95}, \"description\": \"Temperature\"},\n",
    "        {\"config\": {\"temperature\": 1.0, \"top_k\": 30, \"top_p\": 0.95}, \"description\": \"Top-K\"},\n",
    "        {\"config\": {\"temperature\": 1.0, \"top_k\": 0, \"top_p\": 0.85}, \"description\": \"Top-P\"},\n",
    "        {\"config\": {\"temperature\": 1.0, \"top_k\": 50, \"top_p\": 0.95, \"num_beams\": 5, \"early_stopping\": True}, \"description\": \"Beam Search\"},\n",
    "        {\"config\": {\"temperature\": 1.0, \"top_k\": 50, \"top_p\": 0.95, \"repetition_penalty\": 2.0}, \"description\": \"No Repetition Penalty\"},\n",
    "\n",
    "        {\"config\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95, \"repetition_penalty\": 2.5}, \"description\": \"Temperature with High Repetition Penalty\"},\n",
    "        {\"config\": {\"temperature\": 0.9, \"top_k\": 20, \"top_p\": 0.85, \"repetition_penalty\": 2.0, \"no_repeat_ngram_size\": 2}, \"description\": \"Top-K with N-Gram Repetition Prevention\"},\n",
    "        {\"config\": {\"temperature\": 1.0, \"top_k\": 0, \"top_p\": 0.8, \"repetition_penalty\": 2.0, \"no_repeat_ngram_size\": 3}, \"description\": \"Top-P with N-Gram Repetition Prevention\"},\n",
    "        {\"config\": {\"num_beams\": 5, \"early_stopping\": True, \"no_repeat_ngram_size\": 3}, \"description\": \"Beam Search with N-Gram Repetition Prevention\"},\n",
    "        {\"config\": {\"temperature\": 1.0, \"top_k\": 50, \"top_p\": 0.95, \"repetition_penalty\": 3.0, \"no_repeat_ngram_size\": 4}, \"description\": \"No Repetition Penalty with Strong N-Gram Prevention\"}\n",
    "    ]\n",
    "    for config in generation_configs:\n",
    "        print(f\"Generating with {config['description']}...\")\n",
    "        sample_outputs = model.generate(\n",
    "             input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            do_sample=True,\n",
    "            max_length=300,\n",
    "            num_return_sequences=num_samples,\n",
    "            **config['config']  # Unpack all other parameters from the config dictionary\n",
    "        )\n",
    "\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            print(f\"{config['description']} Sample {i+1}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "generate_text_samples(model, tokenizer, prompt=\"<startsong>\", device=device, num_samples=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
