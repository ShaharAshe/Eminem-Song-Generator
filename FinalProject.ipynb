{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nlp final project\n",
    "\n",
    "1. Yaniv Gabay - 205745615 - yanivga@edu.hac.ac.il\n",
    "2. Shahar Asher - 209305408 - shaharas@edu.hac.ac.il\n",
    "3. Hadar Liel Harush - 211721568 - hadarhar@edu.hac.ac.il\n",
    "## Eminem song generator\n",
    "\n",
    "This project will compare different models, all trained on the legendary hip-hop artist Eminem lyrics.\n",
    "We want to expirment with different models, our own model, different apis, to see and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be added?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######imports\n",
    "\n",
    "#pip install allennlp\n",
    "#pip install flair\n",
    "#pip install pronouncing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, ELMoEmbeddings,TransformerWordEmbeddings\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import pronouncing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download NLTK data (e.g., WordNet, Punkt tokenizer models)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with encoding utf-8: 'utf-8' codec can't decode byte 0x92 in position 6: invalid start byte\n",
      "Success with encoding: latin1\n",
      "                        Album_Name       Song_Name  \\\n",
      "0  Music To Be Murdered By: Side B  Alfred (Intro)   \n",
      "1  Music To Be Murdered By: Side B     Black Magic   \n",
      "2  Music To Be Murdered By: Side B  Alfreds Theme   \n",
      "3  Music To Be Murdered By: Side B       Tone Deaf   \n",
      "4  Music To Be Murdered By: Side B  Book of Rhymes   \n",
      "\n",
      "                                              Lyrics  \\\n",
      "0  [Intro: Alfred Hitchcock]\\nThus far, this albu...   \n",
      "1  [Chorus: Skylar Grey & Eminem]\\nBlack magic, n...   \n",
      "2  [Verse 1]\\nBefore I check the mic (Check, chec...   \n",
      "3  [Intro]\\nYeah, I'm sorry (Huh?)\\nWhat did you ...   \n",
      "4  [Intro]\\nI don't smile, I don't frown, get too...   \n",
      "\n",
      "                                           Album_URL   Views  \\\n",
      "0  https://genius.com/albums/Eminem/Music-to-be-m...   24.3K   \n",
      "1  https://genius.com/albums/Eminem/Music-to-be-m...  180.6K   \n",
      "2  https://genius.com/albums/Eminem/Music-to-be-m...  285.6K   \n",
      "3  https://genius.com/albums/Eminem/Music-to-be-m...  210.9K   \n",
      "4  https://genius.com/albums/Eminem/Music-to-be-m...  193.3K   \n",
      "\n",
      "        Release_date Unnamed: 6  \n",
      "0  December 18, 2020        NaN  \n",
      "1  December 18, 2020        NaN  \n",
      "2  December 18, 2020        NaN  \n",
      "3  December 18, 2020        NaN  \n",
      "4  December 18, 2020        NaN  \n"
     ]
    }
   ],
   "source": [
    "##### Loading the data\n",
    "### we be using <startVerse> <endVerse> <startChorus><endChorus>\n",
    "## to give our model some different between the verses and the choruses.\n",
    "\n",
    "#important to export the data to see the effect\n",
    "#of changes, especilly with a small dataset\n",
    "def export_data(data,stage):\n",
    "    try:\n",
    "        data.to_csv('data_'+stage+'.csv')\n",
    "    except Exception as e:\n",
    "        print(\"Error exporting the csv file:\", e)  \n",
    "#basic import\n",
    "def import_data(location):\n",
    "    df = read_data(location)\n",
    "    return df\n",
    "#read with try except,can add more specific\n",
    "#we try different encoding, you can never know what chars you will find.\n",
    "# this one is working with latin1, but we still left the function\n",
    "#exceptions\n",
    "def read_data(location):\n",
    "    try_encodings = ['utf-8', 'latin1', 'utf-16', 'cp1252', 'ISO-8859-1', 'windows-1252']\n",
    "    for encoding in try_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(location ,encoding=encoding,delimiter='\\t')\n",
    "            print(f\"Success with encoding: {encoding}\")\n",
    "            print(df.head())\n",
    "            return df\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error with encoding {encoding}: {e}\")\n",
    "####\n",
    "file_loc = './Eminem_Lyrics.csv'\n",
    "df = import_data(file_loc)\n",
    "export_data(df,'afterImport-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      <startSong>\\n<startIntro>\\nThus far, this albu...\n",
      "1      <startSong>\\n<startChorus>\\nBlack magic, night...\n",
      "2      <startSong>\\n<startVerse>\\nBefore I check the ...\n",
      "3      <startSong>\\n<startIntro>\\nYeah, I'm sorry (Hu...\n",
      "4      <startSong>\\n<startIntro>\\nI don't smile, I do...\n",
      "                             ...                        \n",
      "343    <startSong>\\n<startChorus>\\nI know there's som...\n",
      "344    <startSong>\\n<startIntro>\\nYeah, yeah, I get i...\n",
      "345    <startSong>\\n<startVerse>\\nI cut back on the s...\n",
      "346    <startSong>\\n<startIntro>\\nC'mon!\\n\\n<endIntro...\n",
      "347    <startSong>\\n<startIntro>\\nCDs ain't selling n...\n",
      "Name: Lyrics, Length: 348, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#actual messing with the dataset\n",
    "import re\n",
    "\n",
    "\n",
    "def insert_tokens(lyrics):\n",
    "    # Pattern to identify all tags (verses, choruses, etc.)\n",
    "    pattern = re.compile(r'\\[(Verse|Chorus|Intro|Outro)')\n",
    "    \n",
    "    # Placeholder for processed lyrics\n",
    "    processed_lyrics = \"<startSong>\\n\"\n",
    "    last_tag = None\n",
    "\n",
    "    for line in lyrics.split('\\n'):\n",
    "        tag_match = pattern.match(line)\n",
    "        if tag_match:\n",
    "            # Close the previous tag if exists\n",
    "            if last_tag:\n",
    "                processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "            # Update last_tag and open a new tag\n",
    "            last_tag = tag_match.group(1)  # Capture the tag name (Verse or Chorus)\n",
    "            processed_lyrics += f\"<start{last_tag}>\\n\"\n",
    "        else:\n",
    "            processed_lyrics += line + \"\\n\"\n",
    "\n",
    "    # Close the last opened tag\n",
    "    if last_tag:\n",
    "        processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "    processed_lyrics += \"<endSong>\"\n",
    "\n",
    "    return processed_lyrics\n",
    "\n",
    "def remove_remaining_tags(lyrics):\n",
    "    # Pattern to match any text within square brackets\n",
    "    pattern = re.compile(r'\\[[^\\]]*\\]')\n",
    "    cleaned_lyrics = re.sub(pattern, '', lyrics)\n",
    "    return cleaned_lyrics\n",
    "\n",
    "def handle_special(lyrics):\n",
    "    #remove ' and - and ? and !\n",
    "    output_string = re.sub(r\"['\\-?!]\", '', lyrics)\n",
    "    return output_string\n",
    "\n",
    "new_df = df['Lyrics'].apply(insert_tokens)\n",
    "\n",
    "print(new_df)\n",
    "export_data(new_df,'afterUniqueTokens')\n",
    "\n",
    "## some songs can have some another tags, like post-chorus,pre etc. \n",
    "## we arent intresnted in those, so we will just delete them in the next step\n",
    "new_df = new_df.apply(remove_remaining_tags)\n",
    "new_df = new_df.apply(handle_special)\n",
    "#lowercase everything\n",
    "new_df = new_df.str.lower()\n",
    "\n",
    "export_data(new_df,'afterLowerAndRemove')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed characters: \n",
      "Removed characters: á\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ö\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: áó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: óéáí\n",
      "Removed characters: \n",
      "Removed characters: ú\n",
      "Removed characters: \n",
      "Removed characters: â\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: è\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: áâ\n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éàéàéà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éß\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ï\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ç\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n"
     ]
    }
   ],
   "source": [
    "### special chars\n",
    "def remove_non_ascii_and_print(text):\n",
    "    cleaned_text = []\n",
    "    removed_chars = []\n",
    "    for char in text:\n",
    "        if ord(char) < 128:\n",
    "            cleaned_text.append(char)\n",
    "        else:\n",
    "            removed_chars.append(char)\n",
    "    if removed_chars:\n",
    "        print(f\"Removed characters: {''.join(removed_chars)}\")\n",
    "    return ''.join(cleaned_text)\n",
    "clean = new_df.apply(remove_non_ascii_and_print)\n",
    "\n",
    "export_data(clean,'clean')\n",
    "#### we still have (lyrics) in the dataset, need to decide\n",
    "### we still have ! or ? in the dataset, should we remove? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will try several tokenizer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#python -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[0;32m     24\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m spacy_tokens_df \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#python -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[0;32m     24\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m spacy_tokens_df \u001b[38;5;241m=\u001b[39m temp_clean\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\pipeline\\lemmatizer.py:135\u001b[0m, in \u001b[0;36mLemmatizer.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverwrite \u001b[38;5;129;01mor\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m             token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\pipeline\\lemmatizer.py:214\u001b[0m, in \u001b[0;36mLemmatizer.rule_lemmatize\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [string\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# See Issue #435 for example of where this logic is requied.\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_base_form\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [string\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[0;32m    216\u001b[0m index_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookups\u001b[38;5;241m.\u001b[39mget_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\lang\\en\\lemmatizer.py:8\u001b[0m, in \u001b[0;36mEnglishLemmatizer.is_base_form\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEnglishLemmatizer\u001b[39;00m(Lemmatizer):\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"English lemmatizer. Only overrides is_base_form.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_base_form\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: Token) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        Check whether we're dealing with an uninflected paradigm, so we can\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m        avoid lemmatization entirely.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m            Universal Dependencies scheme.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         univ_pos \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mpos_\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_switch = {'<startverse>':'startverse',\n",
    "                '<endverse>':'endverse',\n",
    "                '<startchorus>':'startchorus',\n",
    "                '<endchorus>':'endchorus',\n",
    "                '<startoutro>':'startoutro',\n",
    "                '<endoutro>':'endoutro',\n",
    "                '<startintro>':'startintro',\n",
    "                '<endintro>':'endintro',\n",
    "                '<startsong>':'startsong',\n",
    "                '<endsong>':'endsong'}\n",
    "def replace_tags(text, temp_switch, reverse=False):\n",
    "    if reverse:\n",
    "        temp_switch = {v: k for k, v in temp_switch.items()}\n",
    "    \n",
    "    for tag, replacement in temp_switch.items():\n",
    "        text = text.replace(tag, replacement)\n",
    "    return text\n",
    "\n",
    "temp_clean = clean.apply(lambda x: replace_tags(x, temp_switch, reverse=False))\n",
    "export_data(temp_clean,'temp_clean')\n",
    "#pip install spacy\n",
    "import spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens_df = temp_clean.apply(lambda x: [token.text for token in nlp(x)])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "nltk_tokens_df = temp_clean.apply(word_tokenize)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "# For BERT\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "BERT_tokens_df = temp_clean.apply(lambda x: tokenizer_bert.tokenize(x))\n",
    "# For GPT-2\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokens_df = temp_clean.apply(lambda x: tokenizer_gpt2.tokenize(x))\n",
    "\n",
    "export_data(spacy_tokens_df,'spacy')\n",
    "export_data(nltk_tokens_df,'nltk')\n",
    "export_data(BERT_tokens_df,'BERT')\n",
    "export_data(gpt2_tokens_df,'GPT2')\n",
    "\n",
    "total_token_models = [spacy_tokens_df, nltk_tokens_df, BERT_tokens_df, gpt2_tokens_df]\n",
    "for model in total_token_models:\n",
    "    print(\"\")\n",
    "    print(model.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
