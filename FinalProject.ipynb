{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nlp final project\n",
    "\n",
    "1. Yaniv Gabay - 205745615 - yanivga@edu.hac.ac.il\n",
    "2. Shahar Asher - 209305408 - shaharas@edu.hac.ac.il\n",
    "3. Hadar Liel Harush - 211721568 - hadarhar@edu.hac.ac.il\n",
    "## Eminem song generator\n",
    "\n",
    "This project will compare different models, all trained on the legendary hip-hop artist Eminem lyrics.\n",
    "We want to expirment with different models, our own model, different apis, to see and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be added?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######imports\n",
    "\n",
    "#pip install allennlp\n",
    "#pip install flair\n",
    "#pip install pronouncing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, ELMoEmbeddings,TransformerWordEmbeddings\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import pronouncing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download NLTK data (e.g., WordNet, Punkt tokenizer models)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#### if you want to export data\n",
    "####\n",
    "debug = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with encoding utf-8: 'utf-8' codec can't decode byte 0x92 in position 6: invalid start byte\n",
      "Success with encoding: latin1\n",
      "                        Album_Name       Song_Name  \\\n",
      "0  Music To Be Murdered By: Side B  Alfred (Intro)   \n",
      "1  Music To Be Murdered By: Side B     Black Magic   \n",
      "2  Music To Be Murdered By: Side B  Alfreds Theme   \n",
      "3  Music To Be Murdered By: Side B       Tone Deaf   \n",
      "4  Music To Be Murdered By: Side B  Book of Rhymes   \n",
      "\n",
      "                                              Lyrics  \\\n",
      "0  [Intro: Alfred Hitchcock]\\nThus far, this albu...   \n",
      "1  [Chorus: Skylar Grey & Eminem]\\nBlack magic, n...   \n",
      "2  [Verse 1]\\nBefore I check the mic (Check, chec...   \n",
      "3  [Intro]\\nYeah, I'm sorry (Huh?)\\nWhat did you ...   \n",
      "4  [Intro]\\nI don't smile, I don't frown, get too...   \n",
      "\n",
      "                                           Album_URL   Views  \\\n",
      "0  https://genius.com/albums/Eminem/Music-to-be-m...   24.3K   \n",
      "1  https://genius.com/albums/Eminem/Music-to-be-m...  180.6K   \n",
      "2  https://genius.com/albums/Eminem/Music-to-be-m...  285.6K   \n",
      "3  https://genius.com/albums/Eminem/Music-to-be-m...  210.9K   \n",
      "4  https://genius.com/albums/Eminem/Music-to-be-m...  193.3K   \n",
      "\n",
      "        Release_date Unnamed: 6  \n",
      "0  December 18, 2020        NaN  \n",
      "1  December 18, 2020        NaN  \n",
      "2  December 18, 2020        NaN  \n",
      "3  December 18, 2020        NaN  \n",
      "4  December 18, 2020        NaN  \n"
     ]
    }
   ],
   "source": [
    "##### Loading the data\n",
    "### we be using <startVerse> <endVerse> <startChorus><endChorus>\n",
    "## to give our model some different between the verses and the choruses.\n",
    "\n",
    "#important to export the data to see the effect\n",
    "#of changes, especilly with a small dataset\n",
    "def export_data(data,stage,debug_mode):\n",
    "    if(not debug_mode):\n",
    "        return\n",
    "    try:\n",
    "        data.to_csv('data_'+stage+'.csv')\n",
    "    except Exception as e:\n",
    "        print(\"Error exporting the csv file:\", e)  \n",
    "#basic import\n",
    "def import_data(location):\n",
    "    df = read_data(location)\n",
    "    return df\n",
    "#read with try except,can add more specific\n",
    "#we try different encoding, you can never know what chars you will find.\n",
    "# this one is working with latin1, but we still left the function\n",
    "#exceptions\n",
    "def read_data(location):\n",
    "    try_encodings = ['utf-8', 'latin1', 'utf-16', 'cp1252', 'ISO-8859-1', 'windows-1252']\n",
    "    for encoding in try_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(location ,encoding=encoding,delimiter='\\t')\n",
    "            print(f\"Success with encoding: {encoding}\")\n",
    "            print(df.head())\n",
    "            return df\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error with encoding {encoding}: {e}\")\n",
    "####\n",
    "file_loc = './Eminem_Lyrics.csv'\n",
    "df = import_data(file_loc)\n",
    "\n",
    "export_data(df,'afterImport-1',debug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      <startSong>\\n<startIntro>\\nThus far, this albu...\n",
      "1      <startSong>\\n<startChorus>\\nBlack magic, night...\n",
      "2      <startSong>\\n<startVerse>\\nBefore I check the ...\n",
      "3      <startSong>\\n<startIntro>\\nYeah, I'm sorry (Hu...\n",
      "4      <startSong>\\n<startIntro>\\nI don't smile, I do...\n",
      "                             ...                        \n",
      "343    <startSong>\\n<startChorus>\\nI know there's som...\n",
      "344    <startSong>\\n<startIntro>\\nYeah, yeah, I get i...\n",
      "345    <startSong>\\n<startVerse>\\nI cut back on the s...\n",
      "346    <startSong>\\n<startIntro>\\nC'mon!\\n\\n<endIntro...\n",
      "347    <startSong>\\n<startIntro>\\nCDs ain't selling n...\n",
      "Name: Lyrics, Length: 348, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#actual messing with the dataset\n",
    "import re\n",
    "\n",
    "\n",
    "def insert_tokens(lyrics):\n",
    "    # Pattern to identify all tags (verses, choruses, etc.)\n",
    "    pattern = re.compile(r'\\[(Verse|Chorus|Intro|Outro)')\n",
    "    \n",
    "    # Placeholder for processed lyrics\n",
    "    processed_lyrics = \"<startSong>\\n\"\n",
    "    last_tag = None\n",
    "\n",
    "    for line in lyrics.split('\\n'):\n",
    "        tag_match = pattern.match(line)\n",
    "        if tag_match:\n",
    "            # Close the previous tag if exists\n",
    "            if last_tag:\n",
    "                processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "            # Update last_tag and open a new tag\n",
    "            last_tag = tag_match.group(1)  # Capture the tag name (Verse or Chorus)\n",
    "            processed_lyrics += f\"<start{last_tag}>\\n\"\n",
    "        else:\n",
    "            processed_lyrics += line + \"\\n\"\n",
    "\n",
    "    # Close the last opened tag\n",
    "    if last_tag:\n",
    "        processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "    processed_lyrics += \"<endSong>\"\n",
    "\n",
    "    return processed_lyrics\n",
    "\n",
    "def remove_remaining_tags(lyrics):\n",
    "    # Pattern to match any text within square brackets\n",
    "    pattern = re.compile(r'\\[[^\\]]*\\]')\n",
    "    cleaned_lyrics = re.sub(pattern, '', lyrics)\n",
    "    return cleaned_lyrics\n",
    "\n",
    "def handle_special(lyrics):\n",
    "    #remove ' and - and ? and !\n",
    "    output_string = re.sub(r\"['\\-?!]\",'', lyrics)\n",
    "    return output_string\n",
    "\n",
    "new_df = df['Lyrics'].apply(insert_tokens)\n",
    "\n",
    "print(new_df)\n",
    "export_data(new_df,'afterInsertTokens',debug)\n",
    "\n",
    "\n",
    "## some songs can have some another tags, like post-chorus,pre etc. \n",
    "## we arent intresnted in those, so we will just delete them in the next step\n",
    "new_df = new_df.apply(remove_remaining_tags)\n",
    "new_df = new_df.apply(handle_special)\n",
    "#lowercase everything\n",
    "new_df = new_df.str.lower()\n",
    "\n",
    "export_data(new_df,'afterLowerAndRemove',debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed characters: \n",
      "Removed characters: á\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ö\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: áó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: óéáí\n",
      "Removed characters: \n",
      "Removed characters: ú\n",
      "Removed characters: \n",
      "Removed characters: â\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: è\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: áâ\n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éàéàéà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éß\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ï\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ç\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n"
     ]
    }
   ],
   "source": [
    "### special chars\n",
    "def remove_non_ascii_and_print(text):\n",
    "    cleaned_text = []\n",
    "    removed_chars = []\n",
    "    for char in text:\n",
    "        if ord(char) < 128:\n",
    "            cleaned_text.append(char)\n",
    "        else:\n",
    "            removed_chars.append(char)\n",
    "    if removed_chars:\n",
    "        print(f\"Removed characters: {''.join(removed_chars)}\")\n",
    "    return ''.join(cleaned_text)\n",
    "clean = new_df.apply(remove_non_ascii_and_print)\n",
    "\n",
    "\n",
    "export_data(clean,'afterRemoveNonAscii',debug)\n",
    "\n",
    "#### we still have (lyrics) in the dataset, need to decide\n",
    "### we still have ! or ? in the dataset, should we remove? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will try several tokenizer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of added tokens: 10\n",
      "\n",
      "0    [startsong, \\n, startintro, \\n, thus, far, ,, ...\n",
      "1    [startsong, \\n, startchorus, \\n, black, magic,...\n",
      "2    [startsong, \\n, startverse, \\n, before, i, che...\n",
      "3    [startsong, \\n, startintro, \\n, yeah, ,, i, m,...\n",
      "4    [startsong, \\n, startintro, \\n, i, do, nt, smi...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, far, ,, this, al...\n",
      "1    [startsong, startchorus, black, magic, ,, nigh...\n",
      "2    [startsong, startverse, before, i, check, the,...\n",
      "3    [startsong, startintro, yeah, ,, im, sorry, (,...\n",
      "4    [startsong, startintro, i, dont, smile, ,, i, ...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, far, ,, this, al...\n",
      "1    [startsong, startchorus, black, magic, ,, nigh...\n",
      "2    [startsong, startverse, before, i, check, the,...\n",
      "3    [startsong, startintro, yeah, ,, im, sorry, (,...\n",
      "4    [startsong, startintro, i, don, ##t, smile, ,,...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, Ġfar, ,, Ġthis, ...\n",
      "1    [startsong, startchorus, black, Ġmagic, ,, Ġni...\n",
      "2    [startsong, startverse, before, Ġi, Ġcheck, Ġt...\n",
      "3    [startsong, startintro, yeah, ,, Ġim, Ġsorry, ...\n",
      "4    [startsong, startintro, i, Ġdont, Ġsmile, ,, Ġ...\n",
      "Name: Lyrics, dtype: object\n"
     ]
    }
   ],
   "source": [
    "temp_switch = {'<startverse>':'startverse',\n",
    "                '<endverse>':'endverse',\n",
    "                '<startchorus>':'startchorus',\n",
    "                '<endchorus>':'endchorus',\n",
    "                '<startoutro>':'startoutro',\n",
    "                '<endoutro>':'endoutro',\n",
    "                '<startintro>':'startintro',\n",
    "                '<endintro>':'endintro',\n",
    "                '<startsong>':'startsong',\n",
    "                '<endsong>':'endsong'}\n",
    "def replace_tags(text, temp_switch, reverse=False):\n",
    "    if reverse:\n",
    "        temp_switch = {v: k for k, v in temp_switch.items()}\n",
    "    \n",
    "    for tag, replacement in temp_switch.items():\n",
    "        text = text.replace(tag, replacement)\n",
    "    return text\n",
    "\n",
    "temp_clean = clean.apply(lambda x: replace_tags(x, temp_switch, reverse=False))\n",
    "export_data(temp_clean,'temp_clean',debug)\n",
    "#pip install spacy\n",
    "import spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens_df = temp_clean.apply(lambda x: [token.text for token in nlp(x)])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "nltk_tokens_df = temp_clean.apply(word_tokenize)\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# For BERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "num_of_added_tokens = tokenizer_bert.add_special_tokens({'additional_special_tokens':\n",
    "                                    ['startverse', 'endverse',\n",
    "                                      'startchorus', 'endchorus',\n",
    "                                        'startoutro', 'endoutro', \n",
    "                                        'startintro', 'endintro',\n",
    "                                        'startsong', 'endsong']})\n",
    "print(f\"Number of added tokens: {num_of_added_tokens}\")\n",
    "BERT_tokens_df = temp_clean.apply(lambda x: tokenizer_bert.tokenize(x))\n",
    "from transformers import GPT2Tokenizer\n",
    "# For GPT-2\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "num_of_added_tokens = tokenizer_gpt2.add_special_tokens({'additional_special_tokens':\n",
    "                                    ['startverse', 'endverse',\n",
    "                                      'startchorus', 'endchorus',\n",
    "                                        'startoutro', 'endoutro', \n",
    "                                        'startintro', 'endintro',\n",
    "                                          'startsong', 'endsong']})\n",
    "gpt2_tokens_df = temp_clean.apply(lambda x: tokenizer_gpt2.tokenize(x))\n",
    "\n",
    "export_data(spacy_tokens_df,'spacy',debug)\n",
    "export_data(nltk_tokens_df,'nltk',debug)\n",
    "export_data(BERT_tokens_df,'BERT',debug)\n",
    "export_data(gpt2_tokens_df,'GPT2', debug )\n",
    "\n",
    "total_token_models = [spacy_tokens_df, nltk_tokens_df, BERT_tokens_df, gpt2_tokens_df]\n",
    "for model in total_token_models:\n",
    "    print(\"\")\n",
    "    print(model.head())\n",
    "\n",
    "## pronouncing library\n",
    "## we will use it to get the phonetic features of the words\n",
    "    \n",
    "\n",
    "import pronouncing\n",
    "\n",
    "def get_phonetic_features(word):\n",
    "    phones = pronouncing.phones_for_word(word)\n",
    "    stress = [pronouncing.stresses(p) for p in phones] if phones else ''\n",
    "    return {'phones': phones, 'stress': stress}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spacy_phonetics_df = spacy_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "nltk_phonetics_df = nltk_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "bert_phonetics_df = BERT_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "gpt2_phonetics_df = gpt2_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "\n",
    "export_data(spacy_phonetics_df, 'spacy_phonetics',debug)\n",
    "export_data(nltk_phonetics_df, 'nltk_phonetics',  debug)\n",
    "export_data(bert_phonetics_df, 'bert_phonetics', debug)\n",
    "export_data(gpt2_phonetics_df, 'gpt2_phonetics', debug)\n",
    "\n",
    "total_phonetic_models = [spacy_phonetics_df, nltk_phonetics_df, bert_phonetics_df, gpt2_phonetics_df]\n",
    "\n",
    "\n",
    "##### now , we need to perpare the data for the embeddings\n",
    "### so padding, indexing unique tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with encoding: utf-8\n",
      "                                                  ,0\n",
      "0  0,\"[tensor([ 0.0000,  0.0000,  0.0000,  ..., -...\n",
      "1         device='cuda:0'), tensor([ 0.0000,  0.0...\n",
      "2         device='cuda:0'), tensor([-0.0742,  0.0...\n",
      "3         device='cuda:0'), tensor([-1.2351e-01, ...\n",
      "4          -7.3642e-02,  5.4657e-04], device='cud...\n",
      "     phone_K R EH1 D IH0 T  phone_W ER1 IY0 Z  phone_HH AA1 S T AH0 L  \\\n",
      "0                        0                  0                       0   \n",
      "1                        0                  0                       0   \n",
      "2                        0                  0                       0   \n",
      "3                        0                  0                       0   \n",
      "4                        0                  0                       0   \n",
      "..                     ...                ...                     ...   \n",
      "343                      0                  0                       0   \n",
      "344                      0                  0                       0   \n",
      "345                      0                  0                       0   \n",
      "346                      0                  0                       0   \n",
      "347                      0                  0                       0   \n",
      "\n",
      "     phone_T AE1 NG G OW0  phone_S IH1 R AH0 P  phone_G ER1 L F R EH2 N D  \\\n",
      "0                       0                    0                          0   \n",
      "1                       0                    0                          0   \n",
      "2                       0                    0                          0   \n",
      "3                       0                    0                          0   \n",
      "4                       0                    0                          0   \n",
      "..                    ...                  ...                        ...   \n",
      "343                     0                    0                          0   \n",
      "344                     0                    0                          0   \n",
      "345                     0                    0                          0   \n",
      "346                     0                    0                          0   \n",
      "347                     0                    0                          0   \n",
      "\n",
      "     phone_F IH1 Z IH0 K AH0 L IY0  phone_W EH2 L B IY1 IH0 NG  phone_W ER1 S  \\\n",
      "0                                0                           0              0   \n",
      "1                                0                           0              0   \n",
      "2                                0                           0              0   \n",
      "3                                0                           0              0   \n",
      "4                                0                           0              0   \n",
      "..                             ...                         ...            ...   \n",
      "343                              0                           0              0   \n",
      "344                              0                           0              0   \n",
      "345                              0                           0              0   \n",
      "346                              0                           0              0   \n",
      "347                              0                           0              0   \n",
      "\n",
      "     phone_G ER0 IH1 L AH0  ...  stress_00010  stress_21002  stress_110  \\\n",
      "0                        0  ...             0             0           0   \n",
      "1                        0  ...             0             0           0   \n",
      "2                        0  ...             0             0           0   \n",
      "3                        0  ...             0             0           0   \n",
      "4                        0  ...             0             0           0   \n",
      "..                     ...  ...           ...           ...         ...   \n",
      "343                      0  ...             0             0           0   \n",
      "344                      0  ...             0             0           0   \n",
      "345                      0  ...             0             0           0   \n",
      "346                      0  ...             0             0           0   \n",
      "347                      0  ...             0             0           0   \n",
      "\n",
      "     stress_1220  stress_1020  stress_212  stress_1200  stress_01000  \\\n",
      "0              0            0           0            0             0   \n",
      "1              0            0           0            1             0   \n",
      "2              0            1           0            0             1   \n",
      "3              0            1           0            0             0   \n",
      "4              0            1           0            0             0   \n",
      "..           ...          ...         ...          ...           ...   \n",
      "343            0            1           0            1             0   \n",
      "344            0            0           0            0             0   \n",
      "345            0            0           0            0             0   \n",
      "346            0            0           0            0             0   \n",
      "347            0            1           0            0             0   \n",
      "\n",
      "     stress_2210  stress_10100  \n",
      "0              0             0  \n",
      "1              0             0  \n",
      "2              0             0  \n",
      "3              0             0  \n",
      "4              0             0  \n",
      "..           ...           ...  \n",
      "343            0             0  \n",
      "344            0             0  \n",
      "345            0             0  \n",
      "346            0             0  \n",
      "347            0             0  \n",
      "\n",
      "[348 rows x 12205 columns]\n",
      "                                                  ,0\n",
      "0  0,\"[tensor([ 0.0000,  0.0000,  0.0000,  ..., -...\n",
      "1         device='cuda:0'), tensor([ 0.0000,  0.0...\n",
      "2         device='cuda:0'), tensor([-0.0742,  0.0...\n",
      "3         device='cuda:0'), tensor([-1.2351e-01, ...\n",
      "4          -7.3642e-02,  5.4657e-04], device='cud...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm  # tqdm is a library to display progress bars\n",
    "\n",
    "\n",
    "# Clear GPU memory\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "# Define stacked embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "    WordEmbeddings('glove'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "])\n",
    "\n",
    "# Initialize a list to store embeddings\n",
    "embeddings_cache = []\n",
    "file_exists = True\n",
    "# Process each song's tokens sequentially\n",
    "if(not file_exists):\n",
    "    for tokens in tqdm(spacy_tokens_df, desc=\"Processing songs\"):\n",
    "        sentence = Sentence(' '.join(tokens))\n",
    "        stacked_embeddings.embed(sentence)\n",
    "        # Convert embeddings to a tensor and store\n",
    "        song_embeddings = torch.stack([token.embedding for token in sentence])\n",
    "        embeddings_cache.append(song_embeddings)\n",
    "    embeddings_tensor = torch.stack(embeddings_cache)\n",
    "    torch.save(embeddings_tensor, 'embeddings_tensor.pt')\n",
    "\n",
    "if(file_exists):\n",
    "    embeddings_tensor = torch.load('embeddings_tensor.pt')\n",
    "\n",
    "print(embeddings_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# kaki = [0.1.12,4,21. 12.54.125.21.521.5] elmo\n",
    "# this = \n",
    "\n",
    "#{'phones': ['DH IH1 S', 'DH IH0 S'], 'stress': ['1', '0']},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_phonoteic():\n",
    "        \n",
    "    spacy_phonetics_embed_df =  pd.DataFrame(spacy_phonetics_df)\n",
    "    export_data(spacy_phonetics_embed_df, 'spacy_phonetics_embed_df', True)\n",
    "    # Flatten the phonetic data and collect all unique symbols and stress patterns\n",
    "    unique_phones = set()\n",
    "    unique_stress = set()\n",
    "    for row in spacy_phonetics_embed_df[\"Lyrics\"]:\n",
    "        for item in row:\n",
    "            unique_phones.update(item[\"phones\"])\n",
    "            unique_stress.update(item[\"stress\"])\n",
    "\n",
    "    # Initialize containers for one-hot encodings\n",
    "    phone_encodings = {}\n",
    "    stress_encodings = {}\n",
    "\n",
    "    # Populate containers with one-hot encodings\n",
    "    for phone in unique_phones:\n",
    "        phone_encodings[f\"phone_{phone}\"] = spacy_phonetics_df.apply(lambda x: any(phone in item.get(\"phones\", []) for item in x)).astype(int)\n",
    "\n",
    "    for stress in unique_stress:\n",
    "        stress_encodings[f\"stress_{stress}\"] = spacy_phonetics_df.apply(lambda x: any(stress in item.get(\"stress\", []) for item in x)).astype(int)\n",
    "\n",
    "    # Convert the encoding dictionaries to DataFrames\n",
    "    phone_df = pd.DataFrame(phone_encodings)\n",
    "    stress_df = pd.DataFrame(stress_encodings)\n",
    "\n",
    "    # Concatenate the new DataFrames with the original DataFrame\n",
    "    spacy_phonetics_embed_df = pd.concat([spacy_phonetics_embed_df, phone_df, stress_df], axis=1)\n",
    "\n",
    "\n",
    "    spacy_phonetics_embed_df = spacy_phonetics_embed_df.drop(columns=[\"Lyrics\"])\n",
    "    print(spacy_phonetics_embed_df)\n",
    "    export_data(spacy_phonetics_embed_df, 'spacy_phonetics_embed', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BILSTM NETWORK\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, use_phonetic_features=False, phonetic_dim=0):\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "        self.use_phonetic_features = use_phonetic_features\n",
    "\n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(embedding_dim + (phonetic_dim if use_phonetic_features else 0), \n",
    "                              hidden_dim, \n",
    "                              bidirectional=True, \n",
    "                              batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, embeddings, phonetic_features=None):\n",
    "        if self.use_phonetic_features and phonetic_features is not None:\n",
    "            embeddings = torch.cat((embeddings, phonetic_features), dim=2)\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        # Take the output of the last time step\n",
    "        final_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(final_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset, random_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming `spacy_embed` is a list of tensors, each representing a song's embeddings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert `spacy_embed` into a single tensor\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      8\u001b[0m embeddings_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(song_embeddings) \u001b[38;5;28;01mfor\u001b[39;00m song_embeddings \u001b[38;5;129;01min\u001b[39;00m embeddings_list])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings_tensor\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1652\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, tup: \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m-> 1652\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_tuple_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:940\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 940\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocation based indexing can only have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] types\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    945\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1554\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(key):\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   1556\u001b[0m     \u001b[38;5;66;03m# a tuple should already have been caught by this point\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;66;03m# so don't treat a tuple as a valid indexer\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many indexers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Define the sequence length and batch size\n",
    "sequence_length = 100  # Choose appropriate sequence length\n",
    "batch_size = 32  # Example batch size\n",
    "\n",
    "# Create datasets - note that this assumes embeddings_tensor is 3D: [num_samples, seq_length, features]\n",
    "dataset = TensorDataset(embeddings_tensor)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the BiLSTM model\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim*2, output_dim)  # Times 2 because of bidirectionality\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        final_output = self.linear(lstm_out[:, -1, :])  # Get the last time step's output\n",
    "        return final_output\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BiLSTM(input_dim=embeddings_tensor.size(-1), hidden_dim=256, num_layers=2, output_dim=embeddings_tensor.size(-1))\n",
    "criterion = nn.MSELoss()  # Example loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Example optimizer\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Example number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, _ in train_loader:  # Assuming you don't have labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Example loss calculation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, _ in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, inputs).item()\n",
    "        val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {loss.item()}, Val Loss: {val_loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
