{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nlp final project\n",
    "\n",
    "1. Yaniv Gabay - 205745615 - yanivga@edu.hac.ac.il\n",
    "2. Shahar Asher - 209305408 - shaharas@edu.hac.ac.il\n",
    "3. Hadar Liel Harush - 211721568 - hadarhar@edu.hac.ac.il\n",
    "## Eminem song generator\n",
    "\n",
    "This project will compare different models, all trained on the legendary hip-hop artist Eminem lyrics.\n",
    "We want to expirment with different models, our own model, different apis, to see and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be added?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######imports\n",
    "\n",
    "#pip install allennlp\n",
    "#pip install flair\n",
    "#pip install pronouncing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, ELMoEmbeddings,TransformerWordEmbeddings\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import pronouncing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download NLTK data (e.g., WordNet, Punkt tokenizer models)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#### if you want to export data\n",
    "####\n",
    "debug = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with encoding utf-8: 'utf-8' codec can't decode byte 0x92 in position 6: invalid start byte\n",
      "Success with encoding: latin1\n",
      "                        Album_Name       Song_Name  \\\n",
      "0  Music To Be Murdered By: Side B  Alfred (Intro)   \n",
      "1  Music To Be Murdered By: Side B     Black Magic   \n",
      "2  Music To Be Murdered By: Side B  Alfreds Theme   \n",
      "3  Music To Be Murdered By: Side B       Tone Deaf   \n",
      "4  Music To Be Murdered By: Side B  Book of Rhymes   \n",
      "\n",
      "                                              Lyrics  \\\n",
      "0  [Intro: Alfred Hitchcock]\\nThus far, this albu...   \n",
      "1  [Chorus: Skylar Grey & Eminem]\\nBlack magic, n...   \n",
      "2  [Verse 1]\\nBefore I check the mic (Check, chec...   \n",
      "3  [Intro]\\nYeah, I'm sorry (Huh?)\\nWhat did you ...   \n",
      "4  [Intro]\\nI don't smile, I don't frown, get too...   \n",
      "\n",
      "                                           Album_URL   Views  \\\n",
      "0  https://genius.com/albums/Eminem/Music-to-be-m...   24.3K   \n",
      "1  https://genius.com/albums/Eminem/Music-to-be-m...  180.6K   \n",
      "2  https://genius.com/albums/Eminem/Music-to-be-m...  285.6K   \n",
      "3  https://genius.com/albums/Eminem/Music-to-be-m...  210.9K   \n",
      "4  https://genius.com/albums/Eminem/Music-to-be-m...  193.3K   \n",
      "\n",
      "        Release_date Unnamed: 6  \n",
      "0  December 18, 2020        NaN  \n",
      "1  December 18, 2020        NaN  \n",
      "2  December 18, 2020        NaN  \n",
      "3  December 18, 2020        NaN  \n",
      "4  December 18, 2020        NaN  \n"
     ]
    }
   ],
   "source": [
    "##### Loading the data\n",
    "### we be using <startVerse> <endVerse> <startChorus><endChorus>\n",
    "## to give our model some different between the verses and the choruses.\n",
    "\n",
    "#important to export the data to see the effect\n",
    "#of changes, especilly with a small dataset\n",
    "def export_data(data,stage,debug_mode):\n",
    "    if(not debug_mode):\n",
    "        return\n",
    "    try:\n",
    "        data.to_csv('data_'+stage+'.csv')\n",
    "    except Exception as e:\n",
    "        print(\"Error exporting the csv file:\", e)  \n",
    "#basic import\n",
    "def import_data(location):\n",
    "    df = read_data(location)\n",
    "    return df\n",
    "#read with try except,can add more specific\n",
    "#we try different encoding, you can never know what chars you will find.\n",
    "# this one is working with latin1, but we still left the function\n",
    "#exceptions\n",
    "def read_data(location):\n",
    "    try_encodings = ['utf-8', 'latin1', 'utf-16', 'cp1252', 'ISO-8859-1', 'windows-1252']\n",
    "    for encoding in try_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(location ,encoding=encoding,delimiter='\\t')\n",
    "            print(f\"Success with encoding: {encoding}\")\n",
    "            print(df.head())\n",
    "            return df\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error with encoding {encoding}: {e}\")\n",
    "####\n",
    "file_loc = './Eminem_Lyrics.csv'\n",
    "df = import_data(file_loc)\n",
    "\n",
    "export_data(df,'afterImport-1',debug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      <startSong>\\n<startIntro>\\nThus far, this albu...\n",
      "1      <startSong>\\n<startChorus>\\nBlack magic, night...\n",
      "2      <startSong>\\n<startVerse>\\nBefore I check the ...\n",
      "3      <startSong>\\n<startIntro>\\nYeah, I'm sorry (Hu...\n",
      "4      <startSong>\\n<startIntro>\\nI don't smile, I do...\n",
      "                             ...                        \n",
      "343    <startSong>\\n<startChorus>\\nI know there's som...\n",
      "344    <startSong>\\n<startIntro>\\nYeah, yeah, I get i...\n",
      "345    <startSong>\\n<startVerse>\\nI cut back on the s...\n",
      "346    <startSong>\\n<startIntro>\\nC'mon!\\n\\n<endIntro...\n",
      "347    <startSong>\\n<startIntro>\\nCDs ain't selling n...\n",
      "Name: Lyrics, Length: 348, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#actual messing with the dataset\n",
    "import re\n",
    "\n",
    "\n",
    "def insert_tokens(lyrics):\n",
    "    # Pattern to identify all tags (verses, choruses, etc.)\n",
    "    pattern = re.compile(r'\\[(Verse|Chorus|Intro|Outro)')\n",
    "    \n",
    "    # Placeholder for processed lyrics\n",
    "    processed_lyrics = \"<startSong>\\n\"\n",
    "    last_tag = None\n",
    "\n",
    "    for line in lyrics.split('\\n'):\n",
    "        tag_match = pattern.match(line)\n",
    "        if tag_match:\n",
    "            # Close the previous tag if exists\n",
    "            if last_tag:\n",
    "                processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "            # Update last_tag and open a new tag\n",
    "            last_tag = tag_match.group(1)  # Capture the tag name (Verse or Chorus)\n",
    "            processed_lyrics += f\"<start{last_tag}>\\n\"\n",
    "        else:\n",
    "            processed_lyrics += line + \"\\n\"\n",
    "\n",
    "    # Close the last opened tag\n",
    "    if last_tag:\n",
    "        processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "    processed_lyrics += \"<endSong>\"\n",
    "\n",
    "    return processed_lyrics\n",
    "\n",
    "def remove_remaining_tags(lyrics):\n",
    "    # Pattern to match any text within square brackets\n",
    "    pattern = re.compile(r'\\[[^\\]]*\\]')\n",
    "    cleaned_lyrics = re.sub(pattern, '', lyrics)\n",
    "    return cleaned_lyrics\n",
    "\n",
    "def handle_special(lyrics):\n",
    "    #remove ' and - and ? and !\n",
    "    output_string = re.sub(r\"['\\-?!]\",'', lyrics)\n",
    "    return output_string\n",
    "\n",
    "new_df = df['Lyrics'].apply(insert_tokens)\n",
    "\n",
    "print(new_df)\n",
    "export_data(new_df,'afterInsertTokens',debug)\n",
    "\n",
    "\n",
    "## some songs can have some another tags, like post-chorus,pre etc. \n",
    "## we arent intresnted in those, so we will just delete them in the next step\n",
    "new_df = new_df.apply(remove_remaining_tags)\n",
    "new_df = new_df.apply(handle_special)\n",
    "#lowercase everything\n",
    "new_df = new_df.str.lower()\n",
    "\n",
    "export_data(new_df,'afterLowerAndRemove',debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed characters: \n",
      "Removed characters: á\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ö\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: áó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: óéáí\n",
      "Removed characters: \n",
      "Removed characters: ú\n",
      "Removed characters: \n",
      "Removed characters: â\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: è\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: áâ\n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éàéàéà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éß\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ï\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ç\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n"
     ]
    }
   ],
   "source": [
    "### special chars\n",
    "def remove_non_ascii_and_print(text):\n",
    "    cleaned_text = []\n",
    "    removed_chars = []\n",
    "    for char in text:\n",
    "        if ord(char) < 128:\n",
    "            cleaned_text.append(char)\n",
    "        else:\n",
    "            removed_chars.append(char)\n",
    "    if removed_chars:\n",
    "        print(f\"Removed characters: {''.join(removed_chars)}\")\n",
    "    return ''.join(cleaned_text)\n",
    "clean = new_df.apply(remove_non_ascii_and_print)\n",
    "\n",
    "\n",
    "export_data(clean,'afterRemoveNonAscii',debug)\n",
    "\n",
    "#### we still have (lyrics) in the dataset, need to decide\n",
    "### we still have ! or ? in the dataset, should we remove? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will try several tokenizer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of added tokens: 10\n",
      "\n",
      "0    [startsong, \\n, startintro, \\n, thus, far, ,, ...\n",
      "1    [startsong, \\n, startchorus, \\n, black, magic,...\n",
      "2    [startsong, \\n, startverse, \\n, before, i, che...\n",
      "3    [startsong, \\n, startintro, \\n, yeah, ,, i, m,...\n",
      "4    [startsong, \\n, startintro, \\n, i, do, nt, smi...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, far, ,, this, al...\n",
      "1    [startsong, startchorus, black, magic, ,, nigh...\n",
      "2    [startsong, startverse, before, i, check, the,...\n",
      "3    [startsong, startintro, yeah, ,, im, sorry, (,...\n",
      "4    [startsong, startintro, i, dont, smile, ,, i, ...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, far, ,, this, al...\n",
      "1    [startsong, startchorus, black, magic, ,, nigh...\n",
      "2    [startsong, startverse, before, i, check, the,...\n",
      "3    [startsong, startintro, yeah, ,, im, sorry, (,...\n",
      "4    [startsong, startintro, i, don, ##t, smile, ,,...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, Ġfar, ,, Ġthis, ...\n",
      "1    [startsong, startchorus, black, Ġmagic, ,, Ġni...\n",
      "2    [startsong, startverse, before, Ġi, Ġcheck, Ġt...\n",
      "3    [startsong, startintro, yeah, ,, Ġim, Ġsorry, ...\n",
      "4    [startsong, startintro, i, Ġdont, Ġsmile, ,, Ġ...\n",
      "Name: Lyrics, dtype: object\n"
     ]
    }
   ],
   "source": [
    "temp_switch = {'<startverse>':'startverse',\n",
    "                '<endverse>':'endverse',\n",
    "                '<startchorus>':'startchorus',\n",
    "                '<endchorus>':'endchorus',\n",
    "                '<startoutro>':'startoutro',\n",
    "                '<endoutro>':'endoutro',\n",
    "                '<startintro>':'startintro',\n",
    "                '<endintro>':'endintro',\n",
    "                '<startsong>':'startsong',\n",
    "                '<endsong>':'endsong'}\n",
    "def replace_tags(text, temp_switch, reverse=False):\n",
    "    if reverse:\n",
    "        temp_switch = {v: k for k, v in temp_switch.items()}\n",
    "    \n",
    "    for tag, replacement in temp_switch.items():\n",
    "        text = text.replace(tag, replacement)\n",
    "    return text\n",
    "\n",
    "temp_clean = clean.apply(lambda x: replace_tags(x, temp_switch, reverse=False))\n",
    "export_data(temp_clean,'temp_clean',debug)\n",
    "#pip install spacy\n",
    "import spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens_df = temp_clean.apply(lambda x: [token.text for token in nlp(x)])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "nltk_tokens_df = temp_clean.apply(word_tokenize)\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# For BERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "num_of_added_tokens = tokenizer_bert.add_special_tokens({'additional_special_tokens':\n",
    "                                    ['startverse', 'endverse',\n",
    "                                      'startchorus', 'endchorus',\n",
    "                                        'startoutro', 'endoutro', \n",
    "                                        'startintro', 'endintro',\n",
    "                                        'startsong', 'endsong']})\n",
    "print(f\"Number of added tokens: {num_of_added_tokens}\")\n",
    "BERT_tokens_df = temp_clean.apply(lambda x: tokenizer_bert.tokenize(x))\n",
    "from transformers import GPT2Tokenizer\n",
    "# For GPT-2\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "num_of_added_tokens = tokenizer_gpt2.add_special_tokens({'additional_special_tokens':\n",
    "                                    ['startverse', 'endverse',\n",
    "                                      'startchorus', 'endchorus',\n",
    "                                        'startoutro', 'endoutro', \n",
    "                                        'startintro', 'endintro',\n",
    "                                          'startsong', 'endsong']})\n",
    "gpt2_tokens_df = temp_clean.apply(lambda x: tokenizer_gpt2.tokenize(x))\n",
    "\n",
    "export_data(spacy_tokens_df,'spacy',debug)\n",
    "export_data(nltk_tokens_df,'nltk',debug)\n",
    "export_data(BERT_tokens_df,'BERT',debug)\n",
    "export_data(gpt2_tokens_df,'GPT2', debug )\n",
    "\n",
    "total_token_models = [spacy_tokens_df, nltk_tokens_df, BERT_tokens_df, gpt2_tokens_df]\n",
    "for model in total_token_models:\n",
    "    print(\"\")\n",
    "    print(model.head())\n",
    "\n",
    "## pronouncing library\n",
    "## we will use it to get the phonetic features of the words\n",
    "    \n",
    "\n",
    "import pronouncing\n",
    "\n",
    "def get_phonetic_features(word):\n",
    "    phones = pronouncing.phones_for_word(word)\n",
    "    stress = [pronouncing.stresses(p) for p in phones] if phones else ''\n",
    "    return {'phones': phones, 'stress': stress}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spacy_phonetics_df = spacy_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "nltk_phonetics_df = nltk_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "bert_phonetics_df = BERT_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "gpt2_phonetics_df = gpt2_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "\n",
    "export_data(spacy_phonetics_df, 'spacy_phonetics',debug)\n",
    "export_data(nltk_phonetics_df, 'nltk_phonetics',  debug)\n",
    "export_data(bert_phonetics_df, 'bert_phonetics', debug)\n",
    "export_data(gpt2_phonetics_df, 'gpt2_phonetics', debug)\n",
    "\n",
    "total_phonetic_models = [spacy_phonetics_df, nltk_phonetics_df, bert_phonetics_df, gpt2_phonetics_df]\n",
    "\n",
    "\n",
    "##### now , we need to perpare the data for the embeddings\n",
    "### so padding, indexing unique tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing songs:   4%|▍         | 15/348 [06:19<2:20:34, 25.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m tqdm(spacy_tokens_df, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing songs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     21\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m Sentence(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens))\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mstacked_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     embeddings_cache\u001b[38;5;241m.\u001b[39mappend([token\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convert the list of embeddings to a pandas Series (or DataFrame) aligned with your original DataFrame's index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\token.py:100\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[1;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[0;32m     97\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sentences]\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings:\n\u001b[1;32m--> 100\u001b[0m     \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\embeddings\\token.py:818\u001b[0m, in \u001b[0;36mFlairEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    815\u001b[0m end_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# get hidden states from language model\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_chunk\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune:\n\u001b[0;32m    823\u001b[0m     all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m all_hidden_states_in_lm\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\models\\language_model.py:160\u001b[0m, in \u001b[0;36mLanguageModel.get_representation\u001b[1;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m    159\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 160\u001b[0m     rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     output_parts\u001b[38;5;241m.\u001b[39mappend(rnn_output)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# concatenate all chunks to make final output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\flair\\models\\language_model.py:88\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, input, hidden, ordered_sequence_lengths, decode)\u001b[0m\n\u001b[0;32m     86\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (h,)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm  # tqdm is a library to display progress bars\n",
    "import pandas as pd\n",
    "\n",
    "# Clear GPU memory\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "# Define stacked embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "    WordEmbeddings('glove'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "])\n",
    "\n",
    "# Initialize a list to store embeddings\n",
    "embeddings_cache = []\n",
    "\n",
    "# Process each song's tokens sequentially\n",
    "for tokens in tqdm(spacy_tokens_df, desc=\"Processing songs\"):\n",
    "    sentence = Sentence(' '.join(tokens))\n",
    "    stacked_embeddings.embed(sentence)\n",
    "    embeddings_cache.append([token.embedding for token in sentence])\n",
    "\n",
    "# Convert the list of embeddings to a pandas Series (or DataFrame) aligned with your original DataFrame's index\n",
    "spacy_embed = pd.Series(embeddings_cache, index=spacy_tokens_df.index)\n",
    "\n",
    "print(spacy_embed.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
