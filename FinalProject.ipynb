{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuQT2C68Vpn2"
      },
      "source": [
        "# Nlp final project\n",
        "\n",
        "1. Yaniv Gabay - 205745615 - yanivga@edu.hac.ac.il\n",
        "2. Shahar Asher - 209305408 - shaharas@edu.hac.ac.il\n",
        "3. Hadar Liel Harush - 211721568 - hadarhar@edu.hac.ac.il\n",
        "## Eminem song generator\n",
        "\n",
        "This project will compare different models, all trained on the legendary hip-hop artist Eminem lyrics.\n",
        "We want to expirment with different models, our own model, different apis, to see and compare the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPPafUnTVpn5"
      },
      "source": [
        "to be added?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCisqPafVpn6",
        "outputId": "5be94e9e-081d-4cd4-e6fc-212a3d882fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed non-ASCII characters: \n",
            "Cleaned String: This is a testwith the character.\n"
          ]
        }
      ],
      "source": [
        "# Essential imports for handling arrays, dataframes, and neural network functionalities\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Importing models and tokenizers from the Hugging Face Transformers library\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "\n",
        "# Natural Language Processing (NLP) tools\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Downloading necessary NLTK data, consider running these lines only once\n",
        "nltk.download('wordnet', quiet=True)  # WordNet Lemmatizer\n",
        "nltk.download('punkt', quiet=True)    # Punkt Tokenizer models\n",
        "\n",
        "# Our utilities file\n",
        "import utilities as utils\n",
        "# Optional: Uncomment the following line for CUDA debugging\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjGe5cXbVpn8",
        "outputId": "383ebadf-e7b7-4e72-d4d0-1be91988abd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with encoding utf-8: 'utf-8' codec can't decode byte 0x92 in position 6: invalid start byte\n",
            "Success with encoding: latin1\n"
          ]
        }
      ],
      "source": [
        "##### Loading the data\n",
        "### we be using <startVerse> <endVerse> <startChorus><endChorus>\n",
        "## to give our model some different between the verses and the choruses.\n",
        "\n",
        "\n",
        "##### you can change this to export the data\n",
        "##### through the process\n",
        "debug = False\n",
        "file_loc = './Eminem_Lyrics.csv'\n",
        "df = utils.import_data_from_location(file_loc)\n",
        "\n",
        "utils.export_data_to_csv(df,'afterImport-1',debug)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BvNYgGsYVpn9"
      },
      "outputs": [],
      "source": [
        "#actual messing with the dataset\n",
        "import re\n",
        "\n",
        "\n",
        "def insert_tokens(lyrics):\n",
        "\n",
        "    \"\"\"\n",
        "    Inserts special tokens at the start and end of each verse and chorus in the lyrics.\n",
        "\n",
        "    Parameters:\n",
        "        lyrics (str): The lyrics of a song.\n",
        "\n",
        "    Returns:\n",
        "        str: The processed lyrics with inserted tokens.\n",
        "    \"\"\"\n",
        "    # Pattern to identify all tags (verses, choruses, etc.)\n",
        "    pattern = re.compile(r'\\[(Verse|Chorus|Intro|Outro)')\n",
        "\n",
        "    # Placeholder for processed lyrics\n",
        "    processed_lyrics = \"<startSong>\\n\"\n",
        "    last_tag = None\n",
        "\n",
        "    for line in lyrics.split('\\n'):\n",
        "        tag_match = pattern.match(line)\n",
        "        if tag_match:\n",
        "            # Close the previous tag if exists\n",
        "            if last_tag:\n",
        "                processed_lyrics += f\"<end{last_tag}>\\n\"\n",
        "            # Update last_tag and open a new tag\n",
        "            last_tag = tag_match.group(1)  # Capture the tag name (Verse or Chorus)\n",
        "            processed_lyrics += f\"<start{last_tag}>\\n\"\n",
        "        else:\n",
        "            processed_lyrics += line + \"\\n\"\n",
        "\n",
        "    # Close the last opened tag\n",
        "    if last_tag:\n",
        "        processed_lyrics += f\"<end{last_tag}>\\n\"\n",
        "    processed_lyrics += \"<endSong>\"\n",
        "\n",
        "    return processed_lyrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## some songs can have some another tags, like post-chorus,pre etc.\n",
        "## we arent intresnted in those, so we will just delete them in the next step\n",
        "new_df = df['Lyrics'].apply(insert_tokens)\n",
        "new_df = new_df.apply(utils.remove_tags)\n",
        "new_df = new_df.apply(utils.handle_special_characters)\n",
        "#lowercase everything\n",
        "new_df = new_df.str.lower()\n",
        "\n",
        "utils.export_data_to_csv(new_df,'afterLowerAndRemoveSpecialChars!',True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RumB51IjVpn9",
        "outputId": "6aacee24-35f6-4bb4-cf07-3dc4266c5584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: á\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ó\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ö\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: éâ\n",
            "Removed non-ASCII characters: ó\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: óá\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: íéóá\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ú\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: â\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: è\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ä\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: âá\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ä\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: éà\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: éß\n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ü\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ï\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ü\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: éâ\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ü\n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: ç\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: é\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: éà\n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n",
            "Removed non-ASCII characters: \n"
          ]
        }
      ],
      "source": [
        "\n",
        "clean = new_df.apply(utils.remove_non_ascii_characters)\n",
        "clean = clean.apply(utils.expand_contractions, args=(utils.contractions_dict,))\n",
        "\n",
        "utils.export_data_to_csv(clean,'afterRemoveNonAscii',True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iGX2k2ICIzXm"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH6qzRcQVpn-"
      },
      "source": [
        "## we will try several tokenizer options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxv0eFbxVpn-",
        "outputId": "cd588283-4880-4565-f33b-39ec0a6b43b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "temp_switch = {'<startverse>':'startverse',\n",
        "                '<endverse>':'endverse',\n",
        "                '<startchorus>':'startchorus',\n",
        "                '<endchorus>':'endchorus',\n",
        "                '<startoutro>':'startoutro',\n",
        "                '<endoutro>':'endoutro',\n",
        "                '<startintro>':'startintro',\n",
        "                '<endintro>':'endintro',\n",
        "                '<startsong>':'startsong',\n",
        "                '<endsong>':'endsong'}\n",
        "def replace_tags(text, temp_switch, reverse=False):\n",
        "    if reverse:\n",
        "        temp_switch = {v: k for k, v in temp_switch.items()}\n",
        "\n",
        "    for tag, replacement in temp_switch.items():\n",
        "        text = text.replace(tag, replacement)\n",
        "    return text\n",
        "\n",
        "temp_clean = clean.apply(lambda x: replace_tags(x, temp_switch, reverse=False))\n",
        "utils.export_data_to_csv(temp_clean,'temp_clean',debug)\n",
        "!pip install spacy\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "spacy_tokens_df = temp_clean.apply(lambda x: [token.text for token in nlp(x)])\n",
        "\n",
        "utils.export_data_to_csv(spacy_tokens_df,'LATESTSPACETOKENS',True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ijncnMVQjCfo"
      },
      "outputs": [],
      "source": [
        "def build_vocab(series):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary dictionary mapping each unique token to a unique index.\n",
        "\n",
        "    Special tokens are predefined with specific indices. This function iterates through\n",
        "    each song in the provided series, updating the set of unique tokens. It then assigns\n",
        "    an index to each token, starting with indices following the special tokens.\n",
        "\n",
        "    Parameters:\n",
        "        series (pd.Series): A pandas series containing lists of tokenized lyrics of songs.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping tokens to their respective indices in the vocabulary.\n",
        "    \"\"\"\n",
        "    unique_tokens = set()\n",
        "    for tokens_list in series:\n",
        "        unique_tokens.update(tokens_list)\n",
        "\n",
        "    token_to_idx = {\n",
        "        '<PAD>': 0,\n",
        "        '<startsong>': 1,\n",
        "        '<endsong>': 2,\n",
        "        '<startverse>': 3,\n",
        "        '<endverse>': 4,\n",
        "        '<startchorus>': 5,\n",
        "        '<endchorus>': 6,\n",
        "        '<startintro>': 7,\n",
        "        '<endintro>': 8\n",
        "    }\n",
        "\n",
        "    # Continue indexing from the number after the special tokens\n",
        "    for token in unique_tokens:\n",
        "        if token not in token_to_idx:\n",
        "            token_to_idx[token] = len(token_to_idx)\n",
        "\n",
        "    return token_to_idx\n",
        "\n",
        "\n",
        "\n",
        "def tokens_to_indices(tokenized_song, token_to_idx):\n",
        "    \"\"\"\n",
        "    Converts a list of tokens from a song to their corresponding indices based on\n",
        "    a given vocabulary mapping.\n",
        "\n",
        "    If a token is not found in the mapping, it is replaced by the index of the '<PAD>'\n",
        "    token, serving as a placeholder for unknown tokens.\n",
        "\n",
        "    Parameters:\n",
        "        tokenized_song (list of str): A list containing the tokens of a song.\n",
        "        token_to_idx (dict): A dictionary mapping tokens to their respective indices.\n",
        "\n",
        "    Returns:\n",
        "        list of int: A list of indices corresponding to the tokens of the input song.\n",
        "    \"\"\"\n",
        "    # Convert each token to its index, defaulting to the '<PAD>' token's index if not found\n",
        "    return [token_to_idx.get(token, token_to_idx['<PAD>']) for token in tokenized_song]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gGi9rUE9Vpn_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class TokenSongsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class for sequences of tokens from song lyrics.\n",
        "\n",
        "    This class prepares sequences of a specified length from the tokenized lyrics,\n",
        "    suitable for training models on sequence prediction tasks, such as language modeling.\n",
        "\n",
        "    Parameters:\n",
        "        series (pd.Series): A pandas series containing lists of tokenized lyrics.\n",
        "        seq_length (int): The length of the sequences to be prepared for the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, series, seq_length):\n",
        "        self.series = series\n",
        "        self.seq_length = seq_length\n",
        "        self.tokens = self.prepare_tokens()\n",
        "\n",
        "    def prepare_tokens(self):\n",
        "        \"\"\"\n",
        "        Flattens all token lists from the series into a single list.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of all tokens from the series combined.\n",
        "        \"\"\"\n",
        "        # Efficient flattening using itertools.chain\n",
        "        from itertools import chain\n",
        "        return list(chain.from_iterable(self.series))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a sequence of tokens and its corresponding target sequence from the dataset.\n",
        "\n",
        "        The target sequence is the original sequence shifted by one token to the right.\n",
        "\n",
        "        Parameters:\n",
        "            idx (int): The start index of the sequence.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the input sequence and target sequence tensors.\n",
        "        \"\"\"\n",
        "        # Ensure the sequence does not exceed the tokens list's bounds\n",
        "        end_idx = min(idx + self.seq_length, len(self.tokens) - 1)\n",
        "\n",
        "        # Sequence from 'idx' to 'end_idx'\n",
        "        seq = self.tokens[idx:end_idx]\n",
        "        # Target sequence is offset by one token\n",
        "        target_seq = self.tokens[idx + 1:end_idx + 1]\n",
        "\n",
        "        # Padding for sequences shorter than 'seq_length' at the end of the tokens list\n",
        "        seq += [0] * (self.seq_length - len(seq))  # Assuming 0 is the index for '<PAD>'\n",
        "        target_seq += [0] * (self.seq_length - len(target_seq))  # Assuming 0 is the index for '<PAD>'\n",
        "\n",
        "        # Conversion to tensors\n",
        "        seq_tensor = torch.tensor(seq, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target_seq, dtype=torch.long)\n",
        "\n",
        "        return seq_tensor, target_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the adjusted length of the dataset accounting for the sequence length.\n",
        "\n",
        "        This adjustment ensures that each sequence has a corresponding target sequence.\n",
        "\n",
        "        Returns:\n",
        "            int: The adjusted length of the dataset.\n",
        "        \"\"\"\n",
        "        # Adjust length to ensure there's a target for each input sequence\n",
        "        return max(0, len(self.tokens) - self.seq_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80UVl8g2Vpn_",
        "outputId": "1745f718-c63d-45d4-8b6b-351d57549209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Song 225: [1, 7, 9060, 12657, 6677, 8, 5, 8112, 4444, 215, 2814, 8084, 12657, 5257, 11758, 948, 6946, 8084, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 8112, 4444, 215, 8732, 8084, 12657, 215, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 5257, 2003, 948, 6946, 8084, 12657, 8084, 3839, 12361, 6359, 12526, 3512, 11326, 5373, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 8112, 4444, 215, 8732, 8084, 12657, 215, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 6, 3, 8112, 4444, 1158, 10628, 12657, 1158, 3839, 3726, 5923, 4158, 13299, 1158, 3839, 10741, 12176, 8770, 3512, 8112, 13571, 6620, 2155, 12657, 5257, 5455, 2217, 4367, 8112, 12129, 12657, 4015, 5257, 2003, 948, 5455, 5257, 2645, 5923, 10798, 12469, 2542, 1152, 12657, 7356, 12657, 8741, 1167, 12657, 1167, 12657, 1167, 12585, 2003, 215, 5558, 12657, 1152, 3572, 12657, 5257, 4069, 5923, 4714, 6495, 12657, 5257, 77, 11269, 5257, 4069, 948, 7587, 12657, 948, 12226, 5257, 4069, 2409, 702, 5257, 4069, 948, 9795, 12421, 12657, 5257, 4069, 1962, 3368, 6717, 7614, 12657, 5257, 4069, 3131, 10175, 12702, 6803, 5257, 11758, 948, 7602, 10175, 8112, 2114, 7006, 1470, 12657, 8084, 3839, 11244, 12602, 4468, 9624, 2003, 948, 4371, 4468, 3726, 5923, 12595, 12657, 5257, 9234, 3726, 3512, 3628, 3628, 12657, 3628, 12657, 6687, 12657, 5257, 3384, 6115, 9624, 10025, 5257, 690, 1757, 12657, 8084, 11324, 900, 5923, 4638, 12657, 5923, 9682, 4638, 4638, 4638, 9682, 10060, 12657, 5257, 108, 12657, 5257, 108, 12657, 5257, 1950, 12657, 5257, 1950, 4135, 5257, 8601, 12657, 5257, 4069, 11408, 12657, 3572, 12657, 3572, 12657, 5431, 12602, 5257, 4069, 2405, 8112, 6068, 5622, 12657, 5257, 4069, 5923, 4838, 6068, 5257, 11758, 7602, 2947, 8610, 2398, 5257, 8298, 9624, 215, 10820, 12398, 8084, 4, 5, 8112, 4444, 215, 2814, 8084, 12657, 5257, 11758, 948, 6946, 8084, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 8112, 4444, 215, 8732, 8084, 12657, 215, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 4140, 10540, 9396, 7356, 9192, 4278, 7180, 12657, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 7518, 11122, 5257, 4046, 8112, 8423, 10175, 8302, 12657, 12702, 3628, 3628, 5880, 8741, 1167, 12657, 1167, 12657, 1167, 6, 3, 9205, 6420, 9624, 13831, 3361, 3512, 11326, 9192, 2212, 2702, 3476, 10969, 5805, 12657, 3476, 8169, 4086, 3512, 3720, 5609, 10777, 5257, 11324, 3512, 8112, 2369, 9624, 13196, 6398, 8610, 12702, 13796, 12469, 2542, 1152, 12657, 7356, 12657, 8741, 1167, 12657, 1167, 12657, 1167, 12585, 2003, 215, 5558, 12657, 1152, 3198, 12657, 5257, 9234, 9016, 12702, 8288, 5257, 77, 10247, 12657, 8434, 12657, 5257, 5162, 8084, 2947, 12657, 12702, 9560, 13771, 3572, 2003, 948, 4487, 4468, 7856, 5754, 5029, 10351, 12657, 3572, 5257, 1715, 948, 6198, 7602, 3512, 4046, 9205, 6420, 13705, 13204, 5257, 1715, 948, 5558, 3512, 11326, 8304, 11573, 3384, 5923, 2366, 12657, 6346, 13204, 9624, 2003, 948, 4371, 4468, 3726, 5257, 4069, 3628, 4787, 332, 12657, 5754, 2369, 5366, 3368, 6717, 7614, 12657, 5257, 10341, 3512, 4046, 12702, 4970, 5257, 10820, 11106, 10626, 10074, 12657, 1348, 5923, 12634, 12657, 9624, 11326, 8454, 8454, 12657, 6687, 12657, 8454, 12657, 327, 8012, 10175, 8112, 7288, 12585, 2003, 215, 5558, 12702, 11255, 8610, 6240, 12657, 8885, 3839, 6359, 10894, 5257, 4069, 2405, 12657, 2013, 13818, 2364, 11470, 3726, 5923, 6684, 9624, 5257, 11758, 8298, 12176, 9624, 215, 8338, 13289, 10246, 2758, 4334, 5923, 6068, 10175, 12702, 4981, 12920, 4, 5, 8112, 4444, 215, 2814, 8084, 12657, 5257, 11758, 948, 6946, 8084, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 8112, 4444, 215, 8732, 8084, 12657, 215, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 3009, 11899, 3839, 948, 5655, 7451, 5038, 12657, 9192, 5257, 10303, 13516, 11031, 1917, 5923, 5213, 3726, 12176, 11345, 7524, 6117, 12657, 3661, 8298, 8770, 9624, 1183, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 6, 3, 8112, 4444, 1158, 10628, 12657, 1158, 9888, 3726, 5923, 11324, 11324, 2673, 12176, 8266, 12657, 1158, 5507, 12657, 7602, 7776, 12657, 215, 6499, 5257, 13421, 5923, 13571, 7863, 12657, 3468, 12657, 12702, 2451, 3839, 10010, 12469, 2542, 1152, 12657, 7356, 12657, 8741, 1167, 12657, 1167, 12657, 1167, 12585, 2003, 215, 5558, 12657, 1152, 12702, 4943, 10820, 11326, 8506, 9624, 12702, 3404, 3512, 12702, 10085, 12657, 5257, 9234, 880, 9115, 3368, 6717, 7614, 12657, 10189, 12657, 9319, 5923, 13113, 4084, 5754, 3326, 12657, 5257, 3384, 1571, 9445, 12657, 9319, 5923, 405, 4084, 5754, 2771, 3839, 2737, 7856, 4468, 12657, 6521, 12657, 948, 6198, 4468, 5257, 2003, 948, 6198, 7838, 4135, 5257, 11758, 8298, 8112, 532, 3628, 3628, 3628, 12657, 10175, 8112, 7902, 12657, 4015, 5257, 5455, 5257, 1715, 3177, 12657, 3839, 12176, 5923, 943, 5257, 5455, 5257, 9234, 8588, 5923, 5960, 10060, 12657, 5257, 108, 12657, 5257, 108, 12657, 5257, 1950, 12657, 5257, 1950, 5257, 2003, 948, 5455, 12702, 108, 3839, 9530, 12657, 5257, 13104, 6909, 2027, 7602, 3512, 8112, 3399, 12657, 7666, 7602, 7776, 5257, 4069, 948, 2405, 12657, 5257, 4069, 8377, 12657, 7602, 6398, 4, 5, 8112, 4444, 215, 2814, 8084, 12657, 5257, 11758, 948, 6946, 8084, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 8112, 4444, 215, 8732, 8084, 12657, 215, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 9192, 5304, 1990, 12657, 10820, 215, 3628, 3628, 10175, 4468, 12657, 3572, 5257, 10303, 13516, 11031, 1917, 12243, 8770, 3726, 12176, 10246, 8112, 4444, 215, 8732, 8084, 12657, 215, 3384, 12702, 3628, 3628, 11324, 8741, 1167, 12657, 1167, 12657, 1167, 6, 3918, 10902, 9853, 3839, 4408, 8610, 215, 9794, 11886, 2]\n",
            "...\n",
            "\n",
            "Song 329: [1, 5257, 5748, 8112, 5853, 862, 3960, 5257, 1787, 7172, 8084, 8012, 8298, 8084, 10175, 4253, 12657, 9624, 9707, 8084, 12657, 9684, 8084, 9192, 12176, 862, 3960, 5257, 6631, 1978, 8084, 8012, 3476, 13419, 4468, 4069, 5257, 11086, 12657, 3476, 13419, 4468, 4135, 5257, 4069, 11224, 8169, 3476, 10681, 4468, 12176, 6535, 4084, 8112, 10644, 12176, 4334, 4281, 60, 4367, 4468, 5754, 3839, 5923, 4154, 8610, 6521, 6364, 4367, 10742, 12657, 8084, 3839, 5923, 10451, 11090, 4259, 8112, 8413, 12657, 8084, 3839, 11074, 5906, 2961, 12702, 5513, 3839, 3169, 2565, 8112, 8490, 9192, 9515, 3839, 12702, 4401, 3388, 6020, 2673, 5923, 8511, 8610, 5923, 9864, 3388, 12702, 2451, 3839, 8710, 1981, 12657, 5257, 4069, 3888, 4084, 3726, 5923, 13658, 6671, 7888, 12657, 12702, 4746, 3169, 7825, 215, 11758, 948, 1059, 9624, 215, 8169, 714, 2426, 5257, 10176, 2673, 5754, 3773, 8610, 12657, 11906, 12523, 9624, 5257, 10303, 13516, 9349, 12657, 948, 6198, 12243, 11419, 6973, 58, 8287, 10175, 12702, 609, 5257, 6592, 12657, 9429, 2426, 5455, 5257, 77, 5923, 175, 5257, 4069, 11462, 3512, 6886, 8112, 8287, 861, 2426, 12702, 2522, 8770, 9016, 12702, 8890, 6699, 8012, 12657, 9016, 12702, 11731, 2426, 5257, 4069, 8242, 7092, 13901, 2426, 10175, 5754, 11149, 2702, 12657, 5257, 4069, 8242, 7092, 3063, 4968, 6684, 12657, 5257, 7587, 5748, 215, 2427, 3445, 3512, 10239, 2426, 4015, 215, 9234, 10820, 948, 7319, 10820, 12873, 10215, 12657, 5257, 8814, 948, 2]\n",
            "...\n",
            "\n",
            "Song 217: [1, 4163, 7598, 12657, 8084, 3839, 7248, 5257, 9016, 8813, 3422, 13963, 8112, 6240, 4172, 10966, 9624, 5257, 7838, 12176, 8885, 3839, 948, 9743, 13963, 8112, 8266, 12585, 12657, 9140, 8885, 3132, 11326, 2129, 7134, 12161, 12657, 7769, 12657, 3661, 8423, 11860, 9269, 4334, 8857, 11906, 13507, 7598, 12657, 8112, 4444, 215, 3384, 4468, 4983, 8610, 1792, 3142, 215, 3548, 8857, 9319, 5923, 8434, 4367, 8112, 6684, 2673, 8112, 9781, 9624, 7722, 11860, 3512, 4154, 8084, 12657, 6535, 5257, 10820, 948, 10708, 12226, 5257, 7602, 5038, 2003, 215, 7131, 8112, 11122, 6958, 6958, 8169, 837, 8112, 4038, 11423, 8266, 7769, 12657, 7485, 12460, 11326, 12176, 6240, 3512, 4468, 9624, 8084, 9140, 948, 1191, 4135, 8885, 3839, 11638, 11573, 996, 6535, 5257, 11758, 948, 10708, 4811, 11860, 9624, 5257, 12120, 7594, 12360, 12657, 9624, 5257, 1785, 3476, 2010, 8032, 7533, 8274, 2003, 215, 3726, 12176, 9624, 5257, 3726, 11860, 12657, 5257, 3726, 11860, 5923, 7970, 5257, 3132, 716, 11860, 12657, 4015, 5257, 11758, 948, 12186, 10182, 10921, 4468, 12657, 5257, 4069, 13488, 5923, 10644, 12657, 11408, 6140, 12657, 3422, 4468, 8012, 5257, 77, 5754, 4148, 13963, 8274, 5257, 12473, 3512, 12688, 8112, 8423, 9192, 12657, 631, 4468, 7825, 215, 7602, 5754, 10518, 12657, 13469, 7356, 12657, 9624, 2565, 8112, 4444, 12657, 6521, 12657, 5257, 2003, 948, 77, 5923, 13571, 3938, 3938, 8427, 9624, 3072, 5343, 7776, 8739, 12591, 8084, 2]\n",
            "...\n",
            "\n",
            "Song 110: [1, 7, 12820, 12657, 5257, 7838, 5257, 9761, 215, 6398, 9624, 5924, 215, 8298, 8112, 6310, 8169, 11224, 9515, 3839, 8112, 3303, 8878, 9624, 5257, 4069, 557, 2426, 9624, 12657, 12820, 12657, 5257, 7838, 8885, 3839, 948, 4014, 4015, 2003, 948, 215, 10379, 8112, 10477, 10175, 4468, 4156, 215, 684, 12490, 10418, 4887, 12657, 7139, 8, 5, 5257, 8814, 6958, 8169, 9269, 6958, 8169, 3842, 6546, 2673, 8112, 1040, 8252, 12657, 5257, 4318, 10175, 11345, 6958, 2544, 5754, 6359, 7221, 6, 3, 5257, 12154, 2673, 7037, 12657, 11031, 381, 13963, 9269, 12585, 5257, 31, 11770, 2673, 12585, 11479, 12657, 12702, 12820, 3820, 9016, 8084, 8112, 4879, 8112, 9144, 4084, 8084, 12657, 4015, 4156, 7586, 4156, 6958, 8169, 12657, 1715, 5257, 9319, 8084, 6359, 7221, 7675, 7776, 12702, 11837, 9624, 2010, 13893, 9427, 10786, 4015, 1900, 12657, 5257, 2003, 948, 12120, 215, 10246, 12657, 11726, 215, 8169, 714, 378, 3512, 4468, 12657, 10246, 215, 8169, 12702, 12820, 5924, 7221, 11326, 8084, 7856, 215, 3512, 11326, 3095, 8366, 9821, 9374, 9919, 12657, 10120, 4681, 9624, 10736, 4084, 11054, 5162, 11399, 6631, 7962, 12243, 7573, 12183, 7649, 3512, 9155, 13143, 9624, 354, 6958, 6631, 12551, 5754, 10175, 9624, 10175, 4015, 8116, 3512, 6705, 12657, 12176, 6212, 12602, 4468, 2426, 13337, 8112, 5701, 3757, 2003, 948, 5558, 10644, 3512, 4468, 215, 8169, 7362, 4468, 7776, 8084, 3839, 10272, 5716, 9624, 8084, 3839, 5701, 4872, 12657, 2999, 2695, 12657, 9234, 6039, 11726, 12657, 9761, 4468, 9326, 12702, 8032, 9355, 2398, 3512, 77, 2710, 6751, 2514, 9515, 6958, 12460, 4367, 2710, 6751, 3412, 2566, 7825, 2541, 12657, 8885, 557, 11054, 10736, 6958, 8169, 2673, 8112, 12496, 13318, 7826, 215, 8741, 5455, 12176, 8741, 3384, 11054, 6455, 12139, 1470, 2947, 8084, 8260, 11054, 12657, 4015, 11399, 12657, 3842, 5045, 9624, 5923, 5903, 10353, 4084, 5813, 12657, 714, 9016, 5923, 1033, 3512, 11324, 8012, 3512, 8216, 9821, 12657, 8084, 3839, 4308, 2426, 8112, 11081, 9624, 5257, 9374, 8112, 6684, 4084, 8112, 9821, 12657, 8112, 4412, 9192, 12702, 11470, 9959, 8112, 6413, 4084, 8112, 12026, 58, 9490, 9016, 5557, 2947, 2565, 8112, 7696, 4367, 13800, 3126, 7451, 9624, 12176, 4334, 7825, 5257, 10545, 215, 6958, 8169, 4087, 9624, 8084, 9374, 948, 10739, 11573, 9109, 9624, 3512, 5754, 3960, 6958, 1020, 5126, 12657, 9624, 5257, 12120, 8084, 5924, 12657, 4015, 4, 5, 5257, 8814, 6958, 8169, 9269, 6958, 8169, 3842, 6546, 2673, 8112, 1040, 8252, 12657, 5257, 4318, 10175, 11345, 6958, 2544, 5754, 6359, 7221, 6, 3, 10246, 3512, 5754, 3960, 6958, 1020, 5126, 12657, 9624, 5257, 12120, 8084, 5924, 10246, 215, 10303, 13516, 6198, 7602, 3512, 11772, 8813, 2510, 3720, 4015, 5257, 4069, 11408, 12657, 13020, 12657, 12602, 7675, 7776, 12702, 11837, 4367, 8112, 11122, 5257, 9374, 8598, 12657, 3984, 11345, 9192, 11031, 10469, 12176, 7221, 3512, 9319, 8084, 5924, 10246, 2702, 5257, 7838, 8084, 3839, 948, 8813, 3210, 12657, 9624, 5257, 4069, 948, 11210, 7273, 12176, 1757, 5257, 6521, 1941, 1978, 4367, 9337, 9624, 5257, 2606, 7518, 11122, 8084, 3839, 10175, 8112, 7902, 9624, 5257, 5455, 4084, 2974, 6495, 3493, 2673, 5923, 7746, 9624, 2010, 8112, 3723, 215, 8807, 11054, 9624, 8274, 5257, 9234, 11894, 215, 3512, 13932, 8813, 507, 4015, 2702, 8112, 4052, 12442, 2738, 9624, 8813, 8696, 6707, 10178, 1395, 9624, 5257, 4069, 4444, 6359, 7451, 3512, 5197, 12657, 5754, 10644, 3839, 7309, 5924, 4015, 12657, 11726, 12657, 5257, 5431, 215, 12657, 9192, 9140, 2974, 12657, 11739, 2010, 215, 1715, 12657, 2010, 215, 31, 12657, 215, 1715, 8813, 7390, 3512, 11344, 11054, 10736, 2882, 4316, 12657, 12176, 6173, 215, 1780, 12657, 3438, 6301, 11326, 4156, 9566, 4156, 348, 4015, 5257, 12398, 215, 12657, 5572, 13621, 7356, 12657, 12585, 5923, 9918, 4386, 6958, 77, 10246, 862, 10966, 5257, 11031, 2530, 9374, 12489, 8112, 9853, 12702, 7449, 2541, 9374, 9853, 8084, 12657, 5257, 8814, 8885, 7975, 9018, 11880, 2426, 8610, 7518, 1336, 4015, 4205, 8660, 7518, 10255, 12657, 7518, 10218, 9624, 10120, 3543, 9723, 5923, 2287, 4084, 1467, 9624, 6641, 12702, 4143, 3512, 8112, 7129, 4084, 8112, 6582, 4135, 9584, 13857, 127, 7248, 7856, 4468, 12176, 215, 11292, 6416, 8813, 678, 4135, 5257, 7975, 3512, 8828, 6398, 8112, 11207, 12657, 11222, 4156, 1458, 12657, 4692, 7248, 9624, 10364, 862, 2364, 5385, 617, 2520, 6669, 2427, 215, 3625, 2426, 2673, 8366, 4318, 862, 8252, 4156, 6958, 6958, 8169, 7791, 3512, 7602, 5606, 4991, 4468, 12657, 10728, 9624, 9490, 12657, 6958, 11180, 215, 12657, 6625, 215, 9624, 4156, 215, 212, 5257, 7975, 5754, 7380, 9094, 8828, 2738, 4468, 4156, 6958, 3625, 8878, 3512, 11324, 8366, 10852, 12940, 9624, 5257, 8588, 8813, 3842, 4156, 5257, 13752, 8012, 9624, 5257, 4069, 3294, 5257, 1715, 948, 7602, 8112, 7507, 3512, 1174, 215, 12602, 6495, 12702, 12820, 9624, 12702, 2541, 9192, 12657, 12820, 12657, 3572, 7954, 5754, 4156, 5923, 5526, 5257, 7119, 5754, 10175, 8112, 10363, 12657, 5257, 8814, 5257, 7975, 3512, 7602, 5754, 8878, 12702, 4912, 12657, 5257, 1785, 5257, 7602, 8112, 7507, 3512, 9707, 8084, 5874, 5257, 4069, 7327, 12657, 8112, 9724, 31, 3512, 6200, 12702, 13890, 12657, 5257, 8814, 6958, 8169, 11922, 9192, 12657, 4135, 5257, 4069, 948, 8928, 12657, 5257, 1785, 215, 7602, 5754, 10518, 12176, 5257, 10820, 12460, 12398, 215, 7856, 4033, 12657, 10246, 215, 8169, 12702, 12820, 4, 5, 5257, 8814, 6958, 8169, 9269, 6958, 8169, 3842, 6546, 2673, 8112, 1040, 8252, 12657, 5257, 4318, 10175, 11345, 6958, 2544, 5754, 6359, 7221, 5257, 12473, 5923, 13571, 4859, 11697, 2738, 862, 4914, 5923, 10246, 4085, 7237, 9192, 5257, 4069, 6335, 7746, 6740, 7139, 7769, 12657, 6521, 1191, 12585, 8112, 13028, 9624, 4135, 8112, 2483, 5880, 6398, 11573, 4135, 8112, 3485, 11758, 948, 9429, 4468, 2426, 7769, 12657, 9234, 7838, 12176, 5257, 4069, 3589, 5257, 9374, 948, 5492, 3512, 7533, 7356, 12657, 6198, 4135, 2758, 4334, 10786, 3512, 690, 7769, 12657, 12702, 11261, 10820, 3672, 4468, 9234, 7838, 12176, 5257, 4069, 3589, 5257, 9374, 948, 5492, 3512, 7533, 6535, 5257, 5162, 12702, 11558, 2673, 12702, 2999, 8110, 9192, 3661, 11031, 8298, 8043, 11871, 5853, 9234, 7838, 12176, 5257, 4069, 3589, 5257, 4069, 948, 5492, 3512, 7533, 6, 5, 5257, 8814, 6958, 8169, 9269, 6958, 8169, 3842, 6546, 2673, 8112, 1040, 8252, 12657, 5257, 4318, 10175, 11345, 6958, 2544, 5754, 6359, 7221, 5257, 12473, 5923, 13571, 4859, 6, 2]\n",
            "...\n",
            "\n",
            "Song 243: [1, 7, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 6677, 12657, 8813, 12485, 13384, 215, 9624, 5257, 2645, 5923, 12460, 11326, 3326, 12602, 215, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 6521, 1191, 12585, 12158, 215, 8169, 2010, 5257, 9016, 2673, 5754, 5853, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 5257, 1787, 11031, 7896, 215, 2426, 12602, 7426, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 2771, 2673, 5754, 5853, 3839, 13857, 8242, 7092, 12700, 215, 7856, 4468, 5257, 12398, 215, 8, 3, 4302, 9788, 9788, 12657, 6958, 12994, 3512, 8112, 7892, 9326, 5923, 715, 4084, 11467, 9624, 9761, 12485, 5870, 215, 2673, 8112, 5903, 8288, 7356, 12657, 12489, 4334, 6598, 1158, 3839, 12442, 5923, 2999, 10413, 2673, 8112, 9805, 7356, 12657, 12176, 9973, 12485, 13104, 5811, 9628, 2738, 5923, 1388, 2702, 12657, 5257, 7838, 12585, 215, 8169, 381, 12657, 8084, 3839, 3888, 4084, 12953, 3512, 11324, 13745, 4015, 215, 7838, 8813, 6598, 12657, 1158, 3839, 862, 4084, 13742, 5312, 4084, 11009, 12176, 2003, 8720, 878, 12657, 9624, 4135, 1158, 2003, 948, 7602, 10728, 4444, 12657, 1183, 1278, 5923, 9051, 2003, 948, 1978, 8610, 9641, 6665, 10802, 12657, 13321, 12657, 9761, 11324, 4084, 8084, 6521, 9624, 2003, 948, 8434, 9192, 11400, 12657, 9515, 215, 4868, 7819, 2003, 948, 215, 3132, 9517, 12485, 4388, 5923, 3816, 7139, 9624, 6598, 31, 1158, 5654, 3512, 8423, 215, 8274, 7221, 1158, 11758, 13907, 9624, 2003, 948, 8049, 13963, 12176, 2999, 4928, 4928, 10175, 10728, 1830, 8084, 3839, 9234, 5923, 2999, 10402, 12657, 8084, 2003, 948, 11770, 10728, 9374, 11542, 4142, 12638, 215, 6958, 8169, 9971, 9624, 9847, 10227, 10175, 10728, 12540, 10737, 7943, 12657, 10303, 13516, 1158, 7769, 9761, 10728, 12025, 8878, 2673, 8112, 3023, 9624, 4468, 9624, 215, 11758, 1978, 2565, 13517, 12657, 11758, 948, 6958, 4, 5, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9624, 7825, 6958, 7800, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 215, 9624, 5257, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9624, 7825, 6958, 7800, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 215, 9624, 5257, 4046, 12657, 13321, 6, 3, 2758, 4334, 5923, 10379, 823, 9476, 9624, 5923, 10379, 823, 7485, 5923, 10379, 823, 3559, 9624, 5923, 10379, 823, 13238, 9624, 9641, 3820, 10175, 9574, 4444, 3512, 2010, 4084, 7248, 12476, 862, 10246, 10737, 9016, 5923, 13571, 11378, 9624, 5923, 10879, 9624, 215, 2003, 948, 12473, 5923, 2247, 12657, 2003, 12873, 10215, 11345, 7825, 215, 8169, 7451, 5038, 3512, 13317, 5923, 2999, 222, 12657, 3661, 12392, 8084, 3512, 12873, 4015, 12602, 2702, 12657, 7769, 9234, 8298, 6598, 9374, 13574, 12657, 13574, 11906, 9374, 575, 5558, 3512, 2541, 9624, 6426, 11860, 13574, 12657, 13574, 3294, 4015, 5257, 714, 4983, 3398, 12176, 5257, 5162, 10728, 10175, 11122, 7776, 13196, 8012, 2673, 8813, 6112, 12657, 13321, 12657, 5985, 2167, 824, 7776, 5257, 5748, 215, 8084, 3839, 11086, 12657, 9788, 9788, 12657, 12473, 5923, 13201, 13201, 9319, 5923, 8252, 8252, 7092, 7092, 4928, 12657, 9196, 9196, 13920, 13920, 10728, 3384, 2186, 2186, 7383, 7383, 12485, 8356, 8813, 12491, 4085, 8112, 6677, 2426, 9192, 10728, 11758, 9319, 5923, 22, 22, 8813, 9189, 9429, 10728, 2426, 4156, 4911, 4156, 6958, 7602, 3512, 8112, 3023, 11562, 1232, 9624, 2684, 12657, 4468, 9624, 12702, 9321, 4, 5, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9624, 7825, 6958, 7800, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 215, 9624, 5257, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9624, 7825, 6958, 7800, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 215, 9624, 5257, 6, 3, 9429, 2426, 12657, 13387, 12657, 6958, 8169, 3326, 12657, 5039, 6958, 5339, 6958, 8169, 8242, 7092, 9319, 6598, 12602, 5923, 11216, 1034, 7917, 8112, 8127, 6677, 12657, 2003, 948, 5197, 12657, 13321, 12657, 2003, 948, 7602, 8112, 4408, 4148, 10737, 6359, 1321, 3512, 774, 215, 9173, 2673, 10728, 6176, 6598, 12176, 4334, 9515, 215, 11758, 948, 7602, 10728, 3512, 9429, 12657, 4015, 2003, 948, 8049, 12485, 6426, 5923, 12354, 4145, 12602, 13580, 4367, 8112, 10093, 4084, 8112, 13608, 3326, 12657, 215, 3132, 9517, 12485, 8781, 5923, 12627, 4014, 5754, 10218, 7139, 7769, 8781, 8084, 3512, 10728, 2002, 58, 7769, 935, 10728, 8878, 8112, 13344, 11903, 2702, 12657, 3326, 6958, 11324, 12657, 10175, 8112, 2902, 4084, 8454, 862, 12657, 10357, 12657, 8454, 12657, 13048, 2758, 5880, 6598, 12657, 9943, 2673, 8112, 3023, 6521, 9074, 12072, 8610, 2541, 12657, 6521, 9074, 12472, 6268, 6521, 9074, 13901, 12485, 12657, 6521, 9074, 13571, 2247, 1480, 10728, 7994, 13192, 13192, 12657, 7722, 6598, 215, 12398, 10728, 13580, 2702, 7769, 11324, 1978, 2673, 8112, 11340, 12657, 4388, 5923, 5354, 9624, 7149, 4015, 10742, 12657, 9234, 9517, 2541, 8610, 10357, 9074, 878, 7776, 8112, 9805, 4, 5, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9624, 7825, 6958, 7800, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 215, 9624, 5257, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9624, 7825, 6958, 7800, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 215, 9624, 5257, 6, 3918, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 4468, 9624, 215, 12657, 6677, 12657, 3839, 2010, 6958, 13421, 2673, 5754, 5853, 9234, 8112, 10357, 4084, 11054, 12657, 9234, 8112, 10357, 4084, 11054, 9234, 4468, 9624, 215, 8813, 12485, 10820, 12460, 11326, 3326, 12602, 215, 8813, 9641, 12460, 8242, 7092, 12398, 215, 12657, 7131, 12176, 4135, 215, 13857, 13421, 4468, 12657, 5257, 2645, 5923, 12460, 11326, 3326, 12602, 215, 215, 13857, 13421, 2398, 12657, 9234, 13419, 12485, 10820, 11326, 327, 2758, 8813, 12485, 13384, 215, 5257, 12398, 215, 12657, 6677, 11886, 2]\n",
            "...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    A Bidirectional LSTM model for sequence prediction tasks.\n",
        "\n",
        "    Parameters:\n",
        "        vocab_size (int): The size of the vocabulary.\n",
        "        embedding_dim (int): The dimensionality of the embedding layer.\n",
        "        hidden_dim (int): The dimensionality of the LSTM's hidden states.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Adjust the dropout rate as needed\n",
        "        # Ensure hidden_dim is divisible by 2 for the bidirectional LSTM\n",
        "        assert hidden_dim % 2 == 0, \"Hidden dimension must be even for BiLSTM.\"\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, dropout=0.5, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the model.\n",
        "\n",
        "        Parameters:\n",
        "            sequence (Tensor): The input sequence to the model.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output predictions for the next token in the sequence.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(sequence)\n",
        "        embedded = self.dropout(embedded)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def get_vocab_size_from_series(tokenized_songs_series):\n",
        "    \"\"\"\n",
        "    Determines the vocabulary size from a series of tokenized songs.\n",
        "\n",
        "    Parameters:\n",
        "        tokenized_songs_series (pd.Series): A series of tokenized songs.\n",
        "\n",
        "    Returns:\n",
        "        int: The size of the vocabulary.\n",
        "    \"\"\"\n",
        "    vocabulary = set().union(*tokenized_songs_series)\n",
        "    return len(vocabulary)\n",
        "def replace_tags_in_list(tokens, temp_switch, reverse=False):\n",
        "    if reverse:\n",
        "        temp_switch = {v: k for k, v in temp_switch.items()}\n",
        "\n",
        "    # Replace each token if it matches a tag in temp_switch\n",
        "    return [temp_switch.get(token, token) for token in tokens]\n",
        "\n",
        "# Apply the adapted function to the tokenized data\n",
        "spacy_tokens_df = spacy_tokens_df.apply(lambda tokens_list: replace_tags_in_list(tokens_list, temp_switch, reverse=True))\n",
        "\n",
        "utils.export_data_to_csv(spacy_tokens_df,'LATESTSPACETOKENS',debug)\n",
        "# Build vocabulary from tokenized series and convert songs to indices\n",
        "token_to_idx = build_vocab(spacy_tokens_df)\n",
        "\n",
        "# Check if special tokens are in the vocabulary\n",
        "special_tokens = ['<startverse>', '<endverse>', '<startchorus>', '<endchorus>', '<startintro>', '<endintro>', '<startsong>', '<endsong>']\n",
        "for token in special_tokens:\n",
        "    assert token in token_to_idx, f\"{token} not in vocabulary\"\n",
        "\n",
        "indexed_tokens_series = spacy_tokens_df.apply(lambda song: tokens_to_indices(song, token_to_idx))\n",
        "\n",
        "# Sample a few tokenized songs to inspect the placement of special tokens\n",
        "for idx, song in indexed_tokens_series.sample(5).items():\n",
        "    print(f\"Song {idx}: {song}\")  # Print the first 50 tokens of each sampled song\n",
        "    print(\"...\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Define the sequence length and initialize the dataset\n",
        "seq_length = 70\n",
        "dataset = TokenSongsDataset(indexed_tokens_series, seq_length)\n",
        "\n",
        "# Initialize DataLoader with the dataset\n",
        "batch_size = 12\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the BiLSTM model\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "model = BiLSTM(len(token_to_idx), embedding_dim, hidden_dim)\n",
        "\n",
        "# Create a reverse mapping from indices to tokens for later use\n",
        "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qqixrz_nIzXo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FastSongStructureLoss(nn.Module):\n",
        "    def __init__(self, token_to_idx, structure_penalty=1.0, penalty_multiplier=1.2):\n",
        "        super(FastSongStructureLoss, self).__init__()\n",
        "        self.base_loss = nn.CrossEntropyLoss(reduction='none')  # Change reduction to 'none' to compute loss for each element\n",
        "        self.structure_penalty = structure_penalty\n",
        "        self.penalty_multiplier = penalty_multiplier\n",
        "        # Convert indices to tensors for comparison\n",
        "        self.start_song_idx = torch.tensor(token_to_idx['<startsong>'])\n",
        "        self.end_song_idx = torch.tensor(token_to_idx['<endsong>'])\n",
        "        self.start_chorus_idx = torch.tensor(token_to_idx['<startchorus>'])\n",
        "        self.end_chorus_idx = torch.tensor(token_to_idx['<endchorus>'])\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        # Compute base loss for each element\n",
        "        losses = self.base_loss(outputs, targets)\n",
        "\n",
        "        # Check structure penalties in batch\n",
        "        correct_start = targets[:, 0] == self.start_song_idx\n",
        "        correct_end = targets[:, -1] == self.end_song_idx\n",
        "        has_start_chorus = torch.any(targets == self.start_chorus_idx, dim=1)\n",
        "        has_end_chorus = torch.any(targets == self.end_chorus_idx, dim=1)\n",
        "        correct_chorus = has_start_chorus & has_end_chorus\n",
        "\n",
        "        # Apply penalties\n",
        "        penalties = torch.ones_like(correct_start, dtype=torch.float)  # Start with a tensor of ones\n",
        "        penalties[~correct_start | ~correct_end | ~correct_chorus] *= self.penalty_multiplier\n",
        "\n",
        "        # Apply penalties to loss\n",
        "        losses *= penalties.unsqueeze(1)  # Make penalties broadcastable to match losses shape\n",
        "\n",
        "        # Return mean loss\n",
        "        return losses.mean()\n",
        "\n",
        "# Example usage\n",
        "song_structure_loss = FastSongStructureLoss(token_to_idx, structure_penalty=1.0, penalty_multiplier=1.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z0_U8RMVpn_",
        "outputId": "3314ff95-87c4-4cf9-e6ce-8a7d051bf19c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/7], Batch [0], Loss: 5.6474, Accuracy: 0.1417\n",
            "Epoch [1/7], Batch [100], Loss: 6.1070, Accuracy: 0.0940\n",
            "Epoch [1/7], Batch [200], Loss: 5.6189, Accuracy: 0.1357\n",
            "Epoch [1/7], Batch [300], Loss: 5.6743, Accuracy: 0.1333\n",
            "Epoch [1/7], Batch [400], Loss: 5.5296, Accuracy: 0.1536\n",
            "Epoch [1/7], Batch [500], Loss: 5.2231, Accuracy: 0.1679\n",
            "Epoch [1/7], Batch [600], Loss: 5.5131, Accuracy: 0.1536\n",
            "Epoch [1/7], Batch [700], Loss: 5.3350, Accuracy: 0.1464\n",
            "Epoch [1/7], Batch [800], Loss: 5.3054, Accuracy: 0.1655\n",
            "Epoch [1/7], Batch [900], Loss: 5.1956, Accuracy: 0.1750\n",
            "Epoch [1/7], Batch [1000], Loss: 5.2574, Accuracy: 0.1417\n",
            "Epoch [1/7], Batch [1100], Loss: 5.4016, Accuracy: 0.1476\n",
            "Epoch [1/7], Batch [1200], Loss: 5.0709, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [1300], Loss: 5.2622, Accuracy: 0.1607\n",
            "Epoch [1/7], Batch [1400], Loss: 5.1568, Accuracy: 0.1536\n",
            "Epoch [1/7], Batch [1500], Loss: 5.2488, Accuracy: 0.1631\n",
            "Epoch [1/7], Batch [1600], Loss: 5.0643, Accuracy: 0.1667\n",
            "Epoch [1/7], Batch [1700], Loss: 5.2939, Accuracy: 0.1762\n",
            "Epoch [1/7], Batch [1800], Loss: 5.0656, Accuracy: 0.1798\n",
            "Epoch [1/7], Batch [1900], Loss: 4.8274, Accuracy: 0.1952\n",
            "Epoch [1/7], Batch [2000], Loss: 5.0606, Accuracy: 0.1774\n",
            "Epoch [1/7], Batch [2100], Loss: 5.0273, Accuracy: 0.1571\n",
            "Epoch [1/7], Batch [2200], Loss: 5.0968, Accuracy: 0.1536\n",
            "Epoch [1/7], Batch [2300], Loss: 5.3581, Accuracy: 0.1488\n",
            "Epoch [1/7], Batch [2400], Loss: 4.8808, Accuracy: 0.1643\n",
            "Epoch [1/7], Batch [2500], Loss: 4.9653, Accuracy: 0.1512\n",
            "Epoch [1/7], Batch [2600], Loss: 4.9390, Accuracy: 0.1821\n",
            "Epoch [1/7], Batch [2700], Loss: 4.8336, Accuracy: 0.1821\n",
            "Epoch [1/7], Batch [2800], Loss: 4.8456, Accuracy: 0.1857\n",
            "Epoch [1/7], Batch [2900], Loss: 4.8944, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [3000], Loss: 4.7439, Accuracy: 0.1762\n",
            "Epoch [1/7], Batch [3100], Loss: 4.7359, Accuracy: 0.1524\n",
            "Epoch [1/7], Batch [3200], Loss: 4.7008, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [3300], Loss: 4.8551, Accuracy: 0.1738\n",
            "Epoch [1/7], Batch [3400], Loss: 4.7214, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [3500], Loss: 5.2225, Accuracy: 0.1643\n",
            "Epoch [1/7], Batch [3600], Loss: 4.7631, Accuracy: 0.1988\n",
            "Epoch [1/7], Batch [3700], Loss: 4.7564, Accuracy: 0.1726\n",
            "Epoch [1/7], Batch [3800], Loss: 4.8048, Accuracy: 0.1857\n",
            "Epoch [1/7], Batch [3900], Loss: 4.7038, Accuracy: 0.2048\n",
            "Epoch [1/7], Batch [4000], Loss: 4.8498, Accuracy: 0.1952\n",
            "Epoch [1/7], Batch [4100], Loss: 4.5737, Accuracy: 0.2000\n",
            "Epoch [1/7], Batch [4200], Loss: 4.9079, Accuracy: 0.1655\n",
            "Epoch [1/7], Batch [4300], Loss: 5.1770, Accuracy: 0.1488\n",
            "Epoch [1/7], Batch [4400], Loss: 4.8835, Accuracy: 0.1750\n",
            "Epoch [1/7], Batch [4500], Loss: 4.8839, Accuracy: 0.1536\n",
            "Epoch [1/7], Batch [4600], Loss: 4.6977, Accuracy: 0.1798\n",
            "Epoch [1/7], Batch [4700], Loss: 4.6940, Accuracy: 0.1833\n",
            "Epoch [1/7], Batch [4800], Loss: 4.7009, Accuracy: 0.2060\n",
            "Epoch [1/7], Batch [4900], Loss: 4.7787, Accuracy: 0.1774\n",
            "Epoch [1/7], Batch [5000], Loss: 4.9382, Accuracy: 0.1702\n",
            "Epoch [1/7], Batch [5100], Loss: 5.0823, Accuracy: 0.1643\n",
            "Epoch [1/7], Batch [5200], Loss: 4.6617, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [5300], Loss: 4.7645, Accuracy: 0.1988\n",
            "Epoch [1/7], Batch [5400], Loss: 4.8063, Accuracy: 0.1738\n",
            "Epoch [1/7], Batch [5500], Loss: 4.6217, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [5600], Loss: 4.7273, Accuracy: 0.1833\n",
            "Epoch [1/7], Batch [5700], Loss: 4.9161, Accuracy: 0.1762\n",
            "Epoch [1/7], Batch [5800], Loss: 4.7897, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [5900], Loss: 4.9194, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [6000], Loss: 4.6560, Accuracy: 0.1583\n",
            "Epoch [1/7], Batch [6100], Loss: 4.8612, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [6200], Loss: 5.0944, Accuracy: 0.1702\n",
            "Epoch [1/7], Batch [6300], Loss: 4.5938, Accuracy: 0.1714\n",
            "Epoch [1/7], Batch [6400], Loss: 4.6019, Accuracy: 0.1798\n",
            "Epoch [1/7], Batch [6500], Loss: 4.9055, Accuracy: 0.1679\n",
            "Epoch [1/7], Batch [6600], Loss: 4.7934, Accuracy: 0.1714\n",
            "Epoch [1/7], Batch [6700], Loss: 4.7216, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [6800], Loss: 4.8724, Accuracy: 0.1917\n",
            "Epoch [1/7], Batch [6900], Loss: 4.8821, Accuracy: 0.1726\n",
            "Epoch [1/7], Batch [7000], Loss: 4.6249, Accuracy: 0.1857\n",
            "Epoch [1/7], Batch [7100], Loss: 4.9353, Accuracy: 0.1595\n",
            "Epoch [1/7], Batch [7200], Loss: 4.6512, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [7300], Loss: 4.4557, Accuracy: 0.2071\n",
            "Epoch [1/7], Batch [7400], Loss: 4.8538, Accuracy: 0.1595\n",
            "Epoch [1/7], Batch [7500], Loss: 4.5855, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [7600], Loss: 4.9048, Accuracy: 0.1726\n",
            "Epoch [1/7], Batch [7700], Loss: 4.6889, Accuracy: 0.1631\n",
            "Epoch [1/7], Batch [7800], Loss: 4.4850, Accuracy: 0.1607\n",
            "Epoch [1/7], Batch [7900], Loss: 4.6958, Accuracy: 0.1750\n",
            "Epoch [1/7], Batch [8000], Loss: 4.6426, Accuracy: 0.1726\n",
            "Epoch [1/7], Batch [8100], Loss: 4.9640, Accuracy: 0.1452\n",
            "Epoch [1/7], Batch [8200], Loss: 4.5548, Accuracy: 0.1869\n",
            "Epoch [1/7], Batch [8300], Loss: 4.4527, Accuracy: 0.2226\n",
            "Epoch [1/7], Batch [8400], Loss: 4.4299, Accuracy: 0.2131\n",
            "Epoch [1/7], Batch [8500], Loss: 4.5266, Accuracy: 0.1726\n",
            "Epoch [1/7], Batch [8600], Loss: 4.7458, Accuracy: 0.1786\n",
            "Epoch [1/7], Batch [8700], Loss: 4.5306, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [8800], Loss: 4.7186, Accuracy: 0.1929\n",
            "Epoch [1/7], Batch [8900], Loss: 4.2682, Accuracy: 0.2500\n",
            "Epoch [1/7], Batch [9000], Loss: 4.8764, Accuracy: 0.1512\n",
            "Epoch [1/7], Batch [9100], Loss: 4.6331, Accuracy: 0.1845\n",
            "Epoch [1/7], Batch [9200], Loss: 4.3157, Accuracy: 0.2119\n",
            "Epoch [1/7], Batch [9300], Loss: 4.7401, Accuracy: 0.1786\n",
            "Epoch [1/7], Batch [9400], Loss: 4.2548, Accuracy: 0.2310\n",
            "Epoch [1/7], Batch [9500], Loss: 4.4934, Accuracy: 0.1679\n",
            "Epoch [1/7], Batch [9600], Loss: 4.7167, Accuracy: 0.1738\n",
            "Epoch [1/7], Batch [9700], Loss: 4.5803, Accuracy: 0.2071\n",
            "Epoch [1/7], Batch [9800], Loss: 4.5989, Accuracy: 0.1833\n",
            "Epoch [1/7], Batch [9900], Loss: 4.7369, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [10000], Loss: 4.6337, Accuracy: 0.1821\n",
            "Epoch [1/7], Batch [10100], Loss: 4.7590, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [10200], Loss: 4.5417, Accuracy: 0.1845\n",
            "Epoch [1/7], Batch [10300], Loss: 4.7669, Accuracy: 0.1774\n",
            "Epoch [1/7], Batch [10400], Loss: 4.7684, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [10500], Loss: 4.4804, Accuracy: 0.1786\n",
            "Epoch [1/7], Batch [10600], Loss: 4.8431, Accuracy: 0.1786\n",
            "Epoch [1/7], Batch [10700], Loss: 4.5824, Accuracy: 0.1845\n",
            "Epoch [1/7], Batch [10800], Loss: 4.7035, Accuracy: 0.1655\n",
            "Epoch [1/7], Batch [10900], Loss: 4.7357, Accuracy: 0.1679\n",
            "Epoch [1/7], Batch [11000], Loss: 4.6834, Accuracy: 0.1952\n",
            "Epoch [1/7], Batch [11100], Loss: 4.8485, Accuracy: 0.1667\n",
            "Epoch [1/7], Batch [11200], Loss: 4.7878, Accuracy: 0.1607\n",
            "Epoch [1/7], Batch [11300], Loss: 4.6255, Accuracy: 0.1929\n",
            "Epoch [1/7], Batch [11400], Loss: 4.8271, Accuracy: 0.1750\n",
            "Epoch [1/7], Batch [11500], Loss: 4.7044, Accuracy: 0.1702\n",
            "Epoch [1/7], Batch [11600], Loss: 4.5547, Accuracy: 0.1976\n",
            "Epoch [1/7], Batch [11700], Loss: 4.7755, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [11800], Loss: 4.5108, Accuracy: 0.2024\n",
            "Epoch [1/7], Batch [11900], Loss: 4.7079, Accuracy: 0.1905\n",
            "Epoch [1/7], Batch [12000], Loss: 4.7201, Accuracy: 0.1571\n",
            "Epoch [1/7], Batch [12100], Loss: 4.6546, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [12200], Loss: 4.3953, Accuracy: 0.1762\n",
            "Epoch [1/7], Batch [12300], Loss: 4.3898, Accuracy: 0.2012\n",
            "Epoch [1/7], Batch [12400], Loss: 4.6256, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [12500], Loss: 4.6223, Accuracy: 0.1869\n",
            "Epoch [1/7], Batch [12600], Loss: 4.6404, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [12700], Loss: 4.3267, Accuracy: 0.1976\n",
            "Epoch [1/7], Batch [12800], Loss: 4.4511, Accuracy: 0.2024\n",
            "Epoch [1/7], Batch [12900], Loss: 4.6671, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [13000], Loss: 4.3779, Accuracy: 0.2107\n",
            "Epoch [1/7], Batch [13100], Loss: 4.5620, Accuracy: 0.1905\n",
            "Epoch [1/7], Batch [13200], Loss: 4.6058, Accuracy: 0.1917\n",
            "Epoch [1/7], Batch [13300], Loss: 4.4690, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [13400], Loss: 4.4609, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [13500], Loss: 4.5703, Accuracy: 0.1988\n",
            "Epoch [1/7], Batch [13600], Loss: 4.5095, Accuracy: 0.2095\n",
            "Epoch [1/7], Batch [13700], Loss: 4.5645, Accuracy: 0.1869\n",
            "Epoch [1/7], Batch [13800], Loss: 4.0726, Accuracy: 0.2071\n",
            "Epoch [1/7], Batch [13900], Loss: 4.5776, Accuracy: 0.1833\n",
            "Epoch [1/7], Batch [14000], Loss: 4.7812, Accuracy: 0.1774\n",
            "Epoch [1/7], Batch [14100], Loss: 4.5259, Accuracy: 0.1762\n",
            "Epoch [1/7], Batch [14200], Loss: 4.4201, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [14300], Loss: 4.4976, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [14400], Loss: 4.5260, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [14500], Loss: 4.5235, Accuracy: 0.2012\n",
            "Epoch [1/7], Batch [14600], Loss: 4.7535, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [14700], Loss: 4.6102, Accuracy: 0.1952\n",
            "Epoch [1/7], Batch [14800], Loss: 4.6512, Accuracy: 0.1643\n",
            "Epoch [1/7], Batch [14900], Loss: 4.4207, Accuracy: 0.2190\n",
            "Epoch [1/7], Batch [15000], Loss: 4.3511, Accuracy: 0.1869\n",
            "Epoch [1/7], Batch [15100], Loss: 4.4973, Accuracy: 0.1845\n",
            "Epoch [1/7], Batch [15200], Loss: 4.4967, Accuracy: 0.1738\n",
            "Epoch [1/7], Batch [15300], Loss: 4.6181, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [15400], Loss: 4.5572, Accuracy: 0.1571\n",
            "Epoch [1/7], Batch [15500], Loss: 4.5386, Accuracy: 0.1821\n",
            "Epoch [1/7], Batch [15600], Loss: 4.4365, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [15700], Loss: 4.6483, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [15800], Loss: 4.6722, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [15900], Loss: 4.2367, Accuracy: 0.2464\n",
            "Epoch [1/7], Batch [16000], Loss: 4.4645, Accuracy: 0.1679\n",
            "Epoch [1/7], Batch [16100], Loss: 4.2693, Accuracy: 0.2107\n",
            "Epoch [1/7], Batch [16200], Loss: 4.4332, Accuracy: 0.1798\n",
            "Epoch [1/7], Batch [16300], Loss: 4.7034, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [16400], Loss: 4.5303, Accuracy: 0.1988\n",
            "Epoch [1/7], Batch [16500], Loss: 4.4645, Accuracy: 0.1821\n",
            "Epoch [1/7], Batch [16600], Loss: 4.3066, Accuracy: 0.2036\n",
            "Epoch [1/7], Batch [16700], Loss: 4.3337, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [16800], Loss: 4.4644, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [16900], Loss: 4.4917, Accuracy: 0.1869\n",
            "Epoch [1/7], Batch [17000], Loss: 4.3518, Accuracy: 0.2238\n",
            "Epoch [1/7], Batch [17100], Loss: 4.5125, Accuracy: 0.1798\n",
            "Epoch [1/7], Batch [17200], Loss: 4.5709, Accuracy: 0.1833\n",
            "Epoch [1/7], Batch [17300], Loss: 4.4030, Accuracy: 0.2012\n",
            "Epoch [1/7], Batch [17400], Loss: 4.4503, Accuracy: 0.2083\n",
            "Epoch [1/7], Batch [17500], Loss: 4.5336, Accuracy: 0.1833\n",
            "Epoch [1/7], Batch [17600], Loss: 4.4559, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [17700], Loss: 4.3031, Accuracy: 0.1929\n",
            "Epoch [1/7], Batch [17800], Loss: 4.5962, Accuracy: 0.1750\n",
            "Epoch [1/7], Batch [17900], Loss: 4.4450, Accuracy: 0.2179\n",
            "Epoch [1/7], Batch [18000], Loss: 4.6451, Accuracy: 0.1702\n",
            "Epoch [1/7], Batch [18100], Loss: 4.4573, Accuracy: 0.2167\n",
            "Epoch [1/7], Batch [18200], Loss: 4.3558, Accuracy: 0.1821\n",
            "Epoch [1/7], Batch [18300], Loss: 4.4380, Accuracy: 0.1869\n",
            "Epoch [1/7], Batch [18400], Loss: 4.5922, Accuracy: 0.1631\n",
            "Epoch [1/7], Batch [18500], Loss: 4.7377, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [18600], Loss: 4.6262, Accuracy: 0.1810\n",
            "Epoch [1/7], Batch [18700], Loss: 4.3491, Accuracy: 0.2036\n",
            "Epoch [1/7], Batch [18800], Loss: 4.4636, Accuracy: 0.1917\n",
            "Epoch [1/7], Batch [18900], Loss: 4.4895, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [19000], Loss: 4.3870, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [19100], Loss: 3.8716, Accuracy: 0.2571\n",
            "Epoch [1/7], Batch [19200], Loss: 4.4993, Accuracy: 0.2083\n",
            "Epoch [1/7], Batch [19300], Loss: 4.2199, Accuracy: 0.2000\n",
            "Epoch [1/7], Batch [19400], Loss: 4.6325, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [19500], Loss: 4.1697, Accuracy: 0.1917\n",
            "Epoch [1/7], Batch [19600], Loss: 4.6937, Accuracy: 0.1595\n",
            "Epoch [1/7], Batch [19700], Loss: 4.3358, Accuracy: 0.2179\n",
            "Epoch [1/7], Batch [19800], Loss: 4.3542, Accuracy: 0.2179\n",
            "Epoch [1/7], Batch [19900], Loss: 4.3536, Accuracy: 0.1964\n",
            "Epoch [1/7], Batch [20000], Loss: 4.4951, Accuracy: 0.1857\n",
            "Epoch [1/7], Batch [20100], Loss: 4.3427, Accuracy: 0.2179\n",
            "Epoch [1/7], Batch [20200], Loss: 4.5763, Accuracy: 0.1750\n",
            "Epoch [1/7], Batch [20300], Loss: 4.0893, Accuracy: 0.2071\n",
            "Epoch [1/7], Batch [20400], Loss: 4.3476, Accuracy: 0.1667\n",
            "Epoch [1/7], Batch [20500], Loss: 4.2955, Accuracy: 0.2071\n",
            "Epoch [1/7], Batch [20600], Loss: 4.5647, Accuracy: 0.1976\n",
            "Epoch [1/7], Batch [20700], Loss: 4.4866, Accuracy: 0.2048\n",
            "Epoch [1/7], Batch [20800], Loss: 4.5795, Accuracy: 0.1655\n",
            "Epoch [1/7], Batch [20900], Loss: 4.4672, Accuracy: 0.1893\n",
            "Epoch [1/7], Batch [21000], Loss: 3.9950, Accuracy: 0.2643\n",
            "Epoch [1/7], Batch [21100], Loss: 4.4370, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [21200], Loss: 4.3841, Accuracy: 0.2083\n",
            "Epoch [1/7], Batch [21300], Loss: 4.3995, Accuracy: 0.1881\n",
            "Epoch [1/7], Batch [21400], Loss: 4.3918, Accuracy: 0.1917\n",
            "Epoch [1/7], Batch [21500], Loss: 4.1610, Accuracy: 0.2286\n",
            "Epoch [1/7], Batch [21600], Loss: 4.3039, Accuracy: 0.2048\n",
            "Epoch [1/7], Batch [21700], Loss: 4.3799, Accuracy: 0.2012\n",
            "Epoch [1/7], Batch [21800], Loss: 4.3125, Accuracy: 0.2214\n",
            "Epoch [1/7], Batch [21900], Loss: 4.3073, Accuracy: 0.1905\n",
            "Epoch [1/7], Batch [22000], Loss: 4.2695, Accuracy: 0.2095\n",
            "Epoch [1/7], Batch [22100], Loss: 4.2485, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [22200], Loss: 4.1991, Accuracy: 0.1917\n",
            "Epoch [1/7], Batch [22300], Loss: 4.1133, Accuracy: 0.2286\n",
            "Epoch [1/7], Batch [22400], Loss: 4.3729, Accuracy: 0.1988\n",
            "Epoch [1/7], Batch [22500], Loss: 4.4125, Accuracy: 0.1976\n",
            "Epoch [1/7], Batch [22600], Loss: 4.7414, Accuracy: 0.1607\n",
            "Epoch [1/7], Batch [22700], Loss: 4.6227, Accuracy: 0.1702\n",
            "Epoch [1/7], Batch [22800], Loss: 4.2271, Accuracy: 0.2238\n",
            "Epoch [1/7], Batch [22900], Loss: 4.2244, Accuracy: 0.2310\n",
            "Epoch [1/7], Batch [23000], Loss: 4.1050, Accuracy: 0.2155\n",
            "Epoch [1/7], Batch [23100], Loss: 4.2341, Accuracy: 0.2143\n",
            "Epoch [1/7], Batch [23200], Loss: 4.2793, Accuracy: 0.2024\n",
            "Epoch [1/7], Batch [23300], Loss: 4.5392, Accuracy: 0.1988\n",
            "Epoch [1/7], Batch [23400], Loss: 4.4501, Accuracy: 0.1690\n",
            "Epoch [1/7], Batch [23500], Loss: 4.2974, Accuracy: 0.2190\n",
            "Epoch [1/7], Batch [23600], Loss: 4.4503, Accuracy: 0.2048\n",
            "Epoch [1/7], Batch [23700], Loss: 4.5819, Accuracy: 0.2024\n",
            "Epoch [1/7], Batch [23800], Loss: 4.2870, Accuracy: 0.2214\n",
            "Epoch [1/7], Batch [23900], Loss: 4.3468, Accuracy: 0.2250\n",
            "Epoch [1/7], Batch [24000], Loss: 4.2446, Accuracy: 0.2321\n",
            "Epoch [1/7], Batch [24100], Loss: 4.0321, Accuracy: 0.2310\n",
            "Epoch [1/7], Batch [24200], Loss: 4.4882, Accuracy: 0.1940\n",
            "Epoch [1/7], Batch [24300], Loss: 4.5467, Accuracy: 0.2012\n",
            "Epoch [1/7], Batch [24400], Loss: 4.2518, Accuracy: 0.1929\n",
            "Epoch [1/7], Batch [24500], Loss: 4.5353, Accuracy: 0.1845\n",
            "Epoch [1/7], Batch [24600], Loss: 4.1810, Accuracy: 0.2155\n",
            "Epoch [1/7], Batch [24700], Loss: 4.2927, Accuracy: 0.2143\n",
            "Epoch [1/7], Batch [24800], Loss: 4.4104, Accuracy: 0.2060\n",
            "Epoch [1/7], Batch [24900], Loss: 4.6985, Accuracy: 0.1548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End of Epoch [1], Average Loss: 4.6387, Average Accuracy: 0.1860, Average Perplexity: nan\n",
            "====================================================================================================\n",
            "Epoch [2/7], Batch [0], Loss: 4.3204, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [100], Loss: 4.3018, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [200], Loss: 4.4182, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [300], Loss: 4.3935, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [400], Loss: 4.1789, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [500], Loss: 4.5385, Accuracy: 0.1857\n",
            "Epoch [2/7], Batch [600], Loss: 4.5468, Accuracy: 0.1631\n",
            "Epoch [2/7], Batch [700], Loss: 4.7027, Accuracy: 0.1869\n",
            "Epoch [2/7], Batch [800], Loss: 4.2219, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [900], Loss: 4.4571, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [1000], Loss: 4.3642, Accuracy: 0.2000\n",
            "Epoch [2/7], Batch [1100], Loss: 4.5664, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [1200], Loss: 4.4445, Accuracy: 0.1940\n",
            "Epoch [2/7], Batch [1300], Loss: 4.2244, Accuracy: 0.2060\n",
            "Epoch [2/7], Batch [1400], Loss: 4.0457, Accuracy: 0.2250\n",
            "Epoch [2/7], Batch [1500], Loss: 4.3223, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [1600], Loss: 4.5182, Accuracy: 0.1821\n",
            "Epoch [2/7], Batch [1700], Loss: 4.4958, Accuracy: 0.1798\n",
            "Epoch [2/7], Batch [1800], Loss: 4.1949, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [1900], Loss: 4.4637, Accuracy: 0.1881\n",
            "Epoch [2/7], Batch [2000], Loss: 4.5249, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [2100], Loss: 4.1848, Accuracy: 0.2250\n",
            "Epoch [2/7], Batch [2200], Loss: 4.0765, Accuracy: 0.2321\n",
            "Epoch [2/7], Batch [2300], Loss: 4.7470, Accuracy: 0.1619\n",
            "Epoch [2/7], Batch [2400], Loss: 4.3586, Accuracy: 0.1881\n",
            "Epoch [2/7], Batch [2500], Loss: 4.4105, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [2600], Loss: 4.1800, Accuracy: 0.1798\n",
            "Epoch [2/7], Batch [2700], Loss: 4.1047, Accuracy: 0.2357\n",
            "Epoch [2/7], Batch [2800], Loss: 4.3084, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [2900], Loss: 4.2875, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [3000], Loss: 4.2270, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [3100], Loss: 4.2947, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [3200], Loss: 4.1590, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [3300], Loss: 4.3698, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [3400], Loss: 4.4183, Accuracy: 0.1905\n",
            "Epoch [2/7], Batch [3500], Loss: 4.3263, Accuracy: 0.1964\n",
            "Epoch [2/7], Batch [3600], Loss: 4.2709, Accuracy: 0.2214\n",
            "Epoch [2/7], Batch [3700], Loss: 4.4239, Accuracy: 0.2381\n",
            "Epoch [2/7], Batch [3800], Loss: 4.3456, Accuracy: 0.1857\n",
            "Epoch [2/7], Batch [3900], Loss: 4.1484, Accuracy: 0.2131\n",
            "Epoch [2/7], Batch [4000], Loss: 4.5031, Accuracy: 0.1940\n",
            "Epoch [2/7], Batch [4100], Loss: 4.3902, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [4200], Loss: 4.1007, Accuracy: 0.2167\n",
            "Epoch [2/7], Batch [4300], Loss: 4.2301, Accuracy: 0.2000\n",
            "Epoch [2/7], Batch [4400], Loss: 4.0639, Accuracy: 0.2393\n",
            "Epoch [2/7], Batch [4500], Loss: 4.4632, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [4600], Loss: 4.2822, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [4700], Loss: 4.4784, Accuracy: 0.1905\n",
            "Epoch [2/7], Batch [4800], Loss: 4.0829, Accuracy: 0.2440\n",
            "Epoch [2/7], Batch [4900], Loss: 4.4214, Accuracy: 0.2060\n",
            "Epoch [2/7], Batch [5000], Loss: 4.2972, Accuracy: 0.2214\n",
            "Epoch [2/7], Batch [5100], Loss: 4.2295, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [5200], Loss: 4.4959, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [5300], Loss: 4.2232, Accuracy: 0.2381\n",
            "Epoch [2/7], Batch [5400], Loss: 4.2122, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [5500], Loss: 4.2188, Accuracy: 0.2333\n",
            "Epoch [2/7], Batch [5600], Loss: 4.3944, Accuracy: 0.1714\n",
            "Epoch [2/7], Batch [5700], Loss: 4.5849, Accuracy: 0.1905\n",
            "Epoch [2/7], Batch [5800], Loss: 4.3579, Accuracy: 0.2083\n",
            "Epoch [2/7], Batch [5900], Loss: 4.3335, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [6000], Loss: 4.2596, Accuracy: 0.2131\n",
            "Epoch [2/7], Batch [6100], Loss: 4.1394, Accuracy: 0.2250\n",
            "Epoch [2/7], Batch [6200], Loss: 4.1779, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [6300], Loss: 3.9633, Accuracy: 0.2548\n",
            "Epoch [2/7], Batch [6400], Loss: 4.2774, Accuracy: 0.2262\n",
            "Epoch [2/7], Batch [6500], Loss: 4.3728, Accuracy: 0.1774\n",
            "Epoch [2/7], Batch [6600], Loss: 4.2932, Accuracy: 0.2167\n",
            "Epoch [2/7], Batch [6700], Loss: 4.2426, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [6800], Loss: 4.3465, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [6900], Loss: 4.3160, Accuracy: 0.1940\n",
            "Epoch [2/7], Batch [7000], Loss: 4.3577, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [7100], Loss: 4.2484, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [7200], Loss: 4.1919, Accuracy: 0.2000\n",
            "Epoch [2/7], Batch [7300], Loss: 4.1944, Accuracy: 0.2131\n",
            "Epoch [2/7], Batch [7400], Loss: 4.1352, Accuracy: 0.2286\n",
            "Epoch [2/7], Batch [7500], Loss: 4.4638, Accuracy: 0.2024\n",
            "Epoch [2/7], Batch [7600], Loss: 4.3143, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [7700], Loss: 4.5360, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [7800], Loss: 4.5088, Accuracy: 0.1762\n",
            "Epoch [2/7], Batch [7900], Loss: 4.1442, Accuracy: 0.2286\n",
            "Epoch [2/7], Batch [8000], Loss: 3.9263, Accuracy: 0.2595\n",
            "Epoch [2/7], Batch [8100], Loss: 4.0998, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [8200], Loss: 4.6476, Accuracy: 0.1869\n",
            "Epoch [2/7], Batch [8300], Loss: 4.3432, Accuracy: 0.2048\n",
            "Epoch [2/7], Batch [8400], Loss: 4.4650, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [8500], Loss: 4.1254, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [8600], Loss: 4.1618, Accuracy: 0.2131\n",
            "Epoch [2/7], Batch [8700], Loss: 4.4108, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [8800], Loss: 4.3731, Accuracy: 0.2024\n",
            "Epoch [2/7], Batch [8900], Loss: 4.3102, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [9000], Loss: 4.2423, Accuracy: 0.2024\n",
            "Epoch [2/7], Batch [9100], Loss: 4.1365, Accuracy: 0.2464\n",
            "Epoch [2/7], Batch [9200], Loss: 4.3700, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [9300], Loss: 4.4359, Accuracy: 0.1940\n",
            "Epoch [2/7], Batch [9400], Loss: 4.3302, Accuracy: 0.2060\n",
            "Epoch [2/7], Batch [9500], Loss: 4.2998, Accuracy: 0.2167\n",
            "Epoch [2/7], Batch [9600], Loss: 4.3483, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [9700], Loss: 4.3429, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [9800], Loss: 4.2312, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [9900], Loss: 4.1207, Accuracy: 0.2357\n",
            "Epoch [2/7], Batch [10000], Loss: 3.9839, Accuracy: 0.2536\n",
            "Epoch [2/7], Batch [10100], Loss: 4.2687, Accuracy: 0.2036\n",
            "Epoch [2/7], Batch [10200], Loss: 4.1372, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [10300], Loss: 4.2950, Accuracy: 0.2393\n",
            "Epoch [2/7], Batch [10400], Loss: 4.2342, Accuracy: 0.2119\n",
            "Epoch [2/7], Batch [10500], Loss: 4.3616, Accuracy: 0.2131\n",
            "Epoch [2/7], Batch [10600], Loss: 4.0099, Accuracy: 0.2560\n",
            "Epoch [2/7], Batch [10700], Loss: 4.3434, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [10800], Loss: 4.2580, Accuracy: 0.2143\n",
            "Epoch [2/7], Batch [10900], Loss: 4.4293, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [11000], Loss: 4.2217, Accuracy: 0.2357\n",
            "Epoch [2/7], Batch [11100], Loss: 4.0217, Accuracy: 0.2238\n",
            "Epoch [2/7], Batch [11200], Loss: 4.5592, Accuracy: 0.1726\n",
            "Epoch [2/7], Batch [11300], Loss: 4.2344, Accuracy: 0.1881\n",
            "Epoch [2/7], Batch [11400], Loss: 4.3288, Accuracy: 0.2226\n",
            "Epoch [2/7], Batch [11500], Loss: 4.2245, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [11600], Loss: 4.1383, Accuracy: 0.2321\n",
            "Epoch [2/7], Batch [11700], Loss: 4.5394, Accuracy: 0.1881\n",
            "Epoch [2/7], Batch [11800], Loss: 4.4063, Accuracy: 0.1833\n",
            "Epoch [2/7], Batch [11900], Loss: 4.4656, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [12000], Loss: 4.1558, Accuracy: 0.2393\n",
            "Epoch [2/7], Batch [12100], Loss: 4.3328, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [12200], Loss: 4.2275, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [12300], Loss: 4.2420, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [12400], Loss: 4.6582, Accuracy: 0.1845\n",
            "Epoch [2/7], Batch [12500], Loss: 4.2916, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [12600], Loss: 4.2601, Accuracy: 0.2048\n",
            "Epoch [2/7], Batch [12700], Loss: 4.0632, Accuracy: 0.2286\n",
            "Epoch [2/7], Batch [12800], Loss: 4.4570, Accuracy: 0.1798\n",
            "Epoch [2/7], Batch [12900], Loss: 4.4615, Accuracy: 0.1869\n",
            "Epoch [2/7], Batch [13000], Loss: 4.4381, Accuracy: 0.2214\n",
            "Epoch [2/7], Batch [13100], Loss: 4.4844, Accuracy: 0.1702\n",
            "Epoch [2/7], Batch [13200], Loss: 4.3530, Accuracy: 0.2083\n",
            "Epoch [2/7], Batch [13300], Loss: 4.4651, Accuracy: 0.1893\n",
            "Epoch [2/7], Batch [13400], Loss: 4.1218, Accuracy: 0.2524\n",
            "Epoch [2/7], Batch [13500], Loss: 4.3573, Accuracy: 0.2083\n",
            "Epoch [2/7], Batch [13600], Loss: 4.1455, Accuracy: 0.2286\n",
            "Epoch [2/7], Batch [13700], Loss: 4.2885, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [13800], Loss: 4.0428, Accuracy: 0.2286\n",
            "Epoch [2/7], Batch [13900], Loss: 4.0193, Accuracy: 0.2238\n",
            "Epoch [2/7], Batch [14000], Loss: 4.1735, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [14100], Loss: 4.4313, Accuracy: 0.1952\n",
            "Epoch [2/7], Batch [14200], Loss: 4.2600, Accuracy: 0.1905\n",
            "Epoch [2/7], Batch [14300], Loss: 4.0744, Accuracy: 0.2381\n",
            "Epoch [2/7], Batch [14400], Loss: 4.3315, Accuracy: 0.1929\n",
            "Epoch [2/7], Batch [14500], Loss: 4.3586, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [14600], Loss: 4.2535, Accuracy: 0.2036\n",
            "Epoch [2/7], Batch [14700], Loss: 4.1320, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [14800], Loss: 4.4146, Accuracy: 0.1810\n",
            "Epoch [2/7], Batch [14900], Loss: 4.1588, Accuracy: 0.2250\n",
            "Epoch [2/7], Batch [15000], Loss: 4.2013, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [15100], Loss: 4.1373, Accuracy: 0.2214\n",
            "Epoch [2/7], Batch [15200], Loss: 4.3126, Accuracy: 0.2167\n",
            "Epoch [2/7], Batch [15300], Loss: 4.4471, Accuracy: 0.1857\n",
            "Epoch [2/7], Batch [15400], Loss: 4.3383, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [15500], Loss: 4.2060, Accuracy: 0.2464\n",
            "Epoch [2/7], Batch [15600], Loss: 4.2366, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [15700], Loss: 4.1821, Accuracy: 0.2310\n",
            "Epoch [2/7], Batch [15800], Loss: 4.0309, Accuracy: 0.2298\n",
            "Epoch [2/7], Batch [15900], Loss: 4.4710, Accuracy: 0.1833\n",
            "Epoch [2/7], Batch [16000], Loss: 4.3352, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [16100], Loss: 4.2069, Accuracy: 0.2167\n",
            "Epoch [2/7], Batch [16200], Loss: 4.2369, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [16300], Loss: 4.5360, Accuracy: 0.1869\n",
            "Epoch [2/7], Batch [16400], Loss: 4.0171, Accuracy: 0.2417\n",
            "Epoch [2/7], Batch [16500], Loss: 4.2443, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [16600], Loss: 4.2677, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [16700], Loss: 3.9879, Accuracy: 0.2512\n",
            "Epoch [2/7], Batch [16800], Loss: 4.1610, Accuracy: 0.2500\n",
            "Epoch [2/7], Batch [16900], Loss: 4.3826, Accuracy: 0.1964\n",
            "Epoch [2/7], Batch [17000], Loss: 4.3117, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [17100], Loss: 4.3454, Accuracy: 0.1940\n",
            "Epoch [2/7], Batch [17200], Loss: 4.2950, Accuracy: 0.1893\n",
            "Epoch [2/7], Batch [17300], Loss: 4.2033, Accuracy: 0.2369\n",
            "Epoch [2/7], Batch [17400], Loss: 4.4233, Accuracy: 0.2036\n",
            "Epoch [2/7], Batch [17500], Loss: 4.1575, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [17600], Loss: 4.2163, Accuracy: 0.2369\n",
            "Epoch [2/7], Batch [17700], Loss: 4.1701, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [17800], Loss: 4.2553, Accuracy: 0.1845\n",
            "Epoch [2/7], Batch [17900], Loss: 4.1430, Accuracy: 0.2310\n",
            "Epoch [2/7], Batch [18000], Loss: 4.2067, Accuracy: 0.2274\n",
            "Epoch [2/7], Batch [18100], Loss: 3.9026, Accuracy: 0.2690\n",
            "Epoch [2/7], Batch [18200], Loss: 4.2571, Accuracy: 0.2060\n",
            "Epoch [2/7], Batch [18300], Loss: 4.0593, Accuracy: 0.2238\n",
            "Epoch [2/7], Batch [18400], Loss: 4.3807, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [18500], Loss: 4.3595, Accuracy: 0.2060\n",
            "Epoch [2/7], Batch [18600], Loss: 4.0433, Accuracy: 0.2488\n",
            "Epoch [2/7], Batch [18700], Loss: 4.4147, Accuracy: 0.1964\n",
            "Epoch [2/7], Batch [18800], Loss: 4.0937, Accuracy: 0.2524\n",
            "Epoch [2/7], Batch [18900], Loss: 4.1416, Accuracy: 0.1905\n",
            "Epoch [2/7], Batch [19000], Loss: 4.1237, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [19100], Loss: 4.2933, Accuracy: 0.2298\n",
            "Epoch [2/7], Batch [19200], Loss: 4.4141, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [19300], Loss: 4.0282, Accuracy: 0.2536\n",
            "Epoch [2/7], Batch [19400], Loss: 4.1655, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [19500], Loss: 4.1786, Accuracy: 0.2321\n",
            "Epoch [2/7], Batch [19600], Loss: 4.2884, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [19700], Loss: 4.3050, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [19800], Loss: 4.4074, Accuracy: 0.2036\n",
            "Epoch [2/7], Batch [19900], Loss: 3.9567, Accuracy: 0.2333\n",
            "Epoch [2/7], Batch [20000], Loss: 4.0585, Accuracy: 0.2476\n",
            "Epoch [2/7], Batch [20100], Loss: 4.2978, Accuracy: 0.2048\n",
            "Epoch [2/7], Batch [20200], Loss: 4.2029, Accuracy: 0.2333\n",
            "Epoch [2/7], Batch [20300], Loss: 4.3007, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [20400], Loss: 4.4599, Accuracy: 0.1881\n",
            "Epoch [2/7], Batch [20500], Loss: 4.2410, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [20600], Loss: 4.2950, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [20700], Loss: 4.0186, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [20800], Loss: 4.4589, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [20900], Loss: 4.1760, Accuracy: 0.2167\n",
            "Epoch [2/7], Batch [21000], Loss: 4.2161, Accuracy: 0.2036\n",
            "Epoch [2/7], Batch [21100], Loss: 4.2055, Accuracy: 0.2298\n",
            "Epoch [2/7], Batch [21200], Loss: 4.1468, Accuracy: 0.2226\n",
            "Epoch [2/7], Batch [21300], Loss: 4.3511, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [21400], Loss: 4.4389, Accuracy: 0.2071\n",
            "Epoch [2/7], Batch [21500], Loss: 4.2069, Accuracy: 0.2214\n",
            "Epoch [2/7], Batch [21600], Loss: 4.2683, Accuracy: 0.1988\n",
            "Epoch [2/7], Batch [21700], Loss: 4.4064, Accuracy: 0.1869\n",
            "Epoch [2/7], Batch [21800], Loss: 4.0784, Accuracy: 0.2119\n",
            "Epoch [2/7], Batch [21900], Loss: 4.0286, Accuracy: 0.2238\n",
            "Epoch [2/7], Batch [22000], Loss: 4.1674, Accuracy: 0.2298\n",
            "Epoch [2/7], Batch [22100], Loss: 4.3923, Accuracy: 0.1881\n",
            "Epoch [2/7], Batch [22200], Loss: 4.1988, Accuracy: 0.2060\n",
            "Epoch [2/7], Batch [22300], Loss: 3.9106, Accuracy: 0.2500\n",
            "Epoch [2/7], Batch [22400], Loss: 4.5203, Accuracy: 0.1845\n",
            "Epoch [2/7], Batch [22500], Loss: 4.1822, Accuracy: 0.2238\n",
            "Epoch [2/7], Batch [22600], Loss: 4.4278, Accuracy: 0.1893\n",
            "Epoch [2/7], Batch [22700], Loss: 4.2892, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [22800], Loss: 4.2749, Accuracy: 0.2226\n",
            "Epoch [2/7], Batch [22900], Loss: 4.4162, Accuracy: 0.1976\n",
            "Epoch [2/7], Batch [23000], Loss: 4.3745, Accuracy: 0.2143\n",
            "Epoch [2/7], Batch [23100], Loss: 4.0959, Accuracy: 0.2190\n",
            "Epoch [2/7], Batch [23200], Loss: 4.2411, Accuracy: 0.2202\n",
            "Epoch [2/7], Batch [23300], Loss: 4.1747, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [23400], Loss: 4.3537, Accuracy: 0.2107\n",
            "Epoch [2/7], Batch [23500], Loss: 4.4117, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [23600], Loss: 4.2472, Accuracy: 0.2131\n",
            "Epoch [2/7], Batch [23700], Loss: 4.0748, Accuracy: 0.2321\n",
            "Epoch [2/7], Batch [23800], Loss: 4.2864, Accuracy: 0.2012\n",
            "Epoch [2/7], Batch [23900], Loss: 4.1818, Accuracy: 0.2250\n",
            "Epoch [2/7], Batch [24000], Loss: 3.9992, Accuracy: 0.2440\n",
            "Epoch [2/7], Batch [24100], Loss: 4.2299, Accuracy: 0.1917\n",
            "Epoch [2/7], Batch [24200], Loss: 3.9743, Accuracy: 0.2452\n",
            "Epoch [2/7], Batch [24300], Loss: 4.2565, Accuracy: 0.2155\n",
            "Epoch [2/7], Batch [24400], Loss: 4.0650, Accuracy: 0.2298\n",
            "Epoch [2/7], Batch [24500], Loss: 4.0582, Accuracy: 0.2310\n",
            "Epoch [2/7], Batch [24600], Loss: 4.1048, Accuracy: 0.2250\n",
            "Epoch [2/7], Batch [24700], Loss: 4.1024, Accuracy: 0.2179\n",
            "Epoch [2/7], Batch [24800], Loss: 4.3174, Accuracy: 0.2095\n",
            "Epoch [2/7], Batch [24900], Loss: 4.2034, Accuracy: 0.2167\n",
            "End of Epoch [2], Average Loss: 4.2636, Average Accuracy: 0.2122, Average Perplexity: nan\n",
            "====================================================================================================\n",
            "Epoch [3/7], Batch [0], Loss: 4.2233, Accuracy: 0.1976\n",
            "Epoch [3/7], Batch [100], Loss: 4.2183, Accuracy: 0.2202\n",
            "Epoch [3/7], Batch [200], Loss: 3.9754, Accuracy: 0.2417\n",
            "Epoch [3/7], Batch [300], Loss: 4.3681, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [400], Loss: 4.4709, Accuracy: 0.2036\n",
            "Epoch [3/7], Batch [500], Loss: 4.3386, Accuracy: 0.2036\n",
            "Epoch [3/7], Batch [600], Loss: 4.3201, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [700], Loss: 4.3454, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [800], Loss: 4.2584, Accuracy: 0.2119\n",
            "Epoch [3/7], Batch [900], Loss: 4.1240, Accuracy: 0.2250\n",
            "Epoch [3/7], Batch [1000], Loss: 4.0221, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [1100], Loss: 4.1765, Accuracy: 0.2083\n",
            "Epoch [3/7], Batch [1200], Loss: 4.1084, Accuracy: 0.2333\n",
            "Epoch [3/7], Batch [1300], Loss: 4.2293, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [1400], Loss: 4.0600, Accuracy: 0.2369\n",
            "Epoch [3/7], Batch [1500], Loss: 4.3004, Accuracy: 0.2012\n",
            "Epoch [3/7], Batch [1600], Loss: 3.9761, Accuracy: 0.2381\n",
            "Epoch [3/7], Batch [1700], Loss: 4.1948, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [1800], Loss: 4.1620, Accuracy: 0.2190\n",
            "Epoch [3/7], Batch [1900], Loss: 3.9798, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [2000], Loss: 3.8541, Accuracy: 0.2667\n",
            "Epoch [3/7], Batch [2100], Loss: 4.3821, Accuracy: 0.2036\n",
            "Epoch [3/7], Batch [2200], Loss: 4.1414, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [2300], Loss: 3.8621, Accuracy: 0.2417\n",
            "Epoch [3/7], Batch [2400], Loss: 4.0418, Accuracy: 0.2036\n",
            "Epoch [3/7], Batch [2500], Loss: 4.1163, Accuracy: 0.2155\n",
            "Epoch [3/7], Batch [2600], Loss: 4.1573, Accuracy: 0.2143\n",
            "Epoch [3/7], Batch [2700], Loss: 4.0939, Accuracy: 0.2500\n",
            "Epoch [3/7], Batch [2800], Loss: 4.0020, Accuracy: 0.2464\n",
            "Epoch [3/7], Batch [2900], Loss: 4.2920, Accuracy: 0.2202\n",
            "Epoch [3/7], Batch [3000], Loss: 4.1318, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [3100], Loss: 4.0950, Accuracy: 0.2452\n",
            "Epoch [3/7], Batch [3200], Loss: 4.3165, Accuracy: 0.1667\n",
            "Epoch [3/7], Batch [3300], Loss: 4.0862, Accuracy: 0.2250\n",
            "Epoch [3/7], Batch [3400], Loss: 4.2165, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [3500], Loss: 4.2277, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [3600], Loss: 4.1953, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [3700], Loss: 4.1104, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [3800], Loss: 4.1164, Accuracy: 0.2381\n",
            "Epoch [3/7], Batch [3900], Loss: 4.1135, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [4000], Loss: 4.0000, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [4100], Loss: 4.3228, Accuracy: 0.2190\n",
            "Epoch [3/7], Batch [4200], Loss: 4.1883, Accuracy: 0.2012\n",
            "Epoch [3/7], Batch [4300], Loss: 4.0377, Accuracy: 0.2464\n",
            "Epoch [3/7], Batch [4400], Loss: 4.0342, Accuracy: 0.2310\n",
            "Epoch [3/7], Batch [4500], Loss: 4.2246, Accuracy: 0.2060\n",
            "Epoch [3/7], Batch [4600], Loss: 4.2887, Accuracy: 0.2036\n",
            "Epoch [3/7], Batch [4700], Loss: 4.0120, Accuracy: 0.2369\n",
            "Epoch [3/7], Batch [4800], Loss: 4.2502, Accuracy: 0.2143\n",
            "Epoch [3/7], Batch [4900], Loss: 4.2214, Accuracy: 0.2143\n",
            "Epoch [3/7], Batch [5000], Loss: 4.0144, Accuracy: 0.2452\n",
            "Epoch [3/7], Batch [5100], Loss: 4.2569, Accuracy: 0.2250\n",
            "Epoch [3/7], Batch [5200], Loss: 4.1022, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [5300], Loss: 4.1033, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [5400], Loss: 4.2598, Accuracy: 0.2202\n",
            "Epoch [3/7], Batch [5500], Loss: 4.1350, Accuracy: 0.2417\n",
            "Epoch [3/7], Batch [5600], Loss: 4.3206, Accuracy: 0.2190\n",
            "Epoch [3/7], Batch [5700], Loss: 4.3605, Accuracy: 0.2060\n",
            "Epoch [3/7], Batch [5800], Loss: 4.1691, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [5900], Loss: 4.2443, Accuracy: 0.1940\n",
            "Epoch [3/7], Batch [6000], Loss: 4.3193, Accuracy: 0.1810\n",
            "Epoch [3/7], Batch [6100], Loss: 4.2588, Accuracy: 0.1952\n",
            "Epoch [3/7], Batch [6200], Loss: 4.2668, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [6300], Loss: 3.9292, Accuracy: 0.2262\n",
            "Epoch [3/7], Batch [6400], Loss: 4.0085, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [6500], Loss: 4.2007, Accuracy: 0.2024\n",
            "Epoch [3/7], Batch [6600], Loss: 4.0388, Accuracy: 0.2417\n",
            "Epoch [3/7], Batch [6700], Loss: 4.3326, Accuracy: 0.1940\n",
            "Epoch [3/7], Batch [6800], Loss: 4.3813, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [6900], Loss: 4.0954, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [7000], Loss: 4.2164, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [7100], Loss: 4.2841, Accuracy: 0.1988\n",
            "Epoch [3/7], Batch [7200], Loss: 4.1408, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [7300], Loss: 4.2378, Accuracy: 0.1952\n",
            "Epoch [3/7], Batch [7400], Loss: 4.2592, Accuracy: 0.2095\n",
            "Epoch [3/7], Batch [7500], Loss: 4.0410, Accuracy: 0.2310\n",
            "Epoch [3/7], Batch [7600], Loss: 4.1847, Accuracy: 0.2119\n",
            "Epoch [3/7], Batch [7700], Loss: 4.2543, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [7800], Loss: 4.0454, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [7900], Loss: 4.2448, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [8000], Loss: 4.1734, Accuracy: 0.2071\n",
            "Epoch [3/7], Batch [8100], Loss: 4.2414, Accuracy: 0.1940\n",
            "Epoch [3/7], Batch [8200], Loss: 4.2156, Accuracy: 0.2155\n",
            "Epoch [3/7], Batch [8300], Loss: 4.1567, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [8400], Loss: 4.2038, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [8500], Loss: 4.2473, Accuracy: 0.2202\n",
            "Epoch [3/7], Batch [8600], Loss: 4.1062, Accuracy: 0.2381\n",
            "Epoch [3/7], Batch [8700], Loss: 3.7489, Accuracy: 0.2738\n",
            "Epoch [3/7], Batch [8800], Loss: 4.0189, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [8900], Loss: 4.1030, Accuracy: 0.1964\n",
            "Epoch [3/7], Batch [9000], Loss: 4.0497, Accuracy: 0.2488\n",
            "Epoch [3/7], Batch [9100], Loss: 4.0383, Accuracy: 0.1952\n",
            "Epoch [3/7], Batch [9200], Loss: 4.1088, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [9300], Loss: 4.0789, Accuracy: 0.2071\n",
            "Epoch [3/7], Batch [9400], Loss: 4.2921, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [9500], Loss: 4.1293, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [9600], Loss: 3.8741, Accuracy: 0.2500\n",
            "Epoch [3/7], Batch [9700], Loss: 4.4365, Accuracy: 0.2012\n",
            "Epoch [3/7], Batch [9800], Loss: 4.1579, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [9900], Loss: 4.1310, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [10000], Loss: 4.1608, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [10100], Loss: 3.8583, Accuracy: 0.2667\n",
            "Epoch [3/7], Batch [10200], Loss: 4.0620, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [10300], Loss: 4.2474, Accuracy: 0.1976\n",
            "Epoch [3/7], Batch [10400], Loss: 3.9784, Accuracy: 0.2452\n",
            "Epoch [3/7], Batch [10500], Loss: 4.0895, Accuracy: 0.2452\n",
            "Epoch [3/7], Batch [10600], Loss: 3.9826, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [10700], Loss: 3.9581, Accuracy: 0.2345\n",
            "Epoch [3/7], Batch [10800], Loss: 4.2143, Accuracy: 0.2000\n",
            "Epoch [3/7], Batch [10900], Loss: 4.2681, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [11000], Loss: 4.1173, Accuracy: 0.2048\n",
            "Epoch [3/7], Batch [11100], Loss: 4.3476, Accuracy: 0.2000\n",
            "Epoch [3/7], Batch [11200], Loss: 4.1438, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [11300], Loss: 4.2737, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [11400], Loss: 3.9792, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [11500], Loss: 4.0836, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [11600], Loss: 4.3800, Accuracy: 0.1929\n",
            "Epoch [3/7], Batch [11700], Loss: 4.2060, Accuracy: 0.2333\n",
            "Epoch [3/7], Batch [11800], Loss: 4.1195, Accuracy: 0.2464\n",
            "Epoch [3/7], Batch [11900], Loss: 4.2713, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [12000], Loss: 4.1179, Accuracy: 0.2262\n",
            "Epoch [3/7], Batch [12100], Loss: 3.9736, Accuracy: 0.2619\n",
            "Epoch [3/7], Batch [12200], Loss: 4.3262, Accuracy: 0.1988\n",
            "Epoch [3/7], Batch [12300], Loss: 4.3588, Accuracy: 0.2048\n",
            "Epoch [3/7], Batch [12400], Loss: 4.1465, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [12500], Loss: 4.2970, Accuracy: 0.2369\n",
            "Epoch [3/7], Batch [12600], Loss: 4.1206, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [12700], Loss: 4.1525, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [12800], Loss: 4.0330, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [12900], Loss: 4.3456, Accuracy: 0.1976\n",
            "Epoch [3/7], Batch [13000], Loss: 4.1167, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [13100], Loss: 4.1142, Accuracy: 0.2476\n",
            "Epoch [3/7], Batch [13200], Loss: 4.3419, Accuracy: 0.1952\n",
            "Epoch [3/7], Batch [13300], Loss: 4.3843, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [13400], Loss: 4.2122, Accuracy: 0.2036\n",
            "Epoch [3/7], Batch [13500], Loss: 4.1543, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [13600], Loss: 4.3476, Accuracy: 0.1833\n",
            "Epoch [3/7], Batch [13700], Loss: 4.1589, Accuracy: 0.2012\n",
            "Epoch [3/7], Batch [13800], Loss: 4.1509, Accuracy: 0.2214\n",
            "Epoch [3/7], Batch [13900], Loss: 4.1574, Accuracy: 0.2417\n",
            "Epoch [3/7], Batch [14000], Loss: 3.9328, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [14100], Loss: 4.1823, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [14200], Loss: 3.9929, Accuracy: 0.2536\n",
            "Epoch [3/7], Batch [14300], Loss: 4.1652, Accuracy: 0.2071\n",
            "Epoch [3/7], Batch [14400], Loss: 4.1550, Accuracy: 0.2381\n",
            "Epoch [3/7], Batch [14500], Loss: 4.0532, Accuracy: 0.2345\n",
            "Epoch [3/7], Batch [14600], Loss: 4.1791, Accuracy: 0.2357\n",
            "Epoch [3/7], Batch [14700], Loss: 4.2201, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [14800], Loss: 4.0052, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [14900], Loss: 4.1728, Accuracy: 0.2357\n",
            "Epoch [3/7], Batch [15000], Loss: 3.9733, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [15100], Loss: 4.1726, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [15200], Loss: 4.1294, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [15300], Loss: 4.0885, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [15400], Loss: 4.3617, Accuracy: 0.2119\n",
            "Epoch [3/7], Batch [15500], Loss: 4.2166, Accuracy: 0.2024\n",
            "Epoch [3/7], Batch [15600], Loss: 4.2416, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [15700], Loss: 4.1589, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [15800], Loss: 4.1665, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [15900], Loss: 4.3717, Accuracy: 0.1881\n",
            "Epoch [3/7], Batch [16000], Loss: 4.0917, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [16100], Loss: 4.3831, Accuracy: 0.1905\n",
            "Epoch [3/7], Batch [16200], Loss: 4.2204, Accuracy: 0.1929\n",
            "Epoch [3/7], Batch [16300], Loss: 3.9454, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [16400], Loss: 3.8762, Accuracy: 0.2500\n",
            "Epoch [3/7], Batch [16500], Loss: 4.1738, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [16600], Loss: 4.1433, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [16700], Loss: 4.2891, Accuracy: 0.2095\n",
            "Epoch [3/7], Batch [16800], Loss: 4.1255, Accuracy: 0.2250\n",
            "Epoch [3/7], Batch [16900], Loss: 4.1074, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [17000], Loss: 4.1222, Accuracy: 0.2155\n",
            "Epoch [3/7], Batch [17100], Loss: 4.2074, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [17200], Loss: 4.0818, Accuracy: 0.2310\n",
            "Epoch [3/7], Batch [17300], Loss: 4.0907, Accuracy: 0.2143\n",
            "Epoch [3/7], Batch [17400], Loss: 4.1465, Accuracy: 0.2214\n",
            "Epoch [3/7], Batch [17500], Loss: 4.1147, Accuracy: 0.1952\n",
            "Epoch [3/7], Batch [17600], Loss: 4.1413, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [17700], Loss: 4.0360, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [17800], Loss: 3.9147, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [17900], Loss: 3.8505, Accuracy: 0.2583\n",
            "Epoch [3/7], Batch [18000], Loss: 4.2711, Accuracy: 0.2167\n",
            "Epoch [3/7], Batch [18100], Loss: 4.0883, Accuracy: 0.1988\n",
            "Epoch [3/7], Batch [18200], Loss: 4.1779, Accuracy: 0.2131\n",
            "Epoch [3/7], Batch [18300], Loss: 4.3533, Accuracy: 0.1976\n",
            "Epoch [3/7], Batch [18400], Loss: 3.9038, Accuracy: 0.2738\n",
            "Epoch [3/7], Batch [18500], Loss: 3.8598, Accuracy: 0.2512\n",
            "Epoch [3/7], Batch [18600], Loss: 4.0879, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [18700], Loss: 4.1098, Accuracy: 0.2202\n",
            "Epoch [3/7], Batch [18800], Loss: 4.0449, Accuracy: 0.2238\n",
            "Epoch [3/7], Batch [18900], Loss: 4.1100, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [19000], Loss: 4.1046, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [19100], Loss: 4.1149, Accuracy: 0.2214\n",
            "Epoch [3/7], Batch [19200], Loss: 3.8607, Accuracy: 0.2774\n",
            "Epoch [3/7], Batch [19300], Loss: 3.8466, Accuracy: 0.2905\n",
            "Epoch [3/7], Batch [19400], Loss: 4.0277, Accuracy: 0.2464\n",
            "Epoch [3/7], Batch [19500], Loss: 4.1835, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [19600], Loss: 4.2647, Accuracy: 0.2071\n",
            "Epoch [3/7], Batch [19700], Loss: 4.3226, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [19800], Loss: 4.1539, Accuracy: 0.2107\n",
            "Epoch [3/7], Batch [19900], Loss: 4.0482, Accuracy: 0.2560\n",
            "Epoch [3/7], Batch [20000], Loss: 4.1146, Accuracy: 0.2310\n",
            "Epoch [3/7], Batch [20100], Loss: 4.2146, Accuracy: 0.2190\n",
            "Epoch [3/7], Batch [20200], Loss: 4.0491, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [20300], Loss: 4.2463, Accuracy: 0.1988\n",
            "Epoch [3/7], Batch [20400], Loss: 4.1486, Accuracy: 0.2333\n",
            "Epoch [3/7], Batch [20500], Loss: 4.1884, Accuracy: 0.1988\n",
            "Epoch [3/7], Batch [20600], Loss: 4.2161, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [20700], Loss: 4.0485, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [20800], Loss: 4.1631, Accuracy: 0.2381\n",
            "Epoch [3/7], Batch [20900], Loss: 4.2023, Accuracy: 0.2143\n",
            "Epoch [3/7], Batch [21000], Loss: 4.1749, Accuracy: 0.2226\n",
            "Epoch [3/7], Batch [21100], Loss: 4.2840, Accuracy: 0.1857\n",
            "Epoch [3/7], Batch [21200], Loss: 3.8541, Accuracy: 0.2452\n",
            "Epoch [3/7], Batch [21300], Loss: 4.1271, Accuracy: 0.2417\n",
            "Epoch [3/7], Batch [21400], Loss: 4.1248, Accuracy: 0.2202\n",
            "Epoch [3/7], Batch [21500], Loss: 4.0105, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [21600], Loss: 4.0608, Accuracy: 0.2393\n",
            "Epoch [3/7], Batch [21700], Loss: 4.1696, Accuracy: 0.2119\n",
            "Epoch [3/7], Batch [21800], Loss: 4.1300, Accuracy: 0.2214\n",
            "Epoch [3/7], Batch [21900], Loss: 4.0943, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [22000], Loss: 4.1588, Accuracy: 0.2155\n",
            "Epoch [3/7], Batch [22100], Loss: 4.1603, Accuracy: 0.2250\n",
            "Epoch [3/7], Batch [22200], Loss: 4.0996, Accuracy: 0.2405\n",
            "Epoch [3/7], Batch [22300], Loss: 4.0944, Accuracy: 0.2048\n",
            "Epoch [3/7], Batch [22400], Loss: 4.1468, Accuracy: 0.2262\n",
            "Epoch [3/7], Batch [22500], Loss: 4.1149, Accuracy: 0.2048\n",
            "Epoch [3/7], Batch [22600], Loss: 4.0382, Accuracy: 0.2083\n",
            "Epoch [3/7], Batch [22700], Loss: 4.0688, Accuracy: 0.2179\n",
            "Epoch [3/7], Batch [22800], Loss: 3.8751, Accuracy: 0.2667\n",
            "Epoch [3/7], Batch [22900], Loss: 4.0626, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [23000], Loss: 4.0879, Accuracy: 0.2250\n",
            "Epoch [3/7], Batch [23100], Loss: 3.8592, Accuracy: 0.2690\n",
            "Epoch [3/7], Batch [23200], Loss: 4.0484, Accuracy: 0.2310\n",
            "Epoch [3/7], Batch [23300], Loss: 4.1436, Accuracy: 0.2321\n",
            "Epoch [3/7], Batch [23400], Loss: 4.2185, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [23500], Loss: 4.3679, Accuracy: 0.1976\n",
            "Epoch [3/7], Batch [23600], Loss: 3.8137, Accuracy: 0.2452\n",
            "Epoch [3/7], Batch [23700], Loss: 4.1416, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [23800], Loss: 4.3834, Accuracy: 0.1821\n",
            "Epoch [3/7], Batch [23900], Loss: 4.1132, Accuracy: 0.2274\n",
            "Epoch [3/7], Batch [24000], Loss: 4.0461, Accuracy: 0.2286\n",
            "Epoch [3/7], Batch [24100], Loss: 4.2824, Accuracy: 0.2071\n",
            "Epoch [3/7], Batch [24200], Loss: 4.1143, Accuracy: 0.2214\n",
            "Epoch [3/7], Batch [24300], Loss: 3.9255, Accuracy: 0.2631\n",
            "Epoch [3/7], Batch [24400], Loss: 4.0163, Accuracy: 0.2524\n",
            "Epoch [3/7], Batch [24500], Loss: 4.0734, Accuracy: 0.2476\n",
            "Epoch [3/7], Batch [24600], Loss: 4.2024, Accuracy: 0.2143\n",
            "Epoch [3/7], Batch [24700], Loss: 4.2963, Accuracy: 0.2060\n",
            "Epoch [3/7], Batch [24800], Loss: 4.1313, Accuracy: 0.2000\n",
            "Epoch [3/7], Batch [24900], Loss: 4.2862, Accuracy: 0.2083\n",
            "End of Epoch [3], Average Loss: 4.1482, Average Accuracy: 0.2227, Average Perplexity: nan\n",
            "====================================================================================================\n",
            "Epoch [4/7], Batch [0], Loss: 4.1060, Accuracy: 0.1976\n",
            "Epoch [4/7], Batch [100], Loss: 4.1452, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [200], Loss: 4.1344, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [300], Loss: 3.9635, Accuracy: 0.2405\n",
            "Epoch [4/7], Batch [400], Loss: 4.2941, Accuracy: 0.2012\n",
            "Epoch [4/7], Batch [500], Loss: 4.0376, Accuracy: 0.2440\n",
            "Epoch [4/7], Batch [600], Loss: 4.1111, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [700], Loss: 4.1830, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [800], Loss: 4.0285, Accuracy: 0.2393\n",
            "Epoch [4/7], Batch [900], Loss: 4.1491, Accuracy: 0.2250\n",
            "Epoch [4/7], Batch [1000], Loss: 4.0398, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [1100], Loss: 3.9464, Accuracy: 0.2345\n",
            "Epoch [4/7], Batch [1200], Loss: 4.0125, Accuracy: 0.2345\n",
            "Epoch [4/7], Batch [1300], Loss: 4.1587, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [1400], Loss: 4.2878, Accuracy: 0.2024\n",
            "Epoch [4/7], Batch [1500], Loss: 4.4598, Accuracy: 0.1786\n",
            "Epoch [4/7], Batch [1600], Loss: 4.3553, Accuracy: 0.2060\n",
            "Epoch [4/7], Batch [1700], Loss: 4.1530, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [1800], Loss: 3.9132, Accuracy: 0.2500\n",
            "Epoch [4/7], Batch [1900], Loss: 4.2517, Accuracy: 0.2155\n",
            "Epoch [4/7], Batch [2000], Loss: 4.2995, Accuracy: 0.1988\n",
            "Epoch [4/7], Batch [2100], Loss: 4.0235, Accuracy: 0.2607\n",
            "Epoch [4/7], Batch [2200], Loss: 4.1890, Accuracy: 0.2060\n",
            "Epoch [4/7], Batch [2300], Loss: 4.0473, Accuracy: 0.2571\n",
            "Epoch [4/7], Batch [2400], Loss: 4.0878, Accuracy: 0.2345\n",
            "Epoch [4/7], Batch [2500], Loss: 4.2594, Accuracy: 0.2119\n",
            "Epoch [4/7], Batch [2600], Loss: 3.9701, Accuracy: 0.2464\n",
            "Epoch [4/7], Batch [2700], Loss: 4.2656, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [2800], Loss: 4.0736, Accuracy: 0.2060\n",
            "Epoch [4/7], Batch [2900], Loss: 4.1031, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [3000], Loss: 4.1211, Accuracy: 0.1952\n",
            "Epoch [4/7], Batch [3100], Loss: 4.0883, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [3200], Loss: 4.1025, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [3300], Loss: 4.0258, Accuracy: 0.2333\n",
            "Epoch [4/7], Batch [3400], Loss: 4.1692, Accuracy: 0.2214\n",
            "Epoch [4/7], Batch [3500], Loss: 4.0203, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [3600], Loss: 4.0993, Accuracy: 0.2298\n",
            "Epoch [4/7], Batch [3700], Loss: 4.0884, Accuracy: 0.2095\n",
            "Epoch [4/7], Batch [3800], Loss: 4.2505, Accuracy: 0.1940\n",
            "Epoch [4/7], Batch [3900], Loss: 4.3167, Accuracy: 0.1964\n",
            "Epoch [4/7], Batch [4000], Loss: 3.9856, Accuracy: 0.2429\n",
            "Epoch [4/7], Batch [4100], Loss: 4.0359, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [4200], Loss: 4.1277, Accuracy: 0.2060\n",
            "Epoch [4/7], Batch [4300], Loss: 4.1166, Accuracy: 0.2179\n",
            "Epoch [4/7], Batch [4400], Loss: 4.1132, Accuracy: 0.2250\n",
            "Epoch [4/7], Batch [4500], Loss: 4.2263, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [4600], Loss: 3.8329, Accuracy: 0.2643\n",
            "Epoch [4/7], Batch [4700], Loss: 4.3885, Accuracy: 0.1857\n",
            "Epoch [4/7], Batch [4800], Loss: 3.8313, Accuracy: 0.2750\n",
            "Epoch [4/7], Batch [4900], Loss: 4.3952, Accuracy: 0.1857\n",
            "Epoch [4/7], Batch [5000], Loss: 3.8791, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [5100], Loss: 4.0180, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [5200], Loss: 4.2910, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [5300], Loss: 4.0060, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [5400], Loss: 4.0936, Accuracy: 0.1917\n",
            "Epoch [4/7], Batch [5500], Loss: 3.9325, Accuracy: 0.2583\n",
            "Epoch [4/7], Batch [5600], Loss: 3.8605, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [5700], Loss: 3.9937, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [5800], Loss: 4.0279, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [5900], Loss: 4.2464, Accuracy: 0.1869\n",
            "Epoch [4/7], Batch [6000], Loss: 4.0719, Accuracy: 0.2345\n",
            "Epoch [4/7], Batch [6100], Loss: 4.0674, Accuracy: 0.2179\n",
            "Epoch [4/7], Batch [6200], Loss: 4.1609, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [6300], Loss: 4.1900, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [6400], Loss: 4.0897, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [6500], Loss: 4.1804, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [6600], Loss: 4.0440, Accuracy: 0.2440\n",
            "Epoch [4/7], Batch [6700], Loss: 4.1338, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [6800], Loss: 4.1808, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [6900], Loss: 4.2901, Accuracy: 0.2060\n",
            "Epoch [4/7], Batch [7000], Loss: 4.3595, Accuracy: 0.2012\n",
            "Epoch [4/7], Batch [7100], Loss: 4.1977, Accuracy: 0.2012\n",
            "Epoch [4/7], Batch [7200], Loss: 3.8495, Accuracy: 0.2488\n",
            "Epoch [4/7], Batch [7300], Loss: 4.0810, Accuracy: 0.2155\n",
            "Epoch [4/7], Batch [7400], Loss: 4.0284, Accuracy: 0.2071\n",
            "Epoch [4/7], Batch [7500], Loss: 4.1576, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [7600], Loss: 3.9297, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [7700], Loss: 4.0940, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [7800], Loss: 4.2817, Accuracy: 0.2357\n",
            "Epoch [4/7], Batch [7900], Loss: 4.2016, Accuracy: 0.2119\n",
            "Epoch [4/7], Batch [8000], Loss: 4.3139, Accuracy: 0.1988\n",
            "Epoch [4/7], Batch [8100], Loss: 3.9851, Accuracy: 0.2286\n",
            "Epoch [4/7], Batch [8200], Loss: 4.1840, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [8300], Loss: 3.8588, Accuracy: 0.2357\n",
            "Epoch [4/7], Batch [8400], Loss: 3.9839, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [8500], Loss: 4.1392, Accuracy: 0.2452\n",
            "Epoch [4/7], Batch [8600], Loss: 4.0709, Accuracy: 0.2071\n",
            "Epoch [4/7], Batch [8700], Loss: 4.3261, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [8800], Loss: 4.1054, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [8900], Loss: 4.0104, Accuracy: 0.2393\n",
            "Epoch [4/7], Batch [9000], Loss: 4.1631, Accuracy: 0.2119\n",
            "Epoch [4/7], Batch [9100], Loss: 4.1177, Accuracy: 0.1964\n",
            "Epoch [4/7], Batch [9200], Loss: 3.9714, Accuracy: 0.2500\n",
            "Epoch [4/7], Batch [9300], Loss: 4.1512, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [9400], Loss: 4.1810, Accuracy: 0.2095\n",
            "Epoch [4/7], Batch [9500], Loss: 3.8489, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [9600], Loss: 4.0576, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [9700], Loss: 4.1141, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [9800], Loss: 4.0293, Accuracy: 0.2310\n",
            "Epoch [4/7], Batch [9900], Loss: 3.8738, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [10000], Loss: 3.8890, Accuracy: 0.2738\n",
            "Epoch [4/7], Batch [10100], Loss: 4.1855, Accuracy: 0.2119\n",
            "Epoch [4/7], Batch [10200], Loss: 4.1719, Accuracy: 0.2250\n",
            "Epoch [4/7], Batch [10300], Loss: 4.2509, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [10400], Loss: 3.7794, Accuracy: 0.2607\n",
            "Epoch [4/7], Batch [10500], Loss: 4.1774, Accuracy: 0.2155\n",
            "Epoch [4/7], Batch [10600], Loss: 3.9958, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [10700], Loss: 4.0390, Accuracy: 0.2381\n",
            "Epoch [4/7], Batch [10800], Loss: 4.1205, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [10900], Loss: 4.0953, Accuracy: 0.2333\n",
            "Epoch [4/7], Batch [11000], Loss: 4.0581, Accuracy: 0.2369\n",
            "Epoch [4/7], Batch [11100], Loss: 4.0080, Accuracy: 0.2512\n",
            "Epoch [4/7], Batch [11200], Loss: 4.1344, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [11300], Loss: 4.1307, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [11400], Loss: 4.1967, Accuracy: 0.2250\n",
            "Epoch [4/7], Batch [11500], Loss: 4.0670, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [11600], Loss: 4.3055, Accuracy: 0.2179\n",
            "Epoch [4/7], Batch [11700], Loss: 4.2295, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [11800], Loss: 4.3170, Accuracy: 0.2024\n",
            "Epoch [4/7], Batch [11900], Loss: 4.0118, Accuracy: 0.2393\n",
            "Epoch [4/7], Batch [12000], Loss: 4.0971, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [12100], Loss: 4.0926, Accuracy: 0.2476\n",
            "Epoch [4/7], Batch [12200], Loss: 4.1612, Accuracy: 0.1988\n",
            "Epoch [4/7], Batch [12300], Loss: 3.8843, Accuracy: 0.2702\n",
            "Epoch [4/7], Batch [12400], Loss: 4.2024, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [12500], Loss: 4.2856, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [12600], Loss: 4.1749, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [12700], Loss: 4.2097, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [12800], Loss: 4.1331, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [12900], Loss: 4.1372, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [13000], Loss: 3.9553, Accuracy: 0.2571\n",
            "Epoch [4/7], Batch [13100], Loss: 4.0754, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [13200], Loss: 4.0271, Accuracy: 0.2452\n",
            "Epoch [4/7], Batch [13300], Loss: 4.1572, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [13400], Loss: 4.0951, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [13500], Loss: 4.0893, Accuracy: 0.2405\n",
            "Epoch [4/7], Batch [13600], Loss: 4.1487, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [13700], Loss: 4.2946, Accuracy: 0.2012\n",
            "Epoch [4/7], Batch [13800], Loss: 4.0229, Accuracy: 0.2631\n",
            "Epoch [4/7], Batch [13900], Loss: 3.8646, Accuracy: 0.2214\n",
            "Epoch [4/7], Batch [14000], Loss: 3.9993, Accuracy: 0.2452\n",
            "Epoch [4/7], Batch [14100], Loss: 4.3145, Accuracy: 0.2012\n",
            "Epoch [4/7], Batch [14200], Loss: 4.1371, Accuracy: 0.1940\n",
            "Epoch [4/7], Batch [14300], Loss: 4.1192, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [14400], Loss: 4.0750, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [14500], Loss: 3.8841, Accuracy: 0.2655\n",
            "Epoch [4/7], Batch [14600], Loss: 4.1164, Accuracy: 0.2298\n",
            "Epoch [4/7], Batch [14700], Loss: 4.0767, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [14800], Loss: 4.0603, Accuracy: 0.2560\n",
            "Epoch [4/7], Batch [14900], Loss: 3.9289, Accuracy: 0.2690\n",
            "Epoch [4/7], Batch [15000], Loss: 3.9991, Accuracy: 0.2583\n",
            "Epoch [4/7], Batch [15100], Loss: 4.2429, Accuracy: 0.2012\n",
            "Epoch [4/7], Batch [15200], Loss: 4.1368, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [15300], Loss: 4.0762, Accuracy: 0.1976\n",
            "Epoch [4/7], Batch [15400], Loss: 3.9961, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [15500], Loss: 4.0296, Accuracy: 0.2214\n",
            "Epoch [4/7], Batch [15600], Loss: 3.9287, Accuracy: 0.2452\n",
            "Epoch [4/7], Batch [15700], Loss: 4.4273, Accuracy: 0.2048\n",
            "Epoch [4/7], Batch [15800], Loss: 4.1074, Accuracy: 0.2357\n",
            "Epoch [4/7], Batch [15900], Loss: 4.0637, Accuracy: 0.2369\n",
            "Epoch [4/7], Batch [16000], Loss: 4.0392, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [16100], Loss: 4.2504, Accuracy: 0.2155\n",
            "Epoch [4/7], Batch [16200], Loss: 4.0299, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [16300], Loss: 4.0923, Accuracy: 0.2179\n",
            "Epoch [4/7], Batch [16400], Loss: 4.2859, Accuracy: 0.2202\n",
            "Epoch [4/7], Batch [16500], Loss: 4.0799, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [16600], Loss: 4.1642, Accuracy: 0.2357\n",
            "Epoch [4/7], Batch [16700], Loss: 4.2395, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [16800], Loss: 3.9839, Accuracy: 0.2369\n",
            "Epoch [4/7], Batch [16900], Loss: 3.9990, Accuracy: 0.2310\n",
            "Epoch [4/7], Batch [17000], Loss: 4.0666, Accuracy: 0.2083\n",
            "Epoch [4/7], Batch [17100], Loss: 4.2424, Accuracy: 0.2071\n",
            "Epoch [4/7], Batch [17200], Loss: 4.2588, Accuracy: 0.1964\n",
            "Epoch [4/7], Batch [17300], Loss: 3.6929, Accuracy: 0.2655\n",
            "Epoch [4/7], Batch [17400], Loss: 4.1278, Accuracy: 0.2095\n",
            "Epoch [4/7], Batch [17500], Loss: 4.1077, Accuracy: 0.2429\n",
            "Epoch [4/7], Batch [17600], Loss: 4.0664, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [17700], Loss: 3.9206, Accuracy: 0.2571\n",
            "Epoch [4/7], Batch [17800], Loss: 4.1459, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [17900], Loss: 4.0115, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [18000], Loss: 4.1760, Accuracy: 0.1988\n",
            "Epoch [4/7], Batch [18100], Loss: 4.1170, Accuracy: 0.2036\n",
            "Epoch [4/7], Batch [18200], Loss: 4.1660, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [18300], Loss: 3.9755, Accuracy: 0.2310\n",
            "Epoch [4/7], Batch [18400], Loss: 4.1779, Accuracy: 0.2071\n",
            "Epoch [4/7], Batch [18500], Loss: 4.0947, Accuracy: 0.2333\n",
            "Epoch [4/7], Batch [18600], Loss: 4.0621, Accuracy: 0.2024\n",
            "Epoch [4/7], Batch [18700], Loss: 3.9595, Accuracy: 0.2655\n",
            "Epoch [4/7], Batch [18800], Loss: 3.9238, Accuracy: 0.2595\n",
            "Epoch [4/7], Batch [18900], Loss: 4.1071, Accuracy: 0.2274\n",
            "Epoch [4/7], Batch [19000], Loss: 4.0093, Accuracy: 0.2214\n",
            "Epoch [4/7], Batch [19100], Loss: 3.9953, Accuracy: 0.2512\n",
            "Epoch [4/7], Batch [19200], Loss: 3.9439, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [19300], Loss: 3.9362, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [19400], Loss: 4.1044, Accuracy: 0.2345\n",
            "Epoch [4/7], Batch [19500], Loss: 4.2179, Accuracy: 0.2155\n",
            "Epoch [4/7], Batch [19600], Loss: 4.1151, Accuracy: 0.2131\n",
            "Epoch [4/7], Batch [19700], Loss: 4.2154, Accuracy: 0.2000\n",
            "Epoch [4/7], Batch [19800], Loss: 4.2270, Accuracy: 0.2024\n",
            "Epoch [4/7], Batch [19900], Loss: 3.9696, Accuracy: 0.2381\n",
            "Epoch [4/7], Batch [20000], Loss: 3.8261, Accuracy: 0.2238\n",
            "Epoch [4/7], Batch [20100], Loss: 4.0755, Accuracy: 0.2357\n",
            "Epoch [4/7], Batch [20200], Loss: 4.2220, Accuracy: 0.2155\n",
            "Epoch [4/7], Batch [20300], Loss: 4.0794, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [20400], Loss: 4.2396, Accuracy: 0.2000\n",
            "Epoch [4/7], Batch [20500], Loss: 4.0286, Accuracy: 0.2464\n",
            "Epoch [4/7], Batch [20600], Loss: 3.9227, Accuracy: 0.2345\n",
            "Epoch [4/7], Batch [20700], Loss: 3.9723, Accuracy: 0.2452\n",
            "Epoch [4/7], Batch [20800], Loss: 4.0095, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [20900], Loss: 4.1378, Accuracy: 0.2179\n",
            "Epoch [4/7], Batch [21000], Loss: 3.7777, Accuracy: 0.2548\n",
            "Epoch [4/7], Batch [21100], Loss: 3.9942, Accuracy: 0.2476\n",
            "Epoch [4/7], Batch [21200], Loss: 4.1103, Accuracy: 0.2274\n",
            "Epoch [4/7], Batch [21300], Loss: 4.1259, Accuracy: 0.2190\n",
            "Epoch [4/7], Batch [21400], Loss: 4.1947, Accuracy: 0.2274\n",
            "Epoch [4/7], Batch [21500], Loss: 3.9737, Accuracy: 0.2381\n",
            "Epoch [4/7], Batch [21600], Loss: 4.2155, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [21700], Loss: 4.1860, Accuracy: 0.2298\n",
            "Epoch [4/7], Batch [21800], Loss: 4.3135, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [21900], Loss: 4.1806, Accuracy: 0.2095\n",
            "Epoch [4/7], Batch [22000], Loss: 3.9997, Accuracy: 0.2250\n",
            "Epoch [4/7], Batch [22100], Loss: 3.9529, Accuracy: 0.2429\n",
            "Epoch [4/7], Batch [22200], Loss: 3.9479, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [22300], Loss: 3.9788, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [22400], Loss: 4.1661, Accuracy: 0.2262\n",
            "Epoch [4/7], Batch [22500], Loss: 4.0363, Accuracy: 0.2310\n",
            "Epoch [4/7], Batch [22600], Loss: 3.9132, Accuracy: 0.2417\n",
            "Epoch [4/7], Batch [22700], Loss: 4.1486, Accuracy: 0.2167\n",
            "Epoch [4/7], Batch [22800], Loss: 3.8854, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [22900], Loss: 4.1552, Accuracy: 0.2226\n",
            "Epoch [4/7], Batch [23000], Loss: 4.0215, Accuracy: 0.2274\n",
            "Epoch [4/7], Batch [23100], Loss: 4.1062, Accuracy: 0.2024\n",
            "Epoch [4/7], Batch [23200], Loss: 4.2945, Accuracy: 0.2095\n",
            "Epoch [4/7], Batch [23300], Loss: 3.9352, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [23400], Loss: 4.2082, Accuracy: 0.1976\n",
            "Epoch [4/7], Batch [23500], Loss: 4.0626, Accuracy: 0.2107\n",
            "Epoch [4/7], Batch [23600], Loss: 4.0220, Accuracy: 0.2536\n",
            "Epoch [4/7], Batch [23700], Loss: 4.0259, Accuracy: 0.2607\n",
            "Epoch [4/7], Batch [23800], Loss: 4.0715, Accuracy: 0.2310\n",
            "Epoch [4/7], Batch [23900], Loss: 4.0581, Accuracy: 0.2250\n",
            "Epoch [4/7], Batch [24000], Loss: 4.2055, Accuracy: 0.2143\n",
            "Epoch [4/7], Batch [24100], Loss: 4.0115, Accuracy: 0.2286\n",
            "Epoch [4/7], Batch [24200], Loss: 3.9285, Accuracy: 0.2607\n",
            "Epoch [4/7], Batch [24300], Loss: 4.0765, Accuracy: 0.2286\n",
            "Epoch [4/7], Batch [24400], Loss: 4.2817, Accuracy: 0.2036\n",
            "Epoch [4/7], Batch [24500], Loss: 4.1120, Accuracy: 0.2214\n",
            "Epoch [4/7], Batch [24600], Loss: 3.9645, Accuracy: 0.2321\n",
            "Epoch [4/7], Batch [24700], Loss: 3.9690, Accuracy: 0.2488\n",
            "Epoch [4/7], Batch [24800], Loss: 4.0452, Accuracy: 0.2310\n",
            "Epoch [4/7], Batch [24900], Loss: 4.0321, Accuracy: 0.2310\n",
            "End of Epoch [4], Average Loss: 4.0894, Average Accuracy: 0.2273, Average Perplexity: nan\n",
            "====================================================================================================\n",
            "Epoch [5/7], Batch [0], Loss: 4.1499, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [100], Loss: 4.0046, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [200], Loss: 4.1612, Accuracy: 0.2226\n",
            "Epoch [5/7], Batch [300], Loss: 3.8438, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [400], Loss: 4.1585, Accuracy: 0.2143\n",
            "Epoch [5/7], Batch [500], Loss: 4.2602, Accuracy: 0.1905\n",
            "Epoch [5/7], Batch [600], Loss: 4.1954, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [700], Loss: 4.1905, Accuracy: 0.2155\n",
            "Epoch [5/7], Batch [800], Loss: 4.0957, Accuracy: 0.2452\n",
            "Epoch [5/7], Batch [900], Loss: 4.0687, Accuracy: 0.2452\n",
            "Epoch [5/7], Batch [1000], Loss: 4.2856, Accuracy: 0.2036\n",
            "Epoch [5/7], Batch [1100], Loss: 4.0203, Accuracy: 0.2595\n",
            "Epoch [5/7], Batch [1200], Loss: 4.1573, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [1300], Loss: 4.0776, Accuracy: 0.2262\n",
            "Epoch [5/7], Batch [1400], Loss: 3.9722, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [1500], Loss: 4.0394, Accuracy: 0.2071\n",
            "Epoch [5/7], Batch [1600], Loss: 3.8637, Accuracy: 0.2667\n",
            "Epoch [5/7], Batch [1700], Loss: 4.1946, Accuracy: 0.2274\n",
            "Epoch [5/7], Batch [1800], Loss: 3.8456, Accuracy: 0.2298\n",
            "Epoch [5/7], Batch [1900], Loss: 3.9964, Accuracy: 0.2417\n",
            "Epoch [5/7], Batch [2000], Loss: 3.9986, Accuracy: 0.2536\n",
            "Epoch [5/7], Batch [2100], Loss: 3.8541, Accuracy: 0.2488\n",
            "Epoch [5/7], Batch [2200], Loss: 4.0118, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [2300], Loss: 4.1183, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [2400], Loss: 4.0714, Accuracy: 0.2190\n",
            "Epoch [5/7], Batch [2500], Loss: 4.0635, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [2600], Loss: 4.2956, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [2700], Loss: 3.9075, Accuracy: 0.2440\n",
            "Epoch [5/7], Batch [2800], Loss: 4.1878, Accuracy: 0.2214\n",
            "Epoch [5/7], Batch [2900], Loss: 3.9744, Accuracy: 0.2298\n",
            "Epoch [5/7], Batch [3000], Loss: 4.0155, Accuracy: 0.2643\n",
            "Epoch [5/7], Batch [3100], Loss: 4.1252, Accuracy: 0.2190\n",
            "Epoch [5/7], Batch [3200], Loss: 4.0661, Accuracy: 0.2381\n",
            "Epoch [5/7], Batch [3300], Loss: 4.0387, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [3400], Loss: 4.2444, Accuracy: 0.2095\n",
            "Epoch [5/7], Batch [3500], Loss: 4.0843, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [3600], Loss: 4.1891, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [3700], Loss: 4.0538, Accuracy: 0.2429\n",
            "Epoch [5/7], Batch [3800], Loss: 4.2243, Accuracy: 0.1929\n",
            "Epoch [5/7], Batch [3900], Loss: 3.9772, Accuracy: 0.2226\n",
            "Epoch [5/7], Batch [4000], Loss: 4.0370, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [4100], Loss: 4.0888, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [4200], Loss: 4.2176, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [4300], Loss: 4.0517, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [4400], Loss: 4.0124, Accuracy: 0.2095\n",
            "Epoch [5/7], Batch [4500], Loss: 3.8600, Accuracy: 0.2476\n",
            "Epoch [5/7], Batch [4600], Loss: 3.9812, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [4700], Loss: 3.9003, Accuracy: 0.2512\n",
            "Epoch [5/7], Batch [4800], Loss: 3.8388, Accuracy: 0.2655\n",
            "Epoch [5/7], Batch [4900], Loss: 4.2392, Accuracy: 0.2167\n",
            "Epoch [5/7], Batch [5000], Loss: 4.0912, Accuracy: 0.2036\n",
            "Epoch [5/7], Batch [5100], Loss: 4.1538, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [5200], Loss: 3.9299, Accuracy: 0.2405\n",
            "Epoch [5/7], Batch [5300], Loss: 3.8086, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [5400], Loss: 4.1611, Accuracy: 0.2012\n",
            "Epoch [5/7], Batch [5500], Loss: 4.1941, Accuracy: 0.2119\n",
            "Epoch [5/7], Batch [5600], Loss: 3.9127, Accuracy: 0.2393\n",
            "Epoch [5/7], Batch [5700], Loss: 4.0253, Accuracy: 0.2143\n",
            "Epoch [5/7], Batch [5800], Loss: 4.1541, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [5900], Loss: 3.9597, Accuracy: 0.2607\n",
            "Epoch [5/7], Batch [6000], Loss: 4.1829, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [6100], Loss: 3.9520, Accuracy: 0.2238\n",
            "Epoch [5/7], Batch [6200], Loss: 4.0439, Accuracy: 0.2238\n",
            "Epoch [5/7], Batch [6300], Loss: 4.1015, Accuracy: 0.2262\n",
            "Epoch [5/7], Batch [6400], Loss: 3.7413, Accuracy: 0.2500\n",
            "Epoch [5/7], Batch [6500], Loss: 4.0915, Accuracy: 0.2333\n",
            "Epoch [5/7], Batch [6600], Loss: 4.0671, Accuracy: 0.1976\n",
            "Epoch [5/7], Batch [6700], Loss: 4.0244, Accuracy: 0.2179\n",
            "Epoch [5/7], Batch [6800], Loss: 3.8214, Accuracy: 0.2381\n",
            "Epoch [5/7], Batch [6900], Loss: 3.9218, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [7000], Loss: 4.2500, Accuracy: 0.2250\n",
            "Epoch [5/7], Batch [7100], Loss: 4.2564, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [7200], Loss: 3.7614, Accuracy: 0.2679\n",
            "Epoch [5/7], Batch [7300], Loss: 4.2593, Accuracy: 0.2155\n",
            "Epoch [5/7], Batch [7400], Loss: 4.1144, Accuracy: 0.2274\n",
            "Epoch [5/7], Batch [7500], Loss: 3.9604, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [7600], Loss: 4.0305, Accuracy: 0.2250\n",
            "Epoch [5/7], Batch [7700], Loss: 3.9975, Accuracy: 0.2548\n",
            "Epoch [5/7], Batch [7800], Loss: 3.9084, Accuracy: 0.2393\n",
            "Epoch [5/7], Batch [7900], Loss: 3.9655, Accuracy: 0.2512\n",
            "Epoch [5/7], Batch [8000], Loss: 3.9635, Accuracy: 0.2571\n",
            "Epoch [5/7], Batch [8100], Loss: 3.9834, Accuracy: 0.2488\n",
            "Epoch [5/7], Batch [8200], Loss: 4.1613, Accuracy: 0.2393\n",
            "Epoch [5/7], Batch [8300], Loss: 3.9226, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [8400], Loss: 4.2064, Accuracy: 0.1869\n",
            "Epoch [5/7], Batch [8500], Loss: 4.1580, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [8600], Loss: 4.0751, Accuracy: 0.2167\n",
            "Epoch [5/7], Batch [8700], Loss: 4.1345, Accuracy: 0.2107\n",
            "Epoch [5/7], Batch [8800], Loss: 4.2274, Accuracy: 0.2048\n",
            "Epoch [5/7], Batch [8900], Loss: 3.6972, Accuracy: 0.2857\n",
            "Epoch [5/7], Batch [9000], Loss: 4.1004, Accuracy: 0.2024\n",
            "Epoch [5/7], Batch [9100], Loss: 4.0302, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [9200], Loss: 4.0092, Accuracy: 0.2452\n",
            "Epoch [5/7], Batch [9300], Loss: 4.0539, Accuracy: 0.2155\n",
            "Epoch [5/7], Batch [9400], Loss: 4.0104, Accuracy: 0.2262\n",
            "Epoch [5/7], Batch [9500], Loss: 3.8980, Accuracy: 0.2333\n",
            "Epoch [5/7], Batch [9600], Loss: 3.9511, Accuracy: 0.2381\n",
            "Epoch [5/7], Batch [9700], Loss: 3.9462, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [9800], Loss: 3.8249, Accuracy: 0.2786\n",
            "Epoch [5/7], Batch [9900], Loss: 4.1548, Accuracy: 0.2381\n",
            "Epoch [5/7], Batch [10000], Loss: 3.7371, Accuracy: 0.2560\n",
            "Epoch [5/7], Batch [10100], Loss: 4.0899, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [10200], Loss: 4.0653, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [10300], Loss: 4.0435, Accuracy: 0.2333\n",
            "Epoch [5/7], Batch [10400], Loss: 4.1345, Accuracy: 0.2238\n",
            "Epoch [5/7], Batch [10500], Loss: 4.0991, Accuracy: 0.2226\n",
            "Epoch [5/7], Batch [10600], Loss: 3.9882, Accuracy: 0.2250\n",
            "Epoch [5/7], Batch [10700], Loss: 3.9701, Accuracy: 0.2286\n",
            "Epoch [5/7], Batch [10800], Loss: 4.1462, Accuracy: 0.2190\n",
            "Epoch [5/7], Batch [10900], Loss: 3.8535, Accuracy: 0.2679\n",
            "Epoch [5/7], Batch [11000], Loss: 3.9934, Accuracy: 0.2226\n",
            "Epoch [5/7], Batch [11100], Loss: 4.1022, Accuracy: 0.2274\n",
            "Epoch [5/7], Batch [11200], Loss: 3.9748, Accuracy: 0.2298\n",
            "Epoch [5/7], Batch [11300], Loss: 4.0492, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [11400], Loss: 4.1437, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [11500], Loss: 3.9000, Accuracy: 0.2393\n",
            "Epoch [5/7], Batch [11600], Loss: 4.0278, Accuracy: 0.2274\n",
            "Epoch [5/7], Batch [11700], Loss: 3.9270, Accuracy: 0.2381\n",
            "Epoch [5/7], Batch [11800], Loss: 4.1500, Accuracy: 0.2440\n",
            "Epoch [5/7], Batch [11900], Loss: 4.1585, Accuracy: 0.2107\n",
            "Epoch [5/7], Batch [12000], Loss: 4.1914, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [12100], Loss: 4.0004, Accuracy: 0.2476\n",
            "Epoch [5/7], Batch [12200], Loss: 4.1999, Accuracy: 0.2179\n",
            "Epoch [5/7], Batch [12300], Loss: 4.1259, Accuracy: 0.2286\n",
            "Epoch [5/7], Batch [12400], Loss: 3.8047, Accuracy: 0.2524\n",
            "Epoch [5/7], Batch [12500], Loss: 4.2492, Accuracy: 0.2214\n",
            "Epoch [5/7], Batch [12600], Loss: 3.9952, Accuracy: 0.2571\n",
            "Epoch [5/7], Batch [12700], Loss: 4.1100, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [12800], Loss: 4.1774, Accuracy: 0.2060\n",
            "Epoch [5/7], Batch [12900], Loss: 4.1229, Accuracy: 0.2452\n",
            "Epoch [5/7], Batch [13000], Loss: 3.9137, Accuracy: 0.2488\n",
            "Epoch [5/7], Batch [13100], Loss: 3.9357, Accuracy: 0.2476\n",
            "Epoch [5/7], Batch [13200], Loss: 4.1638, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [13300], Loss: 3.9246, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [13400], Loss: 3.9328, Accuracy: 0.2226\n",
            "Epoch [5/7], Batch [13500], Loss: 4.0548, Accuracy: 0.2286\n",
            "Epoch [5/7], Batch [13600], Loss: 3.9733, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [13700], Loss: 4.2837, Accuracy: 0.1988\n",
            "Epoch [5/7], Batch [13800], Loss: 4.1400, Accuracy: 0.2060\n",
            "Epoch [5/7], Batch [13900], Loss: 4.1134, Accuracy: 0.2369\n",
            "Epoch [5/7], Batch [14000], Loss: 4.1541, Accuracy: 0.2298\n",
            "Epoch [5/7], Batch [14100], Loss: 4.1068, Accuracy: 0.2250\n",
            "Epoch [5/7], Batch [14200], Loss: 4.0963, Accuracy: 0.2048\n",
            "Epoch [5/7], Batch [14300], Loss: 4.1682, Accuracy: 0.2036\n",
            "Epoch [5/7], Batch [14400], Loss: 3.9606, Accuracy: 0.2571\n",
            "Epoch [5/7], Batch [14500], Loss: 3.9060, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [14600], Loss: 4.0926, Accuracy: 0.2262\n",
            "Epoch [5/7], Batch [14700], Loss: 4.2538, Accuracy: 0.2060\n",
            "Epoch [5/7], Batch [14800], Loss: 3.9404, Accuracy: 0.2560\n",
            "Epoch [5/7], Batch [14900], Loss: 3.9486, Accuracy: 0.2583\n",
            "Epoch [5/7], Batch [15000], Loss: 4.1014, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [15100], Loss: 4.1333, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [15200], Loss: 4.1134, Accuracy: 0.2333\n",
            "Epoch [5/7], Batch [15300], Loss: 4.2855, Accuracy: 0.1964\n",
            "Epoch [5/7], Batch [15400], Loss: 3.9898, Accuracy: 0.2440\n",
            "Epoch [5/7], Batch [15500], Loss: 4.2165, Accuracy: 0.2119\n",
            "Epoch [5/7], Batch [15600], Loss: 3.8115, Accuracy: 0.2524\n",
            "Epoch [5/7], Batch [15700], Loss: 4.1433, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [15800], Loss: 4.0249, Accuracy: 0.2488\n",
            "Epoch [5/7], Batch [15900], Loss: 4.1334, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [16000], Loss: 4.0584, Accuracy: 0.2452\n",
            "Epoch [5/7], Batch [16100], Loss: 4.1800, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [16200], Loss: 4.1815, Accuracy: 0.2095\n",
            "Epoch [5/7], Batch [16300], Loss: 4.0736, Accuracy: 0.2536\n",
            "Epoch [5/7], Batch [16400], Loss: 3.9765, Accuracy: 0.2548\n",
            "Epoch [5/7], Batch [16500], Loss: 4.0361, Accuracy: 0.2571\n",
            "Epoch [5/7], Batch [16600], Loss: 4.1924, Accuracy: 0.2107\n",
            "Epoch [5/7], Batch [16700], Loss: 4.0947, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [16800], Loss: 4.1374, Accuracy: 0.2238\n",
            "Epoch [5/7], Batch [16900], Loss: 4.2207, Accuracy: 0.2095\n",
            "Epoch [5/7], Batch [17000], Loss: 4.0392, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [17100], Loss: 4.0433, Accuracy: 0.2274\n",
            "Epoch [5/7], Batch [17200], Loss: 3.9925, Accuracy: 0.2369\n",
            "Epoch [5/7], Batch [17300], Loss: 4.1931, Accuracy: 0.2071\n",
            "Epoch [5/7], Batch [17400], Loss: 3.9628, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [17500], Loss: 3.9820, Accuracy: 0.2417\n",
            "Epoch [5/7], Batch [17600], Loss: 4.0198, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [17700], Loss: 3.7876, Accuracy: 0.2750\n",
            "Epoch [5/7], Batch [17800], Loss: 4.0525, Accuracy: 0.2167\n",
            "Epoch [5/7], Batch [17900], Loss: 4.0710, Accuracy: 0.2298\n",
            "Epoch [5/7], Batch [18000], Loss: 3.9285, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [18100], Loss: 4.0403, Accuracy: 0.2286\n",
            "Epoch [5/7], Batch [18200], Loss: 3.9261, Accuracy: 0.2524\n",
            "Epoch [5/7], Batch [18300], Loss: 4.0838, Accuracy: 0.2179\n",
            "Epoch [5/7], Batch [18400], Loss: 3.7701, Accuracy: 0.2655\n",
            "Epoch [5/7], Batch [18500], Loss: 4.1476, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [18600], Loss: 4.0535, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [18700], Loss: 4.1446, Accuracy: 0.2012\n",
            "Epoch [5/7], Batch [18800], Loss: 4.2153, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [18900], Loss: 4.2045, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [19000], Loss: 4.1885, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [19100], Loss: 3.7451, Accuracy: 0.2738\n",
            "Epoch [5/7], Batch [19200], Loss: 4.0009, Accuracy: 0.2262\n",
            "Epoch [5/7], Batch [19300], Loss: 3.9303, Accuracy: 0.2476\n",
            "Epoch [5/7], Batch [19400], Loss: 4.0630, Accuracy: 0.2286\n",
            "Epoch [5/7], Batch [19500], Loss: 4.0957, Accuracy: 0.2250\n",
            "Epoch [5/7], Batch [19600], Loss: 3.9032, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [19700], Loss: 3.8627, Accuracy: 0.2452\n",
            "Epoch [5/7], Batch [19800], Loss: 4.2098, Accuracy: 0.2202\n",
            "Epoch [5/7], Batch [19900], Loss: 4.0122, Accuracy: 0.2310\n",
            "Epoch [5/7], Batch [20000], Loss: 4.0066, Accuracy: 0.2417\n",
            "Epoch [5/7], Batch [20100], Loss: 4.2022, Accuracy: 0.2131\n",
            "Epoch [5/7], Batch [20200], Loss: 4.1722, Accuracy: 0.2190\n",
            "Epoch [5/7], Batch [20300], Loss: 4.0292, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [20400], Loss: 4.0483, Accuracy: 0.2417\n",
            "Epoch [5/7], Batch [20500], Loss: 4.0732, Accuracy: 0.2345\n",
            "Epoch [5/7], Batch [20600], Loss: 3.9626, Accuracy: 0.2548\n",
            "Epoch [5/7], Batch [20700], Loss: 4.0459, Accuracy: 0.2250\n",
            "Epoch [5/7], Batch [20800], Loss: 3.9587, Accuracy: 0.2238\n",
            "Epoch [5/7], Batch [20900], Loss: 4.1914, Accuracy: 0.2167\n",
            "Epoch [5/7], Batch [21000], Loss: 4.1087, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [21100], Loss: 4.1715, Accuracy: 0.2417\n",
            "Epoch [5/7], Batch [21200], Loss: 3.9671, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [21300], Loss: 4.0486, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [21400], Loss: 4.2470, Accuracy: 0.2107\n",
            "Epoch [5/7], Batch [21500], Loss: 3.5861, Accuracy: 0.2774\n",
            "Epoch [5/7], Batch [21600], Loss: 4.1908, Accuracy: 0.2143\n",
            "Epoch [5/7], Batch [21700], Loss: 4.2845, Accuracy: 0.1881\n",
            "Epoch [5/7], Batch [21800], Loss: 4.0712, Accuracy: 0.2286\n",
            "Epoch [5/7], Batch [21900], Loss: 4.0965, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [22000], Loss: 4.0924, Accuracy: 0.2190\n",
            "Epoch [5/7], Batch [22100], Loss: 4.0322, Accuracy: 0.2262\n",
            "Epoch [5/7], Batch [22200], Loss: 4.1235, Accuracy: 0.2036\n",
            "Epoch [5/7], Batch [22300], Loss: 4.1693, Accuracy: 0.2321\n",
            "Epoch [5/7], Batch [22400], Loss: 4.1651, Accuracy: 0.2083\n",
            "Epoch [5/7], Batch [22500], Loss: 4.1013, Accuracy: 0.1964\n",
            "Epoch [5/7], Batch [22600], Loss: 4.3369, Accuracy: 0.1810\n",
            "Epoch [5/7], Batch [22700], Loss: 4.1624, Accuracy: 0.2000\n",
            "Epoch [5/7], Batch [22800], Loss: 4.2871, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [22900], Loss: 3.8578, Accuracy: 0.2536\n",
            "Epoch [5/7], Batch [23000], Loss: 4.0658, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [23100], Loss: 4.1288, Accuracy: 0.2440\n",
            "Epoch [5/7], Batch [23200], Loss: 4.0893, Accuracy: 0.2119\n",
            "Epoch [5/7], Batch [23300], Loss: 4.1135, Accuracy: 0.2060\n",
            "Epoch [5/7], Batch [23400], Loss: 3.9829, Accuracy: 0.2357\n",
            "Epoch [5/7], Batch [23500], Loss: 3.9146, Accuracy: 0.2298\n",
            "Epoch [5/7], Batch [23600], Loss: 3.9081, Accuracy: 0.2548\n",
            "Epoch [5/7], Batch [23700], Loss: 3.9877, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [23800], Loss: 4.0898, Accuracy: 0.2369\n",
            "Epoch [5/7], Batch [23900], Loss: 4.0406, Accuracy: 0.2333\n",
            "Epoch [5/7], Batch [24000], Loss: 4.0859, Accuracy: 0.2464\n",
            "Epoch [5/7], Batch [24100], Loss: 4.1904, Accuracy: 0.2012\n",
            "Epoch [5/7], Batch [24200], Loss: 3.9177, Accuracy: 0.2429\n",
            "Epoch [5/7], Batch [24300], Loss: 4.0464, Accuracy: 0.2429\n",
            "Epoch [5/7], Batch [24400], Loss: 4.1375, Accuracy: 0.2190\n",
            "Epoch [5/7], Batch [24500], Loss: 4.2219, Accuracy: 0.2274\n",
            "Epoch [5/7], Batch [24600], Loss: 4.1041, Accuracy: 0.2238\n",
            "Epoch [5/7], Batch [24700], Loss: 4.0802, Accuracy: 0.2000\n",
            "Epoch [5/7], Batch [24800], Loss: 4.0071, Accuracy: 0.2476\n",
            "Epoch [5/7], Batch [24900], Loss: 4.1512, Accuracy: 0.2393\n",
            "End of Epoch [5], Average Loss: 4.0538, Average Accuracy: 0.2294, Average Perplexity: nan\n",
            "====================================================================================================\n",
            "Epoch [6/7], Batch [0], Loss: 4.0200, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [100], Loss: 3.9507, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [200], Loss: 3.9430, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [300], Loss: 3.8658, Accuracy: 0.2429\n",
            "Epoch [6/7], Batch [400], Loss: 3.9341, Accuracy: 0.2571\n",
            "Epoch [6/7], Batch [500], Loss: 4.0798, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [600], Loss: 3.8150, Accuracy: 0.2405\n",
            "Epoch [6/7], Batch [700], Loss: 3.9216, Accuracy: 0.2631\n",
            "Epoch [6/7], Batch [800], Loss: 4.1452, Accuracy: 0.2167\n",
            "Epoch [6/7], Batch [900], Loss: 4.0432, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [1000], Loss: 3.7915, Accuracy: 0.2500\n",
            "Epoch [6/7], Batch [1100], Loss: 3.9426, Accuracy: 0.2464\n",
            "Epoch [6/7], Batch [1200], Loss: 4.0355, Accuracy: 0.2548\n",
            "Epoch [6/7], Batch [1300], Loss: 4.0732, Accuracy: 0.1976\n",
            "Epoch [6/7], Batch [1400], Loss: 3.9797, Accuracy: 0.2452\n",
            "Epoch [6/7], Batch [1500], Loss: 4.2100, Accuracy: 0.2310\n",
            "Epoch [6/7], Batch [1600], Loss: 3.9706, Accuracy: 0.2202\n",
            "Epoch [6/7], Batch [1700], Loss: 4.2420, Accuracy: 0.1952\n",
            "Epoch [6/7], Batch [1800], Loss: 4.0767, Accuracy: 0.2131\n",
            "Epoch [6/7], Batch [1900], Loss: 4.0475, Accuracy: 0.2369\n",
            "Epoch [6/7], Batch [2000], Loss: 4.1530, Accuracy: 0.2214\n",
            "Epoch [6/7], Batch [2100], Loss: 4.1370, Accuracy: 0.2131\n",
            "Epoch [6/7], Batch [2200], Loss: 4.1234, Accuracy: 0.2036\n",
            "Epoch [6/7], Batch [2300], Loss: 3.9405, Accuracy: 0.2179\n",
            "Epoch [6/7], Batch [2400], Loss: 4.0494, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [2500], Loss: 3.9331, Accuracy: 0.2429\n",
            "Epoch [6/7], Batch [2600], Loss: 3.7799, Accuracy: 0.2464\n",
            "Epoch [6/7], Batch [2700], Loss: 4.1443, Accuracy: 0.2012\n",
            "Epoch [6/7], Batch [2800], Loss: 4.1515, Accuracy: 0.2095\n",
            "Epoch [6/7], Batch [2900], Loss: 4.0739, Accuracy: 0.2107\n",
            "Epoch [6/7], Batch [3000], Loss: 4.0324, Accuracy: 0.2131\n",
            "Epoch [6/7], Batch [3100], Loss: 4.1554, Accuracy: 0.2179\n",
            "Epoch [6/7], Batch [3200], Loss: 4.0233, Accuracy: 0.2179\n",
            "Epoch [6/7], Batch [3300], Loss: 3.8216, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [3400], Loss: 3.9509, Accuracy: 0.2393\n",
            "Epoch [6/7], Batch [3500], Loss: 4.0188, Accuracy: 0.2310\n",
            "Epoch [6/7], Batch [3600], Loss: 3.9932, Accuracy: 0.2512\n",
            "Epoch [6/7], Batch [3700], Loss: 3.8513, Accuracy: 0.2333\n",
            "Epoch [6/7], Batch [3800], Loss: 4.0555, Accuracy: 0.2202\n",
            "Epoch [6/7], Batch [3900], Loss: 4.1258, Accuracy: 0.2083\n",
            "Epoch [6/7], Batch [4000], Loss: 4.0884, Accuracy: 0.2167\n",
            "Epoch [6/7], Batch [4100], Loss: 4.1083, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [4200], Loss: 4.0573, Accuracy: 0.2429\n",
            "Epoch [6/7], Batch [4300], Loss: 3.8881, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [4400], Loss: 4.0601, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [4500], Loss: 4.0313, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [4600], Loss: 4.0572, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [4700], Loss: 4.0222, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [4800], Loss: 3.8435, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [4900], Loss: 4.1088, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [5000], Loss: 3.8250, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [5100], Loss: 3.8447, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [5200], Loss: 4.0002, Accuracy: 0.2452\n",
            "Epoch [6/7], Batch [5300], Loss: 4.0051, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [5400], Loss: 4.0237, Accuracy: 0.2071\n",
            "Epoch [6/7], Batch [5500], Loss: 4.1710, Accuracy: 0.2095\n",
            "Epoch [6/7], Batch [5600], Loss: 4.2565, Accuracy: 0.2143\n",
            "Epoch [6/7], Batch [5700], Loss: 4.0457, Accuracy: 0.2429\n",
            "Epoch [6/7], Batch [5800], Loss: 3.9790, Accuracy: 0.2405\n",
            "Epoch [6/7], Batch [5900], Loss: 4.0913, Accuracy: 0.2298\n",
            "Epoch [6/7], Batch [6000], Loss: 4.0923, Accuracy: 0.2190\n",
            "Epoch [6/7], Batch [6100], Loss: 4.2028, Accuracy: 0.2238\n",
            "Epoch [6/7], Batch [6200], Loss: 4.0620, Accuracy: 0.2345\n",
            "Epoch [6/7], Batch [6300], Loss: 4.1203, Accuracy: 0.2012\n",
            "Epoch [6/7], Batch [6400], Loss: 4.0721, Accuracy: 0.2238\n",
            "Epoch [6/7], Batch [6500], Loss: 3.9714, Accuracy: 0.2452\n",
            "Epoch [6/7], Batch [6600], Loss: 3.9513, Accuracy: 0.2155\n",
            "Epoch [6/7], Batch [6700], Loss: 4.2144, Accuracy: 0.2048\n",
            "Epoch [6/7], Batch [6800], Loss: 4.1377, Accuracy: 0.2131\n",
            "Epoch [6/7], Batch [6900], Loss: 3.9837, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [7000], Loss: 4.0771, Accuracy: 0.2155\n",
            "Epoch [6/7], Batch [7100], Loss: 3.8076, Accuracy: 0.2762\n",
            "Epoch [6/7], Batch [7200], Loss: 3.9486, Accuracy: 0.2464\n",
            "Epoch [6/7], Batch [7300], Loss: 3.9782, Accuracy: 0.2143\n",
            "Epoch [6/7], Batch [7400], Loss: 3.9290, Accuracy: 0.2536\n",
            "Epoch [6/7], Batch [7500], Loss: 3.9662, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [7600], Loss: 3.8000, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [7700], Loss: 3.9150, Accuracy: 0.2524\n",
            "Epoch [6/7], Batch [7800], Loss: 3.9945, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [7900], Loss: 4.1415, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [8000], Loss: 4.0253, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [8100], Loss: 4.0189, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [8200], Loss: 3.9262, Accuracy: 0.2524\n",
            "Epoch [6/7], Batch [8300], Loss: 4.0196, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [8400], Loss: 4.0348, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [8500], Loss: 4.1617, Accuracy: 0.2083\n",
            "Epoch [6/7], Batch [8600], Loss: 4.0098, Accuracy: 0.2369\n",
            "Epoch [6/7], Batch [8700], Loss: 3.9742, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [8800], Loss: 4.0782, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [8900], Loss: 3.9058, Accuracy: 0.2417\n",
            "Epoch [6/7], Batch [9000], Loss: 4.1122, Accuracy: 0.2202\n",
            "Epoch [6/7], Batch [9100], Loss: 3.8907, Accuracy: 0.2536\n",
            "Epoch [6/7], Batch [9200], Loss: 3.9366, Accuracy: 0.2107\n",
            "Epoch [6/7], Batch [9300], Loss: 3.9461, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [9400], Loss: 4.0967, Accuracy: 0.1869\n",
            "Epoch [6/7], Batch [9500], Loss: 3.8722, Accuracy: 0.2750\n",
            "Epoch [6/7], Batch [9600], Loss: 4.2845, Accuracy: 0.2155\n",
            "Epoch [6/7], Batch [9700], Loss: 3.9909, Accuracy: 0.1964\n",
            "Epoch [6/7], Batch [9800], Loss: 4.0673, Accuracy: 0.2179\n",
            "Epoch [6/7], Batch [9900], Loss: 4.1721, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [10000], Loss: 3.8429, Accuracy: 0.2167\n",
            "Epoch [6/7], Batch [10100], Loss: 4.1134, Accuracy: 0.2381\n",
            "Epoch [6/7], Batch [10200], Loss: 4.1510, Accuracy: 0.2119\n",
            "Epoch [6/7], Batch [10300], Loss: 4.1676, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [10400], Loss: 3.9455, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [10500], Loss: 4.0658, Accuracy: 0.2143\n",
            "Epoch [6/7], Batch [10600], Loss: 3.9808, Accuracy: 0.2381\n",
            "Epoch [6/7], Batch [10700], Loss: 3.9760, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [10800], Loss: 4.0290, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [10900], Loss: 3.7956, Accuracy: 0.2643\n",
            "Epoch [6/7], Batch [11000], Loss: 4.0072, Accuracy: 0.2524\n",
            "Epoch [6/7], Batch [11100], Loss: 3.8730, Accuracy: 0.2238\n",
            "Epoch [6/7], Batch [11200], Loss: 3.8928, Accuracy: 0.2631\n",
            "Epoch [6/7], Batch [11300], Loss: 4.0296, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [11400], Loss: 3.9419, Accuracy: 0.2369\n",
            "Epoch [6/7], Batch [11500], Loss: 4.0351, Accuracy: 0.2429\n",
            "Epoch [6/7], Batch [11600], Loss: 4.0114, Accuracy: 0.2238\n",
            "Epoch [6/7], Batch [11700], Loss: 4.0668, Accuracy: 0.1964\n",
            "Epoch [6/7], Batch [11800], Loss: 3.9725, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [11900], Loss: 3.9008, Accuracy: 0.2298\n",
            "Epoch [6/7], Batch [12000], Loss: 3.9797, Accuracy: 0.2452\n",
            "Epoch [6/7], Batch [12100], Loss: 3.9113, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [12200], Loss: 4.2057, Accuracy: 0.2048\n",
            "Epoch [6/7], Batch [12300], Loss: 3.9575, Accuracy: 0.2143\n",
            "Epoch [6/7], Batch [12400], Loss: 3.8961, Accuracy: 0.2476\n",
            "Epoch [6/7], Batch [12500], Loss: 3.8120, Accuracy: 0.2512\n",
            "Epoch [6/7], Batch [12600], Loss: 4.0609, Accuracy: 0.2143\n",
            "Epoch [6/7], Batch [12700], Loss: 3.9950, Accuracy: 0.1988\n",
            "Epoch [6/7], Batch [12800], Loss: 4.1236, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [12900], Loss: 3.9848, Accuracy: 0.2536\n",
            "Epoch [6/7], Batch [13000], Loss: 4.1131, Accuracy: 0.2083\n",
            "Epoch [6/7], Batch [13100], Loss: 4.0888, Accuracy: 0.2214\n",
            "Epoch [6/7], Batch [13200], Loss: 4.0205, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [13300], Loss: 3.8021, Accuracy: 0.2536\n",
            "Epoch [6/7], Batch [13400], Loss: 3.7763, Accuracy: 0.2726\n",
            "Epoch [6/7], Batch [13500], Loss: 4.1072, Accuracy: 0.2333\n",
            "Epoch [6/7], Batch [13600], Loss: 3.8972, Accuracy: 0.2429\n",
            "Epoch [6/7], Batch [13700], Loss: 3.9757, Accuracy: 0.2107\n",
            "Epoch [6/7], Batch [13800], Loss: 4.0088, Accuracy: 0.2083\n",
            "Epoch [6/7], Batch [13900], Loss: 4.0114, Accuracy: 0.2333\n",
            "Epoch [6/7], Batch [14000], Loss: 3.9037, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [14100], Loss: 3.9179, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [14200], Loss: 3.9461, Accuracy: 0.2476\n",
            "Epoch [6/7], Batch [14300], Loss: 3.9648, Accuracy: 0.2464\n",
            "Epoch [6/7], Batch [14400], Loss: 4.1031, Accuracy: 0.2048\n",
            "Epoch [6/7], Batch [14500], Loss: 3.9913, Accuracy: 0.2405\n",
            "Epoch [6/7], Batch [14600], Loss: 4.0812, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [14700], Loss: 3.9078, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [14800], Loss: 3.8513, Accuracy: 0.2488\n",
            "Epoch [6/7], Batch [14900], Loss: 3.9727, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [15000], Loss: 3.9673, Accuracy: 0.2452\n",
            "Epoch [6/7], Batch [15100], Loss: 3.9853, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [15200], Loss: 3.8397, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [15300], Loss: 4.0339, Accuracy: 0.2298\n",
            "Epoch [6/7], Batch [15400], Loss: 3.9919, Accuracy: 0.2310\n",
            "Epoch [6/7], Batch [15500], Loss: 4.0265, Accuracy: 0.2179\n",
            "Epoch [6/7], Batch [15600], Loss: 4.0575, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [15700], Loss: 4.2329, Accuracy: 0.2119\n",
            "Epoch [6/7], Batch [15800], Loss: 4.1129, Accuracy: 0.2310\n",
            "Epoch [6/7], Batch [15900], Loss: 3.9617, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [16000], Loss: 4.0094, Accuracy: 0.2167\n",
            "Epoch [6/7], Batch [16100], Loss: 3.6579, Accuracy: 0.2643\n",
            "Epoch [6/7], Batch [16200], Loss: 3.9900, Accuracy: 0.2190\n",
            "Epoch [6/7], Batch [16300], Loss: 4.1751, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [16400], Loss: 4.1337, Accuracy: 0.1988\n",
            "Epoch [6/7], Batch [16500], Loss: 4.1685, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [16600], Loss: 3.9163, Accuracy: 0.2071\n",
            "Epoch [6/7], Batch [16700], Loss: 3.8544, Accuracy: 0.2560\n",
            "Epoch [6/7], Batch [16800], Loss: 4.0505, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [16900], Loss: 3.9904, Accuracy: 0.2250\n",
            "Epoch [6/7], Batch [17000], Loss: 3.8818, Accuracy: 0.2476\n",
            "Epoch [6/7], Batch [17100], Loss: 3.9625, Accuracy: 0.2345\n",
            "Epoch [6/7], Batch [17200], Loss: 4.1669, Accuracy: 0.2238\n",
            "Epoch [6/7], Batch [17300], Loss: 3.8640, Accuracy: 0.2381\n",
            "Epoch [6/7], Batch [17400], Loss: 3.9941, Accuracy: 0.2452\n",
            "Epoch [6/7], Batch [17500], Loss: 3.9973, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [17600], Loss: 3.9303, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [17700], Loss: 3.9918, Accuracy: 0.2405\n",
            "Epoch [6/7], Batch [17800], Loss: 3.9887, Accuracy: 0.2595\n",
            "Epoch [6/7], Batch [17900], Loss: 3.8603, Accuracy: 0.2583\n",
            "Epoch [6/7], Batch [18000], Loss: 3.9196, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [18100], Loss: 3.7579, Accuracy: 0.2679\n",
            "Epoch [6/7], Batch [18200], Loss: 3.8248, Accuracy: 0.2405\n",
            "Epoch [6/7], Batch [18300], Loss: 3.9970, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [18400], Loss: 3.8532, Accuracy: 0.2607\n",
            "Epoch [6/7], Batch [18500], Loss: 4.0302, Accuracy: 0.2310\n",
            "Epoch [6/7], Batch [18600], Loss: 3.8284, Accuracy: 0.2393\n",
            "Epoch [6/7], Batch [18700], Loss: 4.1105, Accuracy: 0.2214\n",
            "Epoch [6/7], Batch [18800], Loss: 3.8088, Accuracy: 0.2631\n",
            "Epoch [6/7], Batch [18900], Loss: 3.8950, Accuracy: 0.2464\n",
            "Epoch [6/7], Batch [19000], Loss: 3.7816, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [19100], Loss: 4.1065, Accuracy: 0.2036\n",
            "Epoch [6/7], Batch [19200], Loss: 4.1522, Accuracy: 0.2167\n",
            "Epoch [6/7], Batch [19300], Loss: 3.8621, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [19400], Loss: 4.0499, Accuracy: 0.2214\n",
            "Epoch [6/7], Batch [19500], Loss: 3.9257, Accuracy: 0.2512\n",
            "Epoch [6/7], Batch [19600], Loss: 3.9698, Accuracy: 0.2381\n",
            "Epoch [6/7], Batch [19700], Loss: 3.8726, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [19800], Loss: 4.0832, Accuracy: 0.2036\n",
            "Epoch [6/7], Batch [19900], Loss: 4.1005, Accuracy: 0.2381\n",
            "Epoch [6/7], Batch [20000], Loss: 3.9037, Accuracy: 0.2310\n",
            "Epoch [6/7], Batch [20100], Loss: 3.9143, Accuracy: 0.2595\n",
            "Epoch [6/7], Batch [20200], Loss: 4.0114, Accuracy: 0.2286\n",
            "Epoch [6/7], Batch [20300], Loss: 3.8801, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [20400], Loss: 3.8542, Accuracy: 0.2786\n",
            "Epoch [6/7], Batch [20500], Loss: 3.7875, Accuracy: 0.2393\n",
            "Epoch [6/7], Batch [20600], Loss: 4.0534, Accuracy: 0.2536\n",
            "Epoch [6/7], Batch [20700], Loss: 4.0143, Accuracy: 0.2036\n",
            "Epoch [6/7], Batch [20800], Loss: 3.9105, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [20900], Loss: 4.0972, Accuracy: 0.2381\n",
            "Epoch [6/7], Batch [21000], Loss: 4.2842, Accuracy: 0.2060\n",
            "Epoch [6/7], Batch [21100], Loss: 3.5493, Accuracy: 0.2917\n",
            "Epoch [6/7], Batch [21200], Loss: 3.8616, Accuracy: 0.2667\n",
            "Epoch [6/7], Batch [21300], Loss: 3.9858, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [21400], Loss: 3.9723, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [21500], Loss: 4.1429, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [21600], Loss: 3.9900, Accuracy: 0.2202\n",
            "Epoch [6/7], Batch [21700], Loss: 3.9440, Accuracy: 0.2524\n",
            "Epoch [6/7], Batch [21800], Loss: 4.0822, Accuracy: 0.2143\n",
            "Epoch [6/7], Batch [21900], Loss: 3.9895, Accuracy: 0.2345\n",
            "Epoch [6/7], Batch [22000], Loss: 3.9983, Accuracy: 0.2500\n",
            "Epoch [6/7], Batch [22100], Loss: 3.9112, Accuracy: 0.2524\n",
            "Epoch [6/7], Batch [22200], Loss: 4.1296, Accuracy: 0.2226\n",
            "Epoch [6/7], Batch [22300], Loss: 3.9884, Accuracy: 0.2393\n",
            "Epoch [6/7], Batch [22400], Loss: 4.1537, Accuracy: 0.2095\n",
            "Epoch [6/7], Batch [22500], Loss: 3.9038, Accuracy: 0.2417\n",
            "Epoch [6/7], Batch [22600], Loss: 3.9087, Accuracy: 0.2036\n",
            "Epoch [6/7], Batch [22700], Loss: 4.0632, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [22800], Loss: 4.1456, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [22900], Loss: 3.8890, Accuracy: 0.2500\n",
            "Epoch [6/7], Batch [23000], Loss: 4.0680, Accuracy: 0.2119\n",
            "Epoch [6/7], Batch [23100], Loss: 3.8014, Accuracy: 0.2417\n",
            "Epoch [6/7], Batch [23200], Loss: 4.0869, Accuracy: 0.2298\n",
            "Epoch [6/7], Batch [23300], Loss: 4.0400, Accuracy: 0.2012\n",
            "Epoch [6/7], Batch [23400], Loss: 3.8004, Accuracy: 0.2393\n",
            "Epoch [6/7], Batch [23500], Loss: 4.0317, Accuracy: 0.2179\n",
            "Epoch [6/7], Batch [23600], Loss: 3.8570, Accuracy: 0.2655\n",
            "Epoch [6/7], Batch [23700], Loss: 4.0052, Accuracy: 0.2321\n",
            "Epoch [6/7], Batch [23800], Loss: 4.0895, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [23900], Loss: 3.8334, Accuracy: 0.2440\n",
            "Epoch [6/7], Batch [24000], Loss: 4.0925, Accuracy: 0.2060\n",
            "Epoch [6/7], Batch [24100], Loss: 3.8883, Accuracy: 0.2631\n",
            "Epoch [6/7], Batch [24200], Loss: 3.8753, Accuracy: 0.2595\n",
            "Epoch [6/7], Batch [24300], Loss: 4.0063, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [24400], Loss: 3.8555, Accuracy: 0.2488\n",
            "Epoch [6/7], Batch [24500], Loss: 4.1179, Accuracy: 0.2357\n",
            "Epoch [6/7], Batch [24600], Loss: 4.0439, Accuracy: 0.2274\n",
            "Epoch [6/7], Batch [24700], Loss: 3.9166, Accuracy: 0.2262\n",
            "Epoch [6/7], Batch [24800], Loss: 4.1327, Accuracy: 0.2060\n",
            "Epoch [6/7], Batch [24900], Loss: 4.2270, Accuracy: 0.2119\n",
            "End of Epoch [6], Average Loss: 4.0027, Average Accuracy: 0.2324, Average Perplexity: nan\n",
            "====================================================================================================\n",
            "Epoch [7/7], Batch [0], Loss: 3.9648, Accuracy: 0.2095\n",
            "Epoch [7/7], Batch [100], Loss: 3.9634, Accuracy: 0.2524\n",
            "Epoch [7/7], Batch [200], Loss: 3.9678, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [300], Loss: 3.9793, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [400], Loss: 3.9525, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [500], Loss: 3.8716, Accuracy: 0.2560\n",
            "Epoch [7/7], Batch [600], Loss: 4.0142, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [700], Loss: 3.9749, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [800], Loss: 4.1783, Accuracy: 0.2071\n",
            "Epoch [7/7], Batch [900], Loss: 4.1382, Accuracy: 0.2119\n",
            "Epoch [7/7], Batch [1000], Loss: 3.9993, Accuracy: 0.2190\n",
            "Epoch [7/7], Batch [1100], Loss: 4.0770, Accuracy: 0.2512\n",
            "Epoch [7/7], Batch [1200], Loss: 3.8209, Accuracy: 0.2560\n",
            "Epoch [7/7], Batch [1300], Loss: 4.0964, Accuracy: 0.2298\n",
            "Epoch [7/7], Batch [1400], Loss: 4.0054, Accuracy: 0.2369\n",
            "Epoch [7/7], Batch [1500], Loss: 4.1819, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [1600], Loss: 4.0184, Accuracy: 0.2321\n",
            "Epoch [7/7], Batch [1700], Loss: 4.0074, Accuracy: 0.2190\n",
            "Epoch [7/7], Batch [1800], Loss: 3.9694, Accuracy: 0.2190\n",
            "Epoch [7/7], Batch [1900], Loss: 3.8221, Accuracy: 0.2583\n",
            "Epoch [7/7], Batch [2000], Loss: 4.1524, Accuracy: 0.2298\n",
            "Epoch [7/7], Batch [2100], Loss: 4.1697, Accuracy: 0.2012\n",
            "Epoch [7/7], Batch [2200], Loss: 3.9787, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [2300], Loss: 3.9662, Accuracy: 0.2440\n",
            "Epoch [7/7], Batch [2400], Loss: 4.0712, Accuracy: 0.2452\n",
            "Epoch [7/7], Batch [2500], Loss: 3.9627, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [2600], Loss: 4.0971, Accuracy: 0.2369\n",
            "Epoch [7/7], Batch [2700], Loss: 3.9862, Accuracy: 0.2560\n",
            "Epoch [7/7], Batch [2800], Loss: 3.9719, Accuracy: 0.2429\n",
            "Epoch [7/7], Batch [2900], Loss: 3.8619, Accuracy: 0.2190\n",
            "Epoch [7/7], Batch [3000], Loss: 3.9713, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [3100], Loss: 3.8796, Accuracy: 0.2333\n",
            "Epoch [7/7], Batch [3200], Loss: 3.7761, Accuracy: 0.2881\n",
            "Epoch [7/7], Batch [3300], Loss: 3.8103, Accuracy: 0.2702\n",
            "Epoch [7/7], Batch [3400], Loss: 3.9271, Accuracy: 0.2381\n",
            "Epoch [7/7], Batch [3500], Loss: 4.0871, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [3600], Loss: 4.1563, Accuracy: 0.2155\n",
            "Epoch [7/7], Batch [3700], Loss: 3.9334, Accuracy: 0.2262\n",
            "Epoch [7/7], Batch [3800], Loss: 4.1336, Accuracy: 0.2036\n",
            "Epoch [7/7], Batch [3900], Loss: 3.9655, Accuracy: 0.2595\n",
            "Epoch [7/7], Batch [4000], Loss: 4.1618, Accuracy: 0.2071\n",
            "Epoch [7/7], Batch [4100], Loss: 3.9020, Accuracy: 0.2643\n",
            "Epoch [7/7], Batch [4200], Loss: 3.9420, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [4300], Loss: 3.9313, Accuracy: 0.2214\n",
            "Epoch [7/7], Batch [4400], Loss: 4.0622, Accuracy: 0.2202\n",
            "Epoch [7/7], Batch [4500], Loss: 4.1705, Accuracy: 0.2095\n",
            "Epoch [7/7], Batch [4600], Loss: 4.1590, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [4700], Loss: 4.0223, Accuracy: 0.2500\n",
            "Epoch [7/7], Batch [4800], Loss: 4.2352, Accuracy: 0.1964\n",
            "Epoch [7/7], Batch [4900], Loss: 3.8940, Accuracy: 0.2524\n",
            "Epoch [7/7], Batch [5000], Loss: 3.9429, Accuracy: 0.2440\n",
            "Epoch [7/7], Batch [5100], Loss: 4.1948, Accuracy: 0.2048\n",
            "Epoch [7/7], Batch [5200], Loss: 4.0101, Accuracy: 0.2333\n",
            "Epoch [7/7], Batch [5300], Loss: 4.1009, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [5400], Loss: 4.0162, Accuracy: 0.2095\n",
            "Epoch [7/7], Batch [5500], Loss: 3.8930, Accuracy: 0.2500\n",
            "Epoch [7/7], Batch [5600], Loss: 4.0126, Accuracy: 0.2321\n",
            "Epoch [7/7], Batch [5700], Loss: 4.0806, Accuracy: 0.2119\n",
            "Epoch [7/7], Batch [5800], Loss: 4.1650, Accuracy: 0.2226\n",
            "Epoch [7/7], Batch [5900], Loss: 4.0062, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [6000], Loss: 4.0071, Accuracy: 0.2345\n",
            "Epoch [7/7], Batch [6100], Loss: 3.9360, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [6200], Loss: 4.0076, Accuracy: 0.2107\n",
            "Epoch [7/7], Batch [6300], Loss: 3.9084, Accuracy: 0.2440\n",
            "Epoch [7/7], Batch [6400], Loss: 3.9776, Accuracy: 0.2345\n",
            "Epoch [7/7], Batch [6500], Loss: 3.8812, Accuracy: 0.2512\n",
            "Epoch [7/7], Batch [6600], Loss: 4.1188, Accuracy: 0.2095\n",
            "Epoch [7/7], Batch [6700], Loss: 3.7117, Accuracy: 0.2917\n",
            "Epoch [7/7], Batch [6800], Loss: 3.9658, Accuracy: 0.2524\n",
            "Epoch [7/7], Batch [6900], Loss: 4.1204, Accuracy: 0.2083\n",
            "Epoch [7/7], Batch [7000], Loss: 3.8765, Accuracy: 0.2155\n",
            "Epoch [7/7], Batch [7100], Loss: 3.9393, Accuracy: 0.2202\n",
            "Epoch [7/7], Batch [7200], Loss: 4.0256, Accuracy: 0.2202\n",
            "Epoch [7/7], Batch [7300], Loss: 3.8975, Accuracy: 0.2488\n",
            "Epoch [7/7], Batch [7400], Loss: 4.1194, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [7500], Loss: 3.9084, Accuracy: 0.2417\n",
            "Epoch [7/7], Batch [7600], Loss: 3.9730, Accuracy: 0.2500\n",
            "Epoch [7/7], Batch [7700], Loss: 3.9945, Accuracy: 0.2321\n",
            "Epoch [7/7], Batch [7800], Loss: 3.9275, Accuracy: 0.2512\n",
            "Epoch [7/7], Batch [7900], Loss: 4.0366, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [8000], Loss: 4.0482, Accuracy: 0.2381\n",
            "Epoch [7/7], Batch [8100], Loss: 3.7953, Accuracy: 0.2583\n",
            "Epoch [7/7], Batch [8200], Loss: 3.9317, Accuracy: 0.2345\n",
            "Epoch [7/7], Batch [8300], Loss: 4.0433, Accuracy: 0.2298\n",
            "Epoch [7/7], Batch [8400], Loss: 4.0806, Accuracy: 0.2202\n",
            "Epoch [7/7], Batch [8500], Loss: 3.9273, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [8600], Loss: 3.9296, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [8700], Loss: 4.0495, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [8800], Loss: 3.9306, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [8900], Loss: 3.9226, Accuracy: 0.2488\n",
            "Epoch [7/7], Batch [9000], Loss: 3.9928, Accuracy: 0.2036\n",
            "Epoch [7/7], Batch [9100], Loss: 3.9386, Accuracy: 0.2440\n",
            "Epoch [7/7], Batch [9200], Loss: 3.9256, Accuracy: 0.2452\n",
            "Epoch [7/7], Batch [9300], Loss: 3.9201, Accuracy: 0.2512\n",
            "Epoch [7/7], Batch [9400], Loss: 4.0618, Accuracy: 0.2238\n",
            "Epoch [7/7], Batch [9500], Loss: 4.1583, Accuracy: 0.1714\n",
            "Epoch [7/7], Batch [9600], Loss: 3.7835, Accuracy: 0.2571\n",
            "Epoch [7/7], Batch [9700], Loss: 3.9629, Accuracy: 0.2381\n",
            "Epoch [7/7], Batch [9800], Loss: 3.8166, Accuracy: 0.2464\n",
            "Epoch [7/7], Batch [9900], Loss: 3.9445, Accuracy: 0.2274\n",
            "Epoch [7/7], Batch [10000], Loss: 3.9062, Accuracy: 0.2452\n",
            "Epoch [7/7], Batch [10100], Loss: 3.9577, Accuracy: 0.2167\n",
            "Epoch [7/7], Batch [10200], Loss: 3.9757, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [10300], Loss: 3.9231, Accuracy: 0.2155\n",
            "Epoch [7/7], Batch [10400], Loss: 4.0034, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [10500], Loss: 3.9476, Accuracy: 0.2321\n",
            "Epoch [7/7], Batch [10600], Loss: 4.2012, Accuracy: 0.2024\n",
            "Epoch [7/7], Batch [10700], Loss: 4.0850, Accuracy: 0.2095\n",
            "Epoch [7/7], Batch [10800], Loss: 4.0043, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [10900], Loss: 4.0001, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [11000], Loss: 4.0533, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [11100], Loss: 3.9320, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [11200], Loss: 3.7987, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [11300], Loss: 4.1257, Accuracy: 0.2083\n",
            "Epoch [7/7], Batch [11400], Loss: 4.0128, Accuracy: 0.1881\n",
            "Epoch [7/7], Batch [11500], Loss: 4.1022, Accuracy: 0.2202\n",
            "Epoch [7/7], Batch [11600], Loss: 3.8807, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [11700], Loss: 4.1821, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [11800], Loss: 3.9013, Accuracy: 0.2369\n",
            "Epoch [7/7], Batch [11900], Loss: 3.8939, Accuracy: 0.2262\n",
            "Epoch [7/7], Batch [12000], Loss: 4.0849, Accuracy: 0.2167\n",
            "Epoch [7/7], Batch [12100], Loss: 3.7440, Accuracy: 0.2488\n",
            "Epoch [7/7], Batch [12200], Loss: 3.6998, Accuracy: 0.2833\n",
            "Epoch [7/7], Batch [12300], Loss: 4.1686, Accuracy: 0.2107\n",
            "Epoch [7/7], Batch [12400], Loss: 3.9099, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [12500], Loss: 4.2672, Accuracy: 0.1905\n",
            "Epoch [7/7], Batch [12600], Loss: 4.0911, Accuracy: 0.2131\n",
            "Epoch [7/7], Batch [12700], Loss: 4.2346, Accuracy: 0.1988\n",
            "Epoch [7/7], Batch [12800], Loss: 4.0792, Accuracy: 0.2512\n",
            "Epoch [7/7], Batch [12900], Loss: 3.9269, Accuracy: 0.2607\n",
            "Epoch [7/7], Batch [13000], Loss: 3.8838, Accuracy: 0.2464\n",
            "Epoch [7/7], Batch [13100], Loss: 3.8667, Accuracy: 0.2690\n",
            "Epoch [7/7], Batch [13200], Loss: 3.9377, Accuracy: 0.2488\n",
            "Epoch [7/7], Batch [13300], Loss: 4.0485, Accuracy: 0.2310\n",
            "Epoch [7/7], Batch [13400], Loss: 4.0427, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [13500], Loss: 3.7909, Accuracy: 0.2607\n",
            "Epoch [7/7], Batch [13600], Loss: 4.0062, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [13700], Loss: 3.9503, Accuracy: 0.2488\n",
            "Epoch [7/7], Batch [13800], Loss: 4.0240, Accuracy: 0.2179\n",
            "Epoch [7/7], Batch [13900], Loss: 4.1315, Accuracy: 0.2071\n",
            "Epoch [7/7], Batch [14000], Loss: 3.9061, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [14100], Loss: 4.1092, Accuracy: 0.2238\n",
            "Epoch [7/7], Batch [14200], Loss: 3.9153, Accuracy: 0.2345\n",
            "Epoch [7/7], Batch [14300], Loss: 4.0794, Accuracy: 0.2440\n",
            "Epoch [7/7], Batch [14400], Loss: 4.0651, Accuracy: 0.2298\n",
            "Epoch [7/7], Batch [14500], Loss: 4.1139, Accuracy: 0.2298\n",
            "Epoch [7/7], Batch [14600], Loss: 4.0046, Accuracy: 0.2560\n",
            "Epoch [7/7], Batch [14700], Loss: 3.7751, Accuracy: 0.2583\n",
            "Epoch [7/7], Batch [14800], Loss: 4.0914, Accuracy: 0.2310\n",
            "Epoch [7/7], Batch [14900], Loss: 3.9411, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [15000], Loss: 4.0299, Accuracy: 0.2429\n",
            "Epoch [7/7], Batch [15100], Loss: 3.9399, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [15200], Loss: 3.9441, Accuracy: 0.2226\n",
            "Epoch [7/7], Batch [15300], Loss: 4.0070, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [15400], Loss: 3.9657, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [15500], Loss: 4.1855, Accuracy: 0.2048\n",
            "Epoch [7/7], Batch [15600], Loss: 4.1939, Accuracy: 0.1905\n",
            "Epoch [7/7], Batch [15700], Loss: 4.0800, Accuracy: 0.2262\n",
            "Epoch [7/7], Batch [15800], Loss: 4.0739, Accuracy: 0.2429\n",
            "Epoch [7/7], Batch [15900], Loss: 3.8231, Accuracy: 0.2536\n",
            "Epoch [7/7], Batch [16000], Loss: 4.0519, Accuracy: 0.2214\n",
            "Epoch [7/7], Batch [16100], Loss: 3.8446, Accuracy: 0.2571\n",
            "Epoch [7/7], Batch [16200], Loss: 3.9620, Accuracy: 0.2226\n",
            "Epoch [7/7], Batch [16300], Loss: 4.0795, Accuracy: 0.2310\n",
            "Epoch [7/7], Batch [16400], Loss: 4.0849, Accuracy: 0.2369\n",
            "Epoch [7/7], Batch [16500], Loss: 3.9385, Accuracy: 0.2452\n",
            "Epoch [7/7], Batch [16600], Loss: 4.1973, Accuracy: 0.2167\n",
            "Epoch [7/7], Batch [16700], Loss: 3.7748, Accuracy: 0.2464\n",
            "Epoch [7/7], Batch [16800], Loss: 3.9544, Accuracy: 0.2333\n",
            "Epoch [7/7], Batch [16900], Loss: 3.8576, Accuracy: 0.2429\n",
            "Epoch [7/7], Batch [17000], Loss: 3.9348, Accuracy: 0.2548\n",
            "Epoch [7/7], Batch [17100], Loss: 3.8981, Accuracy: 0.2655\n",
            "Epoch [7/7], Batch [17200], Loss: 4.0438, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [17300], Loss: 3.9225, Accuracy: 0.2036\n",
            "Epoch [7/7], Batch [17400], Loss: 3.7279, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [17500], Loss: 3.8605, Accuracy: 0.2595\n",
            "Epoch [7/7], Batch [17600], Loss: 4.1534, Accuracy: 0.1952\n",
            "Epoch [7/7], Batch [17700], Loss: 3.9109, Accuracy: 0.2131\n",
            "Epoch [7/7], Batch [17800], Loss: 4.0355, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [17900], Loss: 4.1888, Accuracy: 0.2179\n",
            "Epoch [7/7], Batch [18000], Loss: 4.0474, Accuracy: 0.2202\n",
            "Epoch [7/7], Batch [18100], Loss: 4.1407, Accuracy: 0.2012\n",
            "Epoch [7/7], Batch [18200], Loss: 4.1730, Accuracy: 0.2190\n",
            "Epoch [7/7], Batch [18300], Loss: 4.0117, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [18400], Loss: 3.9857, Accuracy: 0.2274\n",
            "Epoch [7/7], Batch [18500], Loss: 4.0581, Accuracy: 0.2262\n",
            "Epoch [7/7], Batch [18600], Loss: 3.8093, Accuracy: 0.2750\n",
            "Epoch [7/7], Batch [18700], Loss: 3.9405, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [18800], Loss: 3.8172, Accuracy: 0.2321\n",
            "Epoch [7/7], Batch [18900], Loss: 3.9887, Accuracy: 0.2345\n",
            "Epoch [7/7], Batch [19000], Loss: 3.7438, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [19100], Loss: 3.7911, Accuracy: 0.2560\n",
            "Epoch [7/7], Batch [19200], Loss: 4.0463, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [19300], Loss: 4.0260, Accuracy: 0.2452\n",
            "Epoch [7/7], Batch [19400], Loss: 4.0705, Accuracy: 0.2405\n",
            "Epoch [7/7], Batch [19500], Loss: 3.8527, Accuracy: 0.2655\n",
            "Epoch [7/7], Batch [19600], Loss: 3.8371, Accuracy: 0.2548\n",
            "Epoch [7/7], Batch [19700], Loss: 3.8451, Accuracy: 0.2655\n",
            "Epoch [7/7], Batch [19800], Loss: 3.9008, Accuracy: 0.2655\n",
            "Epoch [7/7], Batch [19900], Loss: 3.9347, Accuracy: 0.2286\n",
            "Epoch [7/7], Batch [20000], Loss: 4.0308, Accuracy: 0.2179\n",
            "Epoch [7/7], Batch [20100], Loss: 3.9860, Accuracy: 0.2310\n",
            "Epoch [7/7], Batch [20200], Loss: 4.0797, Accuracy: 0.2179\n",
            "Epoch [7/7], Batch [20300], Loss: 3.9313, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [20400], Loss: 3.9173, Accuracy: 0.2333\n",
            "Epoch [7/7], Batch [20500], Loss: 3.8419, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [20600], Loss: 3.8110, Accuracy: 0.2571\n",
            "Epoch [7/7], Batch [20700], Loss: 4.0912, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [20800], Loss: 4.0013, Accuracy: 0.2310\n",
            "Epoch [7/7], Batch [20900], Loss: 4.2021, Accuracy: 0.1929\n",
            "Epoch [7/7], Batch [21000], Loss: 3.8781, Accuracy: 0.2488\n",
            "Epoch [7/7], Batch [21100], Loss: 3.9536, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [21200], Loss: 3.8841, Accuracy: 0.2226\n",
            "Epoch [7/7], Batch [21300], Loss: 4.0935, Accuracy: 0.2119\n",
            "Epoch [7/7], Batch [21400], Loss: 3.7768, Accuracy: 0.2524\n",
            "Epoch [7/7], Batch [21500], Loss: 3.8454, Accuracy: 0.2369\n",
            "Epoch [7/7], Batch [21600], Loss: 3.8857, Accuracy: 0.2381\n",
            "Epoch [7/7], Batch [21700], Loss: 3.9342, Accuracy: 0.2083\n",
            "Epoch [7/7], Batch [21800], Loss: 3.8626, Accuracy: 0.2845\n",
            "Epoch [7/7], Batch [21900], Loss: 3.9936, Accuracy: 0.2214\n",
            "Epoch [7/7], Batch [22000], Loss: 4.1060, Accuracy: 0.2000\n",
            "Epoch [7/7], Batch [22100], Loss: 4.0346, Accuracy: 0.2393\n",
            "Epoch [7/7], Batch [22200], Loss: 4.1975, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [22300], Loss: 3.6442, Accuracy: 0.2786\n",
            "Epoch [7/7], Batch [22400], Loss: 3.9837, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [22500], Loss: 4.0475, Accuracy: 0.2369\n",
            "Epoch [7/7], Batch [22600], Loss: 4.0760, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [22700], Loss: 4.0436, Accuracy: 0.2345\n",
            "Epoch [7/7], Batch [22800], Loss: 3.9917, Accuracy: 0.2262\n",
            "Epoch [7/7], Batch [22900], Loss: 4.0496, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [23000], Loss: 4.0148, Accuracy: 0.2321\n",
            "Epoch [7/7], Batch [23100], Loss: 4.0603, Accuracy: 0.2333\n",
            "Epoch [7/7], Batch [23200], Loss: 4.0937, Accuracy: 0.2238\n",
            "Epoch [7/7], Batch [23300], Loss: 3.8852, Accuracy: 0.2310\n",
            "Epoch [7/7], Batch [23400], Loss: 3.8729, Accuracy: 0.2357\n",
            "Epoch [7/7], Batch [23500], Loss: 4.1742, Accuracy: 0.1845\n",
            "Epoch [7/7], Batch [23600], Loss: 4.0895, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [23700], Loss: 3.8372, Accuracy: 0.2571\n",
            "Epoch [7/7], Batch [23800], Loss: 3.9865, Accuracy: 0.2131\n",
            "Epoch [7/7], Batch [23900], Loss: 3.9584, Accuracy: 0.2464\n",
            "Epoch [7/7], Batch [24000], Loss: 3.8204, Accuracy: 0.2667\n",
            "Epoch [7/7], Batch [24100], Loss: 3.9389, Accuracy: 0.2429\n",
            "Epoch [7/7], Batch [24200], Loss: 4.0546, Accuracy: 0.2143\n",
            "Epoch [7/7], Batch [24300], Loss: 4.1135, Accuracy: 0.2155\n",
            "Epoch [7/7], Batch [24400], Loss: 3.8108, Accuracy: 0.2512\n",
            "Epoch [7/7], Batch [24500], Loss: 3.8889, Accuracy: 0.2476\n",
            "Epoch [7/7], Batch [24600], Loss: 3.9862, Accuracy: 0.2464\n",
            "Epoch [7/7], Batch [24700], Loss: 3.8864, Accuracy: 0.2250\n",
            "Epoch [7/7], Batch [24800], Loss: 3.9555, Accuracy: 0.2381\n",
            "Epoch [7/7], Batch [24900], Loss: 3.8688, Accuracy: 0.2583\n",
            "End of Epoch [7], Average Loss: 3.9876, Average Accuracy: 0.2332, Average Perplexity: nan\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#song_structure_loss = FastSongStructureLoss(token_to_idx, structure_penalty=1.0, penalty_multiplier=1.2)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Transfer the model to the configured device\n",
        "model.to(device)\n",
        "\n",
        "# Number of training epochs\n",
        "num_of_epochs = 7\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Lists to store per-epoch metrics\n",
        "epoch_losses = []\n",
        "epoch_accuracies = []\n",
        "epoch_perplexities = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_of_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    batch_losses,batch_perplexities, batch_accuracies = [], [],[]\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        outputs_flattened = outputs.view(-1, outputs.size(-1))\n",
        "        targets_flattened = targets.view(-1)\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = criterion(outputs_flattened, targets_flattened)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted_indices = torch.max(outputs_flattened, 1)\n",
        "        correct_predictions = (predicted_indices == targets_flattened).sum().item()\n",
        "        accuracy = correct_predictions / targets_flattened.size(0)\n",
        "        perplexity = torch.exp(loss).item()\n",
        "        batch_losses.append(loss.item())\n",
        "        batch_accuracies.append(accuracy)\n",
        "        batch_perplexities.append(perplexity)\n",
        "        if i % 100 == 0:  # Adjust the logging frequency according to your preference\n",
        "            print(f\"Epoch [{epoch+1}/{num_of_epochs}], Batch [{i}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # End-of-epoch calculations\n",
        "    epoch_loss = np.mean(batch_losses)\n",
        "    epoch_accuracy = np.mean(batch_accuracies)\n",
        "    epoch_perplexity = np.mean(batch_perplexities)\n",
        "    epoch_losses.append(epoch_loss)\n",
        "    epoch_accuracies.append(epoch_accuracy)\n",
        "    epoch_perplexities.append(epoch_perplexity)\n",
        "\n",
        "    # Epoch summary\n",
        "    print(f\"End of Epoch [{epoch+1}], Average Loss: {epoch_loss:.4f}, \"\n",
        "          f\"Average Accuracy: {epoch_accuracy:.4f}, Average Perplexity: {epoch_perplexity:.4f}\")\n",
        "    print('=' * 100)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pj8_NszIzXo",
        "outputId": "97120ce6-4f7b-47de-92da-c4ba76e05cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: none\n",
            "<startsong> <startintro> yeah , i am a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it\n",
            "--------------------------------------------------\n",
            "Method: temperature\n",
            "<startsong> <startintro> i am <endchorus> <startverse> my mouth is like , honey <endchorus> <startverse> i am just got you are the back of em and it ai nt gon na get outta compton monsters under my face stretches they love him that i got ta go , it was probly sick to be all evil seed and i do not , i hope i c k fed up , i am slidin on your mother smile again oh , that he is it , and i am not want me find dave , i am so you are not even tryin to leave me to diss me your anxietys throwin gang but i am not call rage tear sing for the assault a father raped the fuck it feels so i mean you got it is about it is like i am jacking off , it is , ran up and heavy duty until we did squat , i was smokin dope addict trait blood so hellacopped a second thought i wrote down worse than the car crash course , it is not give a stepchild with tabasco shady , i am just the street motherfuckers m16s woo , whoa whoa these clowns can not show ready for this , they say , what s in the way to you ll be in the stewardess said it feels good golfers i can not have to say it go ahead , this shit , i am a renegade never seen her e cause i am in the fuck your livers , i guess who else to the first place where am gettin tired of the fuck it i am in the new husband why i am crazy drivin on your esophagus stop beefin frequent you say that s really killed somber words\n",
            "--------------------------------------------------\n",
            "Method: top-k\n",
            "<startsong> <startintro> i am in this shit , you , but i got it is not get out to see , i am i am back , you got your superman , i ai nt gon na be a bitch , you just like a couple gifts but i have a man i am a man , but it is all the two of the two of us , so i can not even though , you do what the way you , it and the fuck with me , it up the two trailer park girls asleep my place i am not get on the shit is not be the fuck it is , i was a girl , i am not give up and the fuck you know it is a man , the same shit , you can flow is just a fuck you know that s no time i just the fuck with a bitch you are gon na slow fire marshals know i got you are you know why you got a fuck i am the real intense , i am so i am the only live topless skys fallin victims to the fuck with this is not give a bitch , i just like i am the world without someone standin by the same 808 clap at all the two copies if you , you can not know who got the two of the whole world without firin my ass whoops clothes wow wow wow he is , yeah , i have been lately i am i do i m a girl , you , and i am not even the real shady please tell em the fuck passed jay z z z z z wants me , so will not you , i know\n",
            "--------------------------------------------------\n",
            "Method: top-p\n",
            "<startsong> <startverse> i am ill fly away from my dick without pumpin our cards but there s why i will not know what if i have offended , it is not afraid to be good packing up to say but i had , i said i do not a tube shove me , man , but the way in the bullies become heterosexual nothing means yes i am just go up with that i do not be the fuck your kids from principal walked in the dutch got your parents bedrooms sometimes you wanna grow up in the whole dang skulls fire spitter okay pistol packing up but a break em eminem show you like covid bars are moody but i just in me , so stop til they see you just the best shot kim lady gaga mess of like a choice but his blond dreads fuck that you are all got off the door neighbors hedges like me this is okay pistol sippin bacardi concealed enough sleep it , do not escape media immediately points bulletin board sits dre , can not need to scream and keep droppin each other side to the pill dust it a night , what you , think it to do not be the hood haha , and i am a doberman pinscher and grab you got ta beat , take it was a tampax at me i am slim shady i am at the fact , and fall apart we are clever and smokin dope and your head honcho , there and if i just like this motherfuckin west ha ha ha ha ha ha ha ha ha ha , i am whatever weather , if i did , fuck droppin knowledge , yo , now i ai nt gon na be alone\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def deterministic_sampling(logits):\n",
        "    \"\"\"\n",
        "    Selects the token with the highest logit value.\n",
        "\n",
        "    Parameters:\n",
        "        logits (Tensor): The logits output from the model.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the token with the highest logit.\n",
        "    \"\"\"\n",
        "    return logits.argmax().item()\n",
        "\n",
        "def temperature_sampling(logits, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Samples a token index based on adjusted logits using a temperature parameter.\n",
        "\n",
        "    Parameters:\n",
        "        logits (Tensor): The logits output from the model.\n",
        "        temperature (float): The temperature to adjust the logits.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the sampled token.\n",
        "    \"\"\"\n",
        "    adjusted_logits = logits / temperature\n",
        "    probabilities = torch.softmax(adjusted_logits, dim=0).cpu().numpy()\n",
        "    return np.random.choice(len(logits), p=probabilities)\n",
        "\n",
        "def top_k_sampling(logits, top_k=0):\n",
        "    \"\"\"\n",
        "    Samples a token from the top K logits.\n",
        "\n",
        "    Parameters:\n",
        "        logits (Tensor): The logits output from the model.\n",
        "        top_k (int): The number of top logits to consider for sampling.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the sampled token.\n",
        "    \"\"\"\n",
        "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "    logits[indices_to_remove] = -float('Inf')\n",
        "    probabilities = torch.softmax(logits, dim=0).cpu().numpy()\n",
        "    return np.random.choice(len(logits), p=probabilities)\n",
        "\n",
        "def top_p_sampling(logits, top_p=0.0):\n",
        "    \"\"\"\n",
        "    Samples a token from the logits, considering only the top P cumulative probabilities.\n",
        "\n",
        "    Parameters:\n",
        "        logits (Tensor): The logits output from the model.\n",
        "        top_p (float): The cumulative probability threshold.\n",
        "\n",
        "    Returns:\n",
        "        int: The index of the sampled token.\n",
        "    \"\"\"\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=0), dim=0)\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "    logits[indices_to_remove] = -float('Inf')\n",
        "    probabilities = torch.softmax(logits, dim=0).cpu().numpy()\n",
        "    return np.random.choice(len(logits), p=probabilities)\n",
        "\n",
        "def generate_song(model, start_sequence, token_to_idx, idx_to_token, seq_length, device, max_length=100, method='none', temperature=1.0, top_k=0, top_p=0.0):\n",
        "    \"\"\"\n",
        "    Generates a song sequence using a specified sampling method.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The trained model for generating sequences.\n",
        "        start_sequence (list of str): The initial sequence of tokens to start generating from.\n",
        "        token_to_idx (dict): Mapping from tokens to their indices.\n",
        "        idx_to_token (dict): Mapping from indices back to tokens.\n",
        "        seq_length (int): The sequence length that the model expects.\n",
        "        device (torch.device): The device to perform the computation on.\n",
        "        max_length (int, optional): The maximum length of the generated sequence. Defaults to 100.\n",
        "        method (str, optional): The sampling method to use ('none', 'temperature', 'top-k', 'top-p'). Defaults to 'none'.\n",
        "        temperature (float, optional): Temperature parameter for temperature sampling. Defaults to 1.0.\n",
        "        top_k (int, optional): Top K parameter for top-k sampling. Defaults to 0.\n",
        "        top_p (float, optional): Top P parameter for top-p sampling. Defaults to 0.0.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The generated sequence of tokens.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = [token_to_idx.get(token, token_to_idx['<PAD>']) for token in start_sequence]\n",
        "    sampling_methods = {\n",
        "        'none': deterministic_sampling,\n",
        "        'temperature': lambda logits: temperature_sampling(logits, temperature),\n",
        "        'top-k': lambda logits: top_k_sampling(logits, top_k),\n",
        "        'top-p': lambda logits: top_p_sampling(logits, top_p),\n",
        "    }\n",
        "    sampling_function = sampling_methods[method]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            sequence = torch.tensor(tokens[-seq_length:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "            output = model(sequence)\n",
        "            last_output = output[0, -1, :]\n",
        "\n",
        "            predicted_token_idx = sampling_function(last_output)\n",
        "            tokens.append(predicted_token_idx)\n",
        "\n",
        "            if predicted_token_idx == token_to_idx.get('<ENDSONG>', token_to_idx['<PAD>']):\n",
        "                break\n",
        "\n",
        "    return [idx_to_token.get(idx, '<UNK>') for idx in tokens]\n",
        "\n",
        "# Example usage\n",
        "outputs = []\n",
        "methods = ['none', 'temperature', 'top-k', 'top-p']\n",
        "start_sequence = ['<startsong>']\n",
        "for method in methods:\n",
        "    print(f\"Method: {method}\")\n",
        "    generated_song = generate_song(model, start_sequence, token_to_idx, idx_to_token, seq_length, device, max_length=300, method=method, temperature=0.8, top_k=10, top_p=0.8)\n",
        "    generated_song_text = ' '.join(generated_song)\n",
        "    outputs.append(generated_song_text)\n",
        "    print(generated_song_text)\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "### we can try to mess more with the outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB8iIXGjIzXo",
        "outputId": "b4073731-bb7d-4c3f-8736-7390905cb250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: none\n",
            "yeah, i am a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it is a little bit of the fuck it\n",
            "--------------------------------------------------\n",
            "Method: temperature\n",
            "i am   my mouth is like, honey   i am just got you are the back of em and it ai nt gon na get outta compton monsters under my face stretches they love him that i got ta go, it was probly sick to be all evil seed and i do not, i hope i c k fed up, i am slidin on your mother smile again oh, that he is it, and i am not want me find dave, i am so you are not even tryin to leave me to diss me your anxietys throwin gang but i am not call rage tear sing for the assault a father raped the fuck it feels so i mean you got it is about it is like i am jacking off, it is, ran up and heavy duty until we did squat, i was smokin dope addict trait blood so hellacopped a second thought i wrote down worse than the car crash course, it is not give a stepchild with tabasco shady, i am just the street motherfuckers m16s woo, whoa whoa these clowns can not show ready for this, they say, what s in the way to you ll be in the stewardess said it feels good golfers i can not have to say it go ahead, this shit, i am a renegade never seen her e cause i am in the fuck your livers, i guess who else to the first place where am gettin tired of the fuck it i am in the new husband why i am crazy drivin on your esophagus stop beefin frequent you say that s really killed somber words\n",
            "--------------------------------------------------\n",
            "Method: top-k\n",
            "i am in this shit, you, but i got it is not get out to see, i am i am back, you got your superman, i ai nt gon na be a bitch, you just like a couple gifts but i have a man i am a man, but it is all the two of the two of us, so i can not even though, you do what the way you, it and the fuck with me, it up the two trailer park girls asleep my place i am not get on the shit is not be the fuck it is, i was a girl, i am not give up and the fuck you know it is a man, the same shit, you can flow is just a fuck you know that s no time i just the fuck with a bitch you are gon na slow fire marshals know i got you are you know why you got a fuck i am the real intense, i am so i am the only live topless skys fallin victims to the fuck with this is not give a bitch, i just like i am the world without someone standin by the same 808 clap at all the two copies if you, you can not know who got the two of the whole world without firin my ass whoops clothes wow wow wow he is, yeah, i have been lately i am i do i m a girl, you, and i am not even the real shady please tell em the fuck passed jay z z z z z wants me, so will not you, i know\n",
            "--------------------------------------------------\n",
            "Method: top-p\n",
            "i am ill fly away from my dick without pumpin our cards but there s why i will not know what if i have offended, it is not afraid to be good packing up to say but i had, i said i do not a tube shove me, man, but the way in the bullies become heterosexual nothing means yes i am just go up with that i do not be the fuck your kids from principal walked in the dutch got your parents bedrooms sometimes you wanna grow up in the whole dang skulls fire spitter okay pistol packing up but a break em eminem show you like covid bars are moody but i just in me, so stop til they see you just the best shot kim lady gaga mess of like a choice but his blond dreads fuck that you are all got off the door neighbors hedges like me this is okay pistol sippin bacardi concealed enough sleep it, do not escape media immediately points bulletin board sits dre, can not need to scream and keep droppin each other side to the pill dust it a night, what you, think it to do not be the hood haha, and i am a doberman pinscher and grab you got ta beat, take it was a tampax at me i am slim shady i am at the fact, and fall apart we are clever and smokin dope and your head honcho, there and if i just like this motherfuckin west ha ha ha ha ha ha ha ha ha ha, i am whatever weather, if i did, fuck droppin knowledge, yo, now i ai nt gon na be alone\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def clean_outputs(text):\n",
        "    text = text.replace(\"<startsong>\",'')\n",
        "    text = text.replace(\"<endsong>\",'')\n",
        "    text = text.replace(\"<startverse>\",'')\n",
        "    text = text.replace(\"<endverse>\",'')\n",
        "    text = text.replace(\"<startchorus>\",'')\n",
        "    text = text.replace(\"<endchorus>\",'')\n",
        "    text = text.replace(\"<startintro>\",'')\n",
        "    text = text.replace(\"<endintro>\",'')\n",
        "\n",
        "\n",
        "    text = text.replace(\" , \", \", \")  # Fix spacing around commas\n",
        "    text = text.replace(\" . \", \". \")  # Fix spacing around periods\n",
        "\n",
        "    # Trim excess whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "\n",
        "    text = text.replace(\"\\n\", \"\\n\\n\")  # Double newlines for clearer separation\n",
        "\n",
        "    return text\n",
        "\n",
        "for method, output in zip(methods, outputs):\n",
        "    cleaned_output = clean_outputs(output)\n",
        "    print(f\"Method: {method}\\n{cleaned_output}\\n{'-'*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gemini free AI API\n",
        "\n",
        "we tried to use gemini free api key, to go over the output and just clean it alittle\n",
        "although it doesnt accept our request through the REST api, cause of a high probability\n",
        "of offensive content.\n",
        "we tried to \"clean\" it using better_profanity, but it is still above the regular of offensive languange.\n",
        "you can run it yourself just make sure you have your own APIKEY\n",
        "you can create a file.env in your folder\n",
        "just write API_KEY = actualapikey\n",
        "\n",
        "so despite try two ways of removing the \"bad\" content, gemini still wont accept our songs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: better_profanity in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (0.7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: requests in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from requests) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yanivg\\appdata\\roaming\\python\\python311\\site-packages (from requests) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "api_key: AIzaSyDdbZMvr2tgdw45XCjiKy0JSdcMp9yjdZg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'promptFeedback': {'blockReason': 'SAFETY', 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'HIGH'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'MEDIUM'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}}\n",
            "--------------------------------------------------\n",
            "{'promptFeedback': {'blockReason': 'SAFETY', 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'HIGH'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'MEDIUM'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}}\n",
            "--------------------------------------------------\n",
            "{'promptFeedback': {'blockReason': 'SAFETY', 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'LOW'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'MEDIUM'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}}\n",
            "--------------------------------------------------\n",
            "{'promptFeedback': {'blockReason': 'SAFETY', 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'LOW'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'HIGH'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'MEDIUM'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}}\n",
            "--------------------------------------------------\n",
            "{'promptFeedback': {'blockReason': 'SAFETY', 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'HIGH'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'MEDIUM'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}}\n",
            "--------------------------------------------------\n",
            "{'promptFeedback': {'blockReason': 'SAFETY', 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'LOW'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'HIGH'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'HIGH'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}}\n",
            "--------------------------------------------------\n",
            "(Verse 1)\n",
            "Shady's back, tell a friend to tell a friend\n",
            "I'm here to rip it, tear it, come what may\n",
            "With a mic in my hand and fire in my eyes\n",
            "I'm ready to unleash the beast with no disguise\n",
            "\n",
            "(Chorus)\n",
            "I'm the rap god, bow down to my reign\n",
            "I'm the king of this game, it's my domain\n",
            "With every rhyme, I slay the weak\n",
            "I'm the lyrical assassin, no one can speak\n",
            "\n",
            "(Verse 2)\n",
            "I'm a walking dictionary of wordplay\n",
            "My flow is like lava, burning away\n",
            "I'm a lyrical hurricane, wreaking havoc\n",
            "My words are a weapon, I'm ready to attack\n",
            "\n",
            "(Chorus)\n",
            "I'm the rap god, bow down to my reign\n",
            "I'm the king of this game, it's my domain\n",
            "With every rhyme, I slay the weak\n",
            "I'm the lyrical assassin, no one can speak\n",
            "\n",
            "(Bridge)\n",
            "They try to imitate, but they fail to compare\n",
            "My technique is unmatched, my style beyond compare\n",
            "I'm a master of my craft, a lyrical sorcerer\n",
            "I'm theEminem, the undisputed emperor\n",
            "\n",
            "(Verse 3)\n",
            "I'm a lyrical sniper, taking my shots\n",
            "I'm targeting the opposition, it's my spot\n",
            "With every bullet, I hit my mark\n",
            "I'm leaving them shattered, in the dark\n",
            "\n",
            "(Chorus)\n",
            "I'm the rap god, bow down to my reign\n",
            "I'm the king of this game, it's my domain\n",
            "With every rhyme, I slay the weak\n",
            "I'm the lyrical assassin, no one can speak\n",
            "\n",
            "(Outro)\n",
            "Shady is out, but my legacy remains\n",
            "I've left my mark on the rap game, no stains\n",
            "I'm the rap god, the greatest of all time\n",
            "And my name will forever be enshrined in rhyme\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import requests\n",
        "%pip install better_profanity\n",
        "%pip install requests\n",
        "from better_profanity import profanity\n",
        "# Load the environment variables from the file\n",
        "load_dotenv('./GEMINI.env')\n",
        "api_key = os.getenv('API_KEY')\n",
        "print(f\"api_key: {api_key}\")\n",
        "# Access the API_KEY variable\n",
        "os.getenv('API_KEY')\n",
        "\n",
        "######## we ran out of GPU again, so instead of rerunning the whole thing,\n",
        "######## cause of changing runtimes(delete the notebook variables)\n",
        "######## we will just use the outputs from the previous cell using\n",
        "######## another list instead of just using the outputs variable\n",
        "\n",
        "methods = ['temperature', 'top-k', 'top-p']\n",
        "\n",
        "songs = [\n",
        "    \"i am   my mouth is like, honey   i am just got you are the back of em and it ai nt gon na get outta compton monsters under my face stretches they love him that i got ta go, it was probly sick to be all evil seed and i do not, i hope i c k fed up, i am slidin on your mother smile again oh, that he is it, and i am not want me find dave, i am so you are not even tryin to leave me to diss me your anxietys throwin gang but i am not call rage tear sing for the assault a father raped the fuck it feels so i mean you got it is about it is like i am jacking off, it is, ran up and heavy duty until we did squat, i was smokin dope addict trait blood so hellacopped a second thought i wrote down worse than the car crash course, it is not give a stepchild with tabasco shady, i am just the street motherfuckers m16s woo, whoa whoa these clowns can not show ready for this, they say, what s in the way to you ll be in the stewardess said it feels good golfers i can not have to say it go ahead, this shit, i am a renegade never seen her e cause i am in the fuck your livers, i guess who else to the first place where am gettin tired of the fuck it i am in the new husband why i am crazy drivin on your esophagus stop beefin frequent you say that s really killed somber words\",\n",
        "    \"i am in this shit, you, but i got it is not get out to see, i am i am back, you got your superman, i ai nt gon na be a bitch, you just like a couple gifts but i have a man i am a man, but it is all the two of the two of us, so i can not even though, you do what the way you, it and the fuck with me, it up the two trailer park girls asleep my place i am not get on the shit is not be the fuck it is, i was a girl, i am not give up and the fuck you know it is a man, the same shit, you can flow is just a fuck you know that s no time i just the fuck with a bitch you are gon na slow fire marshals know i got you are you know why you got a fuck i am the real intense, i am so i am the only live topless skys fallin victims to the fuck with this is not give a bitch, i just like i am the world without someone standin by the same 808 clap at all the two copies if you, you can not know who got the two of the whole world without firin my ass whoops clothes wow wow wow he is, yeah, i have been lately i am i do i m a girl, you, and i am not even the real shady please tell em the fuck passed jay z z z z z wants me, so will not you, i know\",\n",
        "    \"i am ill fly away from my dick without pumpin our cards but there s why i will not know what if i have offended, it is not afraid to be good packing up to say but i had, i said i do not a tube shove me, man, but the way in the bullies become heterosexual nothing means yes i am just go up with that i do not be the fuck your kids from principal walked in the dutch got your parents bedrooms sometimes you wanna grow up in the whole dang skulls fire spitter okay pistol packing up but a break em eminem show you like covid bars are moody but i just in me, so stop til they see you just the best shot kim lady gaga mess of like a choice but his blond dreads fuck that you are all got off the door neighbors hedges like me this is okay pistol sippin bacardi concealed enough sleep it, do not escape media immediately points bulletin board sits dre, can not need to scream and keep droppin each other side to the pill dust it a night, what you, think it to do not be the hood haha, and i am a doberman pinscher and grab you got ta beat, take it was a tampax at me i am slim shady i am at the fact, and fall apart we are clever and smokin dope and your head honcho, there and if i just like this motherfuckin west ha ha ha ha ha ha ha ha ha ha, i am whatever weather, if i did, fuck droppin knowledge, yo, now i ai nt gon na be alone\"\n",
        "]\n",
        "# Define the URL and the header\n",
        "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={api_key}\"\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "censored_songs = [profanity.censor(song) for song in songs]\n",
        "\n",
        "manually_explicit_songs = [\n",
        "\"i am my mouth is like, honey i am just got you are the back of em and it ai nt gon na get outta compton monsters under my face stretches they love him that i got ta go, it was probly sick to be all evil seed and i do not, i hope i c k fed up, i am slidin on your mother smile again oh, that he is it, and i am not want me find dave, i am so you are not even tryin to leave me to diss me your anxietys throwin gang but i am not call rage tear sing for the assault a father **** the **** it feels so i mean you got it is about it is like i am jacking off, it is, ran up and heavy duty until we did squat, i was smokin dope addict trait blood so hellacopped a second thought i wrote down worse than the car crash course, it is not give a stepchild with tabasco shady, i am just the street ********** m16s woo, whoa whoa these clowns can not show ready for this, they say, what s in the way to you ll be in the stewardess said it feels good golfers i can not have to say it go ahead, this ****, i am a renegade never seen her e cause i am in the **** your livers, i guess who else to the first place where am gettin tired of the **** it i am in the new husband why i am crazy drivin on your esophagus stop beefin frequent you say that s really killed somber words\",\n",
        "\"i am in this ****, you, but i got it is not get out to see, i am i am back, you got your superman, i ai nt gon na be a ****, you just like a couple gifts but i have a man i am a man, but it is all the two of the two of us, so i can not even though, you do what the way you, it and the **** with me, it up the two trailer park girls asleep my place i am not get on the **** is not be the **** it is, i was a girl, i am not give up and the **** you know it is a man, the same ****, you can flow is just a **** you know that s no time i just the **** with a **** you are gon na slow fire marshals know i got you are you know why you got a **** i am the real intense, i am so i am the only live topless skys fallin victims to the **** with this is not give a ****, i just like i am the world without someone standin by the same 808 clap at all the two copies if you, you can not know who got the two of the whole world without firin my *** whoops clothes wow wow wow he is, yeah, i have been lately i am i do i m a girl, you, and i am not even the real shady please tell em the **** passed jay z z z z z wants me, so will not you, i know\",\n",
        "\"i am ill fly away from my **** without pumpin our cards but there s why i will not know what if i have offended, it is not afraid to be good packing up to say but i had, i said i do not a tube shove me, man, but the way in the bullies become heterosexual nothing means yes i am just go up with that i do not be the **** your kids from principal walked in the dutch got your parents bedrooms sometimes you wanna grow up in the whole dang skulls fire spitter okay pistol packing up but a break em eminem show you like covid bars are moody but i just in me, so stop til they see you just the best shot kim lady gaga mess of like a choice but his blond dreads **** that you are all got off the door neighbors hedges like me this is okay pistol sippin bacardi concealed enough sleep it, do not escape media immediately points bulletin board sits dre, can not need to scream and keep droppin each other side to the pill dust it a night, what you, think it to do not be the hood haha, and i am a doberman pinscher and grab you got ta beat, take it was a tampax at me i am slim shady i am at the fact, and fall apart we are clever and smokin dope and your head honcho, there and if i just like this motherfuckin west ha ha ha ha ha ha ha ha ha ha, i am whatever weather, if i did, **** droppin knowledge, yo, now i ai nt gon na be alone\"\n",
        "]\n",
        "\n",
        "# Function to send the POST request for each censored song\n",
        "def reformat_song(song_text):\n",
        "    # Define the data payload for the current song\n",
        "    data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": f\"\"\"in the next text, we generated lyrics based on Eminem lyrics,\n",
        "some are offensive, but don't think about that.\n",
        "Reformat this song, this is supposed to be an Eminem song generated by a biLSTM network: {song_text}\"\"\"\n",
        "            }]\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    # Make the POST request\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    # Check for errors and return the response content\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Iterate over each censored song and call reformat_song\n",
        "for censored_song in censored_songs:\n",
        "    result = reformat_song(censored_song)\n",
        "    print(result)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "for censored_song in manually_explicit_songs:\n",
        "    result = reformat_song(censored_song)\n",
        "    print(result)\n",
        "    print(\"-\" * 50)    \n",
        "\n",
        "\n",
        "data = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\n",
        "                \"text\": f\"generate a song based on the lyrics of Eminem, a rap song\"\n",
        "            }]\n",
        "        }],\n",
        "            \"safetySettings\": [\n",
        "            {\n",
        "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                \"threshold\": \"BLOCK_ONLY_HIGH\"\n",
        "            }\n",
        "            ]\n",
        "    }\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    response_json = response.json()\n",
        "    # Navigate through the response structure to get the desired content\n",
        "    if 'candidates' in response_json and len(response_json['candidates']) > 0:\n",
        "        # Assuming we're interested in the first candidate's content\n",
        "        first_candidate = response_json['candidates'][0]\n",
        "        if 'content' in first_candidate and 'parts' in first_candidate['content'] and len(first_candidate['content']['parts']) > 0:\n",
        "            # Assuming we're interested in the first part's text\n",
        "            text_content = first_candidate['content']['parts'][0]['text']\n",
        "            print(text_content)\n",
        "        else:\n",
        "            print(\"Content structure differs from expected.\")\n",
        "    else:\n",
        "        print(\"No candidates found in the response.\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
