{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nlp final project\n",
    "\n",
    "1. Yaniv Gabay - 205745615 - yanivga@edu.hac.ac.il\n",
    "2. Shahar Asher - 209305408 - shaharas@edu.hac.ac.il\n",
    "3. Hadar Liel Harush - 211721568 - hadarhar@edu.hac.ac.il\n",
    "## Eminem song generator\n",
    "\n",
    "This project will compare different models, all trained on the legendary hip-hop artist Eminem lyrics.\n",
    "We want to expirment with different models, our own model, different apis, to see and compare the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be added?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanivg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######imports\n",
    "\n",
    "#pip install allennlp\n",
    "#pip install flair\n",
    "#pip install pronouncing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings, ELMoEmbeddings,TransformerWordEmbeddings\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import pronouncing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download NLTK data (e.g., WordNet, Punkt tokenizer models)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#### if you want to export data\n",
    "####\n",
    "debug = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with encoding utf-8: 'utf-8' codec can't decode byte 0x92 in position 6: invalid start byte\n",
      "Success with encoding: latin1\n",
      "                        Album_Name       Song_Name  \\\n",
      "0  Music To Be Murdered By: Side B  Alfred (Intro)   \n",
      "1  Music To Be Murdered By: Side B     Black Magic   \n",
      "2  Music To Be Murdered By: Side B  Alfreds Theme   \n",
      "3  Music To Be Murdered By: Side B       Tone Deaf   \n",
      "4  Music To Be Murdered By: Side B  Book of Rhymes   \n",
      "\n",
      "                                              Lyrics  \\\n",
      "0  [Intro: Alfred Hitchcock]\\nThus far, this albu...   \n",
      "1  [Chorus: Skylar Grey & Eminem]\\nBlack magic, n...   \n",
      "2  [Verse 1]\\nBefore I check the mic (Check, chec...   \n",
      "3  [Intro]\\nYeah, I'm sorry (Huh?)\\nWhat did you ...   \n",
      "4  [Intro]\\nI don't smile, I don't frown, get too...   \n",
      "\n",
      "                                           Album_URL   Views  \\\n",
      "0  https://genius.com/albums/Eminem/Music-to-be-m...   24.3K   \n",
      "1  https://genius.com/albums/Eminem/Music-to-be-m...  180.6K   \n",
      "2  https://genius.com/albums/Eminem/Music-to-be-m...  285.6K   \n",
      "3  https://genius.com/albums/Eminem/Music-to-be-m...  210.9K   \n",
      "4  https://genius.com/albums/Eminem/Music-to-be-m...  193.3K   \n",
      "\n",
      "        Release_date Unnamed: 6  \n",
      "0  December 18, 2020        NaN  \n",
      "1  December 18, 2020        NaN  \n",
      "2  December 18, 2020        NaN  \n",
      "3  December 18, 2020        NaN  \n",
      "4  December 18, 2020        NaN  \n"
     ]
    }
   ],
   "source": [
    "##### Loading the data\n",
    "### we be using <startVerse> <endVerse> <startChorus><endChorus>\n",
    "## to give our model some different between the verses and the choruses.\n",
    "\n",
    "#important to export the data to see the effect\n",
    "#of changes, especilly with a small dataset\n",
    "def export_data(data,stage,debug_mode):\n",
    "    if(not debug_mode):\n",
    "        return\n",
    "    try:\n",
    "        data.to_csv('data_'+stage+'.csv')\n",
    "    except Exception as e:\n",
    "        print(\"Error exporting the csv file:\", e)  \n",
    "#basic import\n",
    "def import_data(location):\n",
    "    df = read_data(location)\n",
    "    return df\n",
    "#read with try except,can add more specific\n",
    "#we try different encoding, you can never know what chars you will find.\n",
    "# this one is working with latin1, but we still left the function\n",
    "#exceptions\n",
    "def read_data(location):\n",
    "    try_encodings = ['utf-8', 'latin1', 'utf-16', 'cp1252', 'ISO-8859-1', 'windows-1252']\n",
    "    for encoding in try_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(location ,encoding=encoding,delimiter='\\t')\n",
    "            print(f\"Success with encoding: {encoding}\")\n",
    "            print(df.head())\n",
    "            return df\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error with encoding {encoding}: {e}\")\n",
    "####\n",
    "file_loc = './Eminem_Lyrics.csv'\n",
    "df = import_data(file_loc)\n",
    "\n",
    "export_data(df,'afterImport-1',debug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      <startSong>\\n<startIntro>\\nThus far, this albu...\n",
      "1      <startSong>\\n<startChorus>\\nBlack magic, night...\n",
      "2      <startSong>\\n<startVerse>\\nBefore I check the ...\n",
      "3      <startSong>\\n<startIntro>\\nYeah, I'm sorry (Hu...\n",
      "4      <startSong>\\n<startIntro>\\nI don't smile, I do...\n",
      "                             ...                        \n",
      "343    <startSong>\\n<startChorus>\\nI know there's som...\n",
      "344    <startSong>\\n<startIntro>\\nYeah, yeah, I get i...\n",
      "345    <startSong>\\n<startVerse>\\nI cut back on the s...\n",
      "346    <startSong>\\n<startIntro>\\nC'mon!\\n\\n<endIntro...\n",
      "347    <startSong>\\n<startIntro>\\nCDs ain't selling n...\n",
      "Name: Lyrics, Length: 348, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#actual messing with the dataset\n",
    "import re\n",
    "\n",
    "\n",
    "def insert_tokens(lyrics):\n",
    "    # Pattern to identify all tags (verses, choruses, etc.)\n",
    "    pattern = re.compile(r'\\[(Verse|Chorus|Intro|Outro)')\n",
    "    \n",
    "    # Placeholder for processed lyrics\n",
    "    processed_lyrics = \"<startSong>\\n\"\n",
    "    last_tag = None\n",
    "\n",
    "    for line in lyrics.split('\\n'):\n",
    "        tag_match = pattern.match(line)\n",
    "        if tag_match:\n",
    "            # Close the previous tag if exists\n",
    "            if last_tag:\n",
    "                processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "            # Update last_tag and open a new tag\n",
    "            last_tag = tag_match.group(1)  # Capture the tag name (Verse or Chorus)\n",
    "            processed_lyrics += f\"<start{last_tag}>\\n\"\n",
    "        else:\n",
    "            processed_lyrics += line + \"\\n\"\n",
    "\n",
    "    # Close the last opened tag\n",
    "    if last_tag:\n",
    "        processed_lyrics += f\"<end{last_tag}>\\n\"\n",
    "    processed_lyrics += \"<endSong>\"\n",
    "\n",
    "    return processed_lyrics\n",
    "\n",
    "def remove_remaining_tags(lyrics):\n",
    "    # Pattern to match any text within square brackets\n",
    "    pattern = re.compile(r'\\[[^\\]]*\\]')\n",
    "    cleaned_lyrics = re.sub(pattern, '', lyrics)\n",
    "    return cleaned_lyrics\n",
    "\n",
    "def handle_special(lyrics):\n",
    "    #remove ' and - and ? and !\n",
    "    output_string = re.sub(r\"['\\-?!]\",'', lyrics)\n",
    "    return output_string\n",
    "\n",
    "new_df = df['Lyrics'].apply(insert_tokens)\n",
    "\n",
    "print(new_df)\n",
    "export_data(new_df,'afterInsertTokens',debug)\n",
    "\n",
    "\n",
    "## some songs can have some another tags, like post-chorus,pre etc. \n",
    "## we arent intresnted in those, so we will just delete them in the next step\n",
    "new_df = new_df.apply(remove_remaining_tags)\n",
    "new_df = new_df.apply(handle_special)\n",
    "#lowercase everything\n",
    "new_df = new_df.str.lower()\n",
    "\n",
    "export_data(new_df,'afterLowerAndRemove',debug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed characters: \n",
      "Removed characters: á\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ö\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: ó\n",
      "Removed characters: \n",
      "Removed characters: áó\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: óéáí\n",
      "Removed characters: \n",
      "Removed characters: ú\n",
      "Removed characters: \n",
      "Removed characters: â\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: è\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: áâ\n",
      "Removed characters: \n",
      "Removed characters: ä\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éàéàéà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éß\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ï\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éâé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ü\n",
      "Removed characters: éé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: é\n",
      "Removed characters: é\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ç\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: ééé\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: éà\n",
      "Removed characters: \n",
      "Removed characters: \n",
      "Removed characters: \n"
     ]
    }
   ],
   "source": [
    "### special chars\n",
    "def remove_non_ascii_and_print(text):\n",
    "    cleaned_text = []\n",
    "    removed_chars = []\n",
    "    for char in text:\n",
    "        if ord(char) < 128:\n",
    "            cleaned_text.append(char)\n",
    "        else:\n",
    "            removed_chars.append(char)\n",
    "    if removed_chars:\n",
    "        print(f\"Removed characters: {''.join(removed_chars)}\")\n",
    "    return ''.join(cleaned_text)\n",
    "clean = new_df.apply(remove_non_ascii_and_print)\n",
    "\n",
    "\n",
    "export_data(clean,'afterRemoveNonAscii',debug)\n",
    "\n",
    "#### we still have (lyrics) in the dataset, need to decide\n",
    "### we still have ! or ? in the dataset, should we remove? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will try several tokenizer options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of added tokens: 10\n",
      "\n",
      "0    [startsong, \\n, startintro, \\n, thus, far, ,, ...\n",
      "1    [startsong, \\n, startchorus, \\n, black, magic,...\n",
      "2    [startsong, \\n, startverse, \\n, before, i, che...\n",
      "3    [startsong, \\n, startintro, \\n, yeah, ,, i, m,...\n",
      "4    [startsong, \\n, startintro, \\n, i, do, nt, smi...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, far, ,, this, al...\n",
      "1    [startsong, startchorus, black, magic, ,, nigh...\n",
      "2    [startsong, startverse, before, i, check, the,...\n",
      "3    [startsong, startintro, yeah, ,, im, sorry, (,...\n",
      "4    [startsong, startintro, i, dont, smile, ,, i, ...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, far, ,, this, al...\n",
      "1    [startsong, startchorus, black, magic, ,, nigh...\n",
      "2    [startsong, startverse, before, i, check, the,...\n",
      "3    [startsong, startintro, yeah, ,, im, sorry, (,...\n",
      "4    [startsong, startintro, i, don, ##t, smile, ,,...\n",
      "Name: Lyrics, dtype: object\n",
      "\n",
      "0    [startsong, startintro, thus, Ġfar, ,, Ġthis, ...\n",
      "1    [startsong, startchorus, black, Ġmagic, ,, Ġni...\n",
      "2    [startsong, startverse, before, Ġi, Ġcheck, Ġt...\n",
      "3    [startsong, startintro, yeah, ,, Ġim, Ġsorry, ...\n",
      "4    [startsong, startintro, i, Ġdont, Ġsmile, ,, Ġ...\n",
      "Name: Lyrics, dtype: object\n"
     ]
    }
   ],
   "source": [
    "temp_switch = {'<startverse>':'startverse',\n",
    "                '<endverse>':'endverse',\n",
    "                '<startchorus>':'startchorus',\n",
    "                '<endchorus>':'endchorus',\n",
    "                '<startoutro>':'startoutro',\n",
    "                '<endoutro>':'endoutro',\n",
    "                '<startintro>':'startintro',\n",
    "                '<endintro>':'endintro',\n",
    "                '<startsong>':'startsong',\n",
    "                '<endsong>':'endsong'}\n",
    "def replace_tags(text, temp_switch, reverse=False):\n",
    "    if reverse:\n",
    "        temp_switch = {v: k for k, v in temp_switch.items()}\n",
    "    \n",
    "    for tag, replacement in temp_switch.items():\n",
    "        text = text.replace(tag, replacement)\n",
    "    return text\n",
    "\n",
    "temp_clean = clean.apply(lambda x: replace_tags(x, temp_switch, reverse=False))\n",
    "export_data(temp_clean,'temp_clean',debug)\n",
    "#pip install spacy\n",
    "import spacy\n",
    "#python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_tokens_df = temp_clean.apply(lambda x: [token.text for token in nlp(x)])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk_tokens_df = temp_clean.apply(word_tokenize)\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# For BERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "num_of_added_tokens = tokenizer_bert.add_special_tokens({'additional_special_tokens':\n",
    "                                    ['startverse', 'endverse',\n",
    "                                      'startchorus', 'endchorus',\n",
    "                                        'startoutro', 'endoutro', \n",
    "                                        'startintro', 'endintro',\n",
    "                                        'startsong', 'endsong']})\n",
    "print(f\"Number of added tokens: {num_of_added_tokens}\")\n",
    "#BERT_tokens_df = temp_clean.apply(lambda x: tokenizer_bert.tokenize(x))\n",
    "from transformers import GPT2Tokenizer\n",
    "# For GPT-2\n",
    "\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "num_of_added_tokens = tokenizer_gpt2.add_special_tokens({'additional_special_tokens':\n",
    "                                    ['startverse', 'endverse',\n",
    "                                      'startchorus', 'endchorus',\n",
    "                                        'startoutro', 'endoutro', \n",
    "                                        'startintro', 'endintro',\n",
    "                                          'startsong', 'endsong']})\n",
    "#gpt2_tokens_df = temp_clean.apply(lambda x: tokenizer_gpt2.tokenize(x))\n",
    "\n",
    "#export_data(spacy_tokens_df,'spacy',debug)\n",
    "#export_data(nltk_tokens_df,'nltk',debug)\n",
    "#export_data(BERT_tokens_df,'BERT',debug)\n",
    "#export_data(gpt2_tokens_df,'GPT2', debug )\n",
    "\n",
    "#total_token_models = [spacy_tokens_df, nltk_tokens_df, BERT_tokens_df, gpt2_tokens_df]\n",
    "#for model in total_token_models:\n",
    "  #  print(\"\")\n",
    "   # print(model.head())\n",
    "\n",
    "## pronouncing library\n",
    "## we will use it to get the phonetic features of the words\n",
    "    \n",
    "\n",
    "import pronouncing\n",
    "\n",
    "def get_phonetic_features(word):\n",
    "    phones = pronouncing.phones_for_word(word)\n",
    "    stress = [pronouncing.stresses(p) for p in phones] if phones else ''\n",
    "    return {'phones': phones, 'stress': stress}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#spacy_phonetics_df = spacy_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "#nltk_phonetics_df = nltk_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "#bert_phonetics_df = BERT_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "#gpt2_phonetics_df = gpt2_tokens_df.apply(lambda tokens: [get_phonetic_features(token) for token in tokens])\n",
    "\n",
    "#export_data(spacy_phonetics_df, 'spacy_phonetics',debug)\n",
    "#export_data(nltk_phonetics_df, 'nltk_phonetics',  debug)\n",
    "#xport_data(bert_phonetics_df, 'bert_phonetics', debug)\n",
    "#xport_data(gpt2_phonetics_df, 'gpt2_phonetics', debug)\n",
    "\n",
    "#total_phonetic_models = [spacy_phonetics_df, nltk_phonetics_df, bert_phonetics_df, gpt2_phonetics_df]\n",
    "\n",
    "\n",
    "##### now , we need to perpare the data for the embeddings\n",
    "### so padding, indexing unique tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with encoding: utf-8\n",
      "                                                  ,0\n",
      "0  0,\"[tensor([ 0.0000,  0.0000,  0.0000,  ..., -...\n",
      "1         device='cuda:0'), tensor([ 0.0000,  0.0...\n",
      "2         device='cuda:0'), tensor([-0.0742,  0.0...\n",
      "3         device='cuda:0'), tensor([-1.2351e-01, ...\n",
      "4          -7.3642e-02,  5.4657e-04], device='cud...\n",
      "     phone_K R EH1 D IH0 T  phone_W ER1 IY0 Z  phone_HH AA1 S T AH0 L  \\\n",
      "0                        0                  0                       0   \n",
      "1                        0                  0                       0   \n",
      "2                        0                  0                       0   \n",
      "3                        0                  0                       0   \n",
      "4                        0                  0                       0   \n",
      "..                     ...                ...                     ...   \n",
      "343                      0                  0                       0   \n",
      "344                      0                  0                       0   \n",
      "345                      0                  0                       0   \n",
      "346                      0                  0                       0   \n",
      "347                      0                  0                       0   \n",
      "\n",
      "     phone_T AE1 NG G OW0  phone_S IH1 R AH0 P  phone_G ER1 L F R EH2 N D  \\\n",
      "0                       0                    0                          0   \n",
      "1                       0                    0                          0   \n",
      "2                       0                    0                          0   \n",
      "3                       0                    0                          0   \n",
      "4                       0                    0                          0   \n",
      "..                    ...                  ...                        ...   \n",
      "343                     0                    0                          0   \n",
      "344                     0                    0                          0   \n",
      "345                     0                    0                          0   \n",
      "346                     0                    0                          0   \n",
      "347                     0                    0                          0   \n",
      "\n",
      "     phone_F IH1 Z IH0 K AH0 L IY0  phone_W EH2 L B IY1 IH0 NG  phone_W ER1 S  \\\n",
      "0                                0                           0              0   \n",
      "1                                0                           0              0   \n",
      "2                                0                           0              0   \n",
      "3                                0                           0              0   \n",
      "4                                0                           0              0   \n",
      "..                             ...                         ...            ...   \n",
      "343                              0                           0              0   \n",
      "344                              0                           0              0   \n",
      "345                              0                           0              0   \n",
      "346                              0                           0              0   \n",
      "347                              0                           0              0   \n",
      "\n",
      "     phone_G ER0 IH1 L AH0  ...  stress_00010  stress_21002  stress_110  \\\n",
      "0                        0  ...             0             0           0   \n",
      "1                        0  ...             0             0           0   \n",
      "2                        0  ...             0             0           0   \n",
      "3                        0  ...             0             0           0   \n",
      "4                        0  ...             0             0           0   \n",
      "..                     ...  ...           ...           ...         ...   \n",
      "343                      0  ...             0             0           0   \n",
      "344                      0  ...             0             0           0   \n",
      "345                      0  ...             0             0           0   \n",
      "346                      0  ...             0             0           0   \n",
      "347                      0  ...             0             0           0   \n",
      "\n",
      "     stress_1220  stress_1020  stress_212  stress_1200  stress_01000  \\\n",
      "0              0            0           0            0             0   \n",
      "1              0            0           0            1             0   \n",
      "2              0            1           0            0             1   \n",
      "3              0            1           0            0             0   \n",
      "4              0            1           0            0             0   \n",
      "..           ...          ...         ...          ...           ...   \n",
      "343            0            1           0            1             0   \n",
      "344            0            0           0            0             0   \n",
      "345            0            0           0            0             0   \n",
      "346            0            0           0            0             0   \n",
      "347            0            1           0            0             0   \n",
      "\n",
      "     stress_2210  stress_10100  \n",
      "0              0             0  \n",
      "1              0             0  \n",
      "2              0             0  \n",
      "3              0             0  \n",
      "4              0             0  \n",
      "..           ...           ...  \n",
      "343            0             0  \n",
      "344            0             0  \n",
      "345            0             0  \n",
      "346            0             0  \n",
      "347            0             0  \n",
      "\n",
      "[348 rows x 12205 columns]\n",
      "                                                  ,0\n",
      "0  0,\"[tensor([ 0.0000,  0.0000,  0.0000,  ..., -...\n",
      "1         device='cuda:0'), tensor([ 0.0000,  0.0...\n",
      "2         device='cuda:0'), tensor([-0.0742,  0.0...\n",
      "3         device='cuda:0'), tensor([-1.2351e-01, ...\n",
      "4          -7.3642e-02,  5.4657e-04], device='cud...\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Define stacked embeddings\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "    WordEmbeddings('glove'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "])\n",
    "\n",
    "# Initialize DocumentPoolEmbeddings with stacked embeddings\n",
    "pool_embeddings = DocumentPoolEmbeddings([stacked_embeddings])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(series):\n",
    "    unique_tokens = set()\n",
    "    for song in series:\n",
    "        unique_tokens.update(song)\n",
    "\n",
    "    # Reserve 0 for padding\n",
    "    token_to_idx = {'<PAD>': 0}\n",
    "\n",
    "    # Start indexing from 1 for the rest of the tokens\n",
    "    for idx, token in enumerate(unique_tokens, start=1):\n",
    "        token_to_idx[token] = idx\n",
    "\n",
    "    # Optionally add specific indices for special tokens if not already included\n",
    "    # This is just an example; adjust according to your specific tokens\n",
    "   # special_tokens = ['<START>', '<END>', '<STARTVERSE>', '<ENDVERSE>']\n",
    "   # for token in special_tokens:\n",
    "    #    if token not in token_to_idx:\n",
    "     #       token_to_idx[token] = len(token_to_idx)\n",
    "    #\n",
    "    return token_to_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokens_to_indices(tokenized_song, token_to_idx):\n",
    "    return [token_to_idx.get(token, token_to_idx['<PAD>']) for token in tokenized_song]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TokenSongsDataset(Dataset):\n",
    "    def __init__(self, series, seq_length):\n",
    "        self.series = series\n",
    "        self.seq_length = seq_length\n",
    "        self.tokens = self.prepare_tokens()\n",
    "\n",
    "    def prepare_tokens(self):\n",
    "        all_tokens = []\n",
    "        for token_list in self.series:\n",
    "            all_tokens.extend(token_list)\n",
    "        return all_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.tokens[idx:idx + self.seq_length], dtype=torch.long),\n",
    "            torch.tensor(self.tokens[idx + self.seq_length], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.seq_length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        embedded = self.embedding(sequence)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vocab_size_from_series(tokenized_songs_series):\n",
    "    vocabulary = set()  # Initialize an empty set to store your unique tokens\n",
    "\n",
    "    # Iterate through each item in the Series\n",
    "    for tokenized_song in tokenized_songs_series:\n",
    "        # Update the vocabulary set with tokens from this song\n",
    "        vocabulary.update(tokenized_song)\n",
    "\n",
    "    # The vocabulary size is the number of unique tokens in the set\n",
    "    vocab_size = len(vocabulary)\n",
    "    return vocab_size\n",
    "\n",
    "# Assuming 'spacy_tokens_series' is your Series with tokenized songs\n",
    "token_to_idx = build_vocab(spacy_tokens_df)\n",
    "# Apply the conversion to the entire series\n",
    "indexed_tokens_series = spacy_tokens_df.apply(lambda song: tokens_to_indices(song, token_to_idx))\n",
    "\n",
    "\n",
    "seq_length = 70  # Define the sequence length\n",
    "dataset = TokenSongsDataset(indexed_tokens_series, seq_length)\n",
    "\n",
    "# Vocabulary size is the size of the token_to_idx mapping\n",
    "vocab_size = len(token_to_idx)\n",
    "\n",
    "# Initialize DataLoader\n",
    "batch_size = 12\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "model = BiLSTM(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Training loop remains the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch  # Unpack the batch into inputs and targets\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Targets shape:\", targets.shape)\n",
    "    break  # Just to check the first batch and then break the loop\n",
    "\n",
    "# Device configuration - Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_of_epochs = 10\n",
    "\n",
    "for epoch in range(num_of_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Select the predictions for the last token in each sequence\n",
    "        # Assuming outputs is of shape [batch_size, seq_length, vocab_size]\n",
    "        last_outputs = outputs[:, -1, :]\n",
    "\n",
    "        # Calculate the loss between the predictions of the last token and the targets\n",
    "        loss = criterion(last_outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_of_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
